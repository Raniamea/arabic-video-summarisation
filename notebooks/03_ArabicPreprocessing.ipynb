{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376a981b",
   "metadata": {},
   "source": [
    "# üß† Arabic Preprocessing with CAMeL Tools\n",
    "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Install compatible versions of NumPy and CAMeL Tools\n",
    "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
    "!pip install camel-tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üì• Upload your files: transcript (.txt) and captions (.json)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60040256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÑ Parse transcript file with timecodes\n",
    "import re\n",
    "\n",
    "def load_transcript(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    segments = []\n",
    "    pattern = re.compile(r\"\\[(\\d+\\.\\d+) - (\\d+\\.\\d+)\\]\\s+(.*)\")\n",
    "    for line in lines:\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            start, end, text = match.groups()\n",
    "            segments.append({\n",
    "                \"start\": float(start),\n",
    "                \"end\": float(end),\n",
    "                \"text\": text.strip()\n",
    "            })\n",
    "    return segments\n",
    "\n",
    "transcript_path = [f for f in uploaded if f.endswith(\".txt\")][0]\n",
    "segments = load_transcript(transcript_path)\n",
    "print(f\"Loaded {len(segments)} transcript segments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÑ Load caption JSON\n",
    "import json\n",
    "\n",
    "def load_captions(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    captions = []\n",
    "    for scene_id, meta in data.items():\n",
    "        captions.append({\n",
    "            \"scene_id\": scene_id,\n",
    "            \"scene_time\": meta[\"scene_time\"],\n",
    "            \"caption\": meta[\"arabic\"]\n",
    "        })\n",
    "    return captions\n",
    "\n",
    "captions_path = [f for f in uploaded if f.endswith(\".json\")][0]\n",
    "captions = load_captions(captions_path)\n",
    "print(f\"Loaded {len(captions)} scene captions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47197d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß™ Normalize + Lemmatize using CAMeL Tools\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "disambig = MLEDisambiguator.pretrained()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = dediac_ar(text)  # Remove diacritics\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambig_results = disambig.disambiguate(tokens)\n",
    "    lemmas = [res.analyses[0].lemma if res.analyses else tok for tok, res in zip(tokens, disambig_results)]\n",
    "    return {\n",
    "        \"original\": text,\n",
    "        \"tokens\": tokens,\n",
    "        \"lemmas\": lemmas\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575758c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîÅ Process all transcript segments\n",
    "processed_segments = []\n",
    "for seg in segments:\n",
    "    proc = preprocess(seg[\"text\"])\n",
    "    processed_segments.append({\n",
    "        \"start\": seg[\"start\"],\n",
    "        \"end\": seg[\"end\"],\n",
    "        \"original\": seg[\"text\"],\n",
    "        \"tokens\": proc[\"tokens\"],\n",
    "        \"lemmas\": proc[\"lemmas\"]\n",
    "    })\n",
    "\n",
    "with open(\"processed_transcript.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_segments, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved: processed_transcript.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîÅ Process all captions\n",
    "processed_captions = []\n",
    "for cap in captions:\n",
    "    proc = preprocess(cap[\"caption\"])\n",
    "    processed_captions.append({\n",
    "        \"scene_id\": cap[\"scene_id\"],\n",
    "        \"scene_time\": cap[\"scene_time\"],\n",
    "        \"original\": cap[\"caption\"],\n",
    "        \"tokens\": proc[\"tokens\"],\n",
    "        \"lemmas\": proc[\"lemmas\"]\n",
    "    })\n",
    "\n",
    "with open(\"processed_captions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_captions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved: processed_captions.json\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
