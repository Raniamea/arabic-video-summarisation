{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# ðŸ§  Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.26.4 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools==1.5.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b85e1ab8",
      "metadata": {
        "id": "b85e1ab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d714fd94-0587-4bc8-8299-b3646dab8c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ðŸ“¥ Upload your files: transcript (.txt) and captions (.json)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "import os\n",
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "preprocessing_path = os.path.join(base_path, \"Preprocessed\")\n",
        "\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!camel_data -l"
      ],
      "metadata": {
        "id": "eFQI5ugul8iP"
      },
      "id": "eFQI5ugul8iP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!camel_data -i morphology-db-msa-r13\n",
        "!camel_data -i disambig-mle-calima-msa-r13\n",
        "\n",
        "!camel_data -i morphology-db-egy-r13\n",
        "!camel_data -i disambig-mle-calima-egy-r13\n",
        "\n",
        "!camel_data -i dialectid-model26"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubb97UkLmWBg",
        "outputId": "df255e11-3e04-4e29-9cd0-c6f3942fc8b8"
      },
      "id": "Ubb97UkLmWBg",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No new packages will be installed.\n",
            "No new packages will be installed.\n",
            "No new packages will be installed.\n",
            "The following packages will be installed: 'morphology-db-egy-r13'\n",
            "Downloading package 'morphology-db-egy-r13': 100% 67.3M/67.3M [00:02<00:00, 25.4MB/s]\n",
            "Extracting package 'morphology-db-egy-r13': 100% 67.3M/67.3M [00:00<00:00, 193MB/s]\n",
            "The following packages will be installed: 'disambig-mle-calima-egy-r13'\n",
            "Downloading package 'disambig-mle-calima-egy-r13': 100% 27.2M/27.2M [00:00<00:00, 29.0MB/s]\n",
            "Extracting package 'disambig-mle-calima-egy-r13': 100% 27.2M/27.2M [00:00<00:00, 234MB/s]\n",
            "No new packages will be installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "from camel_tools.dialectid import DialectIdentifier\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "\n",
        "# ðŸ“¥ Initialize all tools\n",
        "dialect_id = DialectIdentifier.pretrained()\n",
        "msa_disambig = MLEDisambiguator.pretrained(model='calima-msa-r13')\n",
        "egy_disambig = MLEDisambiguator.pretrained(model='calima-egy-r13')\n",
        "\n",
        "# âœ… Detect dialect\n",
        "def get_dialect(text):\n",
        "    return dialect_id.predict([text])[0]  # returns 'MSA', 'EGY', etc.\n",
        "\n",
        "# ðŸ§  Map sub-dialect label to broader disambiguation class\n",
        "def map_dialect(did_label):\n",
        "    if did_label == 'MSA':\n",
        "        return 'MSA'\n",
        "    elif did_label in ['EGY', 'CAI', 'ALX', 'ASW', 'SKH', 'MNF', 'ALY']:\n",
        "        return 'EGY'\n",
        "    else:\n",
        "        return 'OTHER'\n",
        "\n",
        "\n",
        "# âœ… Preprocessing function\n",
        "def prepr2ocess(text):\n",
        "    text = dediac_ar(text)\n",
        "    tokens = simple_word_tokenize(text)\n",
        "    detected = dialect_id.predict_sentence(text)\n",
        "    dialect = map_dialect(detected)\n",
        "\n",
        "    if dialect == \"MSA\":\n",
        "        disambig = msa_disambig\n",
        "    elif dialect == \"EGY\":\n",
        "        disambig = egy_disambig\n",
        "    else:\n",
        "        print(f\"âš ï¸ Skipping disambiguation. Unsupported dialect: {detected}\")\n",
        "        return {\n",
        "            \"dialect\": detected,\n",
        "            \"original\": text,\n",
        "            \"tokens\": tokens,\n",
        "            \"lemmas\": tokens  # fallback: no lemmatization\n",
        "        }\n",
        "\n",
        "\n",
        "    # Disambiguate and extract lemmas\n",
        "    result = disambig.disambiguate(tokens)\n",
        "    lemmas = []\n",
        "    for i, r in enumerate(result):\n",
        "        if r.analyses:\n",
        "            analysis = r.analyses[0][1]\n",
        "            lemma = analysis.get('lemma', r.word)\n",
        "            lemmas.append(lemma)\n",
        "        else:\n",
        "            print(f\"âŒ No analysis for token: '{tokens[i]}'\")\n",
        "            lemmas.append(tokens[i])\n",
        "\n",
        "    return {\n",
        "        \"original\": text,\n",
        "        \"dialect\": detected,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": lemmas\n",
        "    }\n"
      ],
      "metadata": {
        "id": "0UJVmZPz3VT5"
      },
      "id": "0UJVmZPz3VT5",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "60040256",
      "metadata": {
        "id": "60040256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "f5b5be5c-646a-4ea1-be04-e4837eef9de1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DIDModel26' object has no attribute 'predict_sentence'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3888868643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{transcripts_path}/{video_name}_ar_with_timecodes.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded and lemmatized {len(segments)} transcript segments.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtranscript_preprocess_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{video_name}_transcript_ar.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3888868643.py\u001b[0m in \u001b[0;36mload_transcript\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             segments.append({\n\u001b[1;32m     14\u001b[0m                 \u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2347543110.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdediac_ar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_word_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdetected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialect_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdialect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_dialect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DIDModel26' object has no attribute 'predict_sentence'"
          ]
        }
      ],
      "source": [
        "# Parse and lemmatize transcript file with timecodes\n",
        "def load_transcript(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    segments = []\n",
        "    pattern = re.compile(r\"\\[(\\d+\\.\\d+)\\s*-\\s*(\\d+\\.\\d+)\\]\\s+(.*)\")\n",
        "    for line in lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            start, end, text = match.groups()\n",
        "            result = preprocess(text.strip())\n",
        "            segments.append({\n",
        "                \"start\": float(start),\n",
        "                \"end\": float(end),\n",
        "                \"original\": text.strip(),\n",
        "                \"tokens\": result[\"tokens\"],\n",
        "                \"lemmas\": result[\"lemmas\"]\n",
        "            })\n",
        "    return segments\n",
        "\n",
        "segments = load_transcript(f\"{transcripts_path}/{video_name}_ar_with_timecodes.txt\")\n",
        "print(f\"Loaded and lemmatized {len(segments)} transcript segments.\")\n",
        "transcript_preprocess_path = os.path.join(preprocessing_path, f\"{video_name}_transcript_ar.json\")\n",
        "with open(transcript_preprocess_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(segments, f, ensure_ascii=False, indent=2)\n",
        "print(f\" Saved: {transcript_preprocess_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "74bf09b8",
      "metadata": {
        "id": "74bf09b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cfcb52-220c-46f8-e44e-f52561cb3105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded and lemmatized 54 scene captions.\n",
            "Saved: /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/PaperMaking_captions_ar.json\n"
          ]
        }
      ],
      "source": [
        "# Parse and lemmatize captions file\n",
        "def load_captions(path, disambig):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    captions = []\n",
        "    for scene_id, meta in data.items():\n",
        "        scene_time = meta.get(\"scene_time\", \"UNKNOWN\")\n",
        "        arabic_caption = meta.get(\"arabic\", \"\")\n",
        "        lemmatized_caption = preprocess(arabic_caption, disambig)\n",
        "\n",
        "        captions.append({\n",
        "            \"scene_id\": scene_id,\n",
        "            \"scene_time\": scene_time,\n",
        "            \"caption\": arabic_caption,\n",
        "            \"lemmas\": lemmatized_caption\n",
        "        })\n",
        "\n",
        "    return captions\n",
        "\n",
        "# âœ… Run\n",
        "captions = load_captions(f\"{captions_path}/{video_name}.json\", disambig)\n",
        "print(f\"Loaded and lemmatized {len(captions)} scene captions.\")\n",
        "captions_preprocess_path = os.path.join(preprocessing_path, f\"{video_name}_captions_ar.json\")\n",
        "with open(captions_preprocess_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(segments, f, ensure_ascii=False, indent=2)\n",
        "print(f\"Saved: {captions_preprocess_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}