{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization & lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# âœ… Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the morphology DB for MSA\n",
        "!camel_data -i morphology-db-msa-r13\n",
        "\n",
        "# Download the MLE disambiguator for MSA\n",
        "!camel_data -i disambig-mle-calima-msa-r13"
      ],
      "metadata": {
        "id": "q-NIP7_Bw6cD"
      },
      "id": "q-NIP7_Bw6cD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-uwk3G4EZ-Wq"
      },
      "id": "-uwk3G4EZ-Wq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Project paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "param_path = os.path.join(base_path, \"params.json\")\n",
        "with open(param_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    params = json.load(f)\n",
        "video_filename = params.get(\"video_file\")\n",
        "video_filename=\"Elhabasa.mp4\"\n",
        "\n",
        "assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar.json\")\n",
        "preprocessed_transcript_path= os.path.join(preprocessed_path, f\"{video_name}_CleanTranscript.json\")\n",
        "\n",
        "print(\"Input :\", transcript_path)\n",
        "print(\"Output:\", preprocessed_transcript_path)\n"
      ],
      "metadata": {
        "id": "5sePVQVO90fy"
      },
      "id": "5sePVQVO90fy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove diacritics and standardize common forms (keep things readable but consistent)\n",
        "_AR_DIAC = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
        "_TATWEEL = \"\\u0640\"\n",
        "\n",
        "def normalize_ar(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    s = text\n",
        "    s = _AR_DIAC.sub(\"\", s)         # strip diacritics\n",
        "    s = s.replace(_TATWEEL, \"\")     # strip tatweel\n",
        "    # Normalize hamza/alef variants\n",
        "    s = s.replace(\"Ø£\", \"Ø§\").replace(\"Ø¥\", \"Ø§\").replace(\"Ø¢\", \"Ø§\")\n",
        "    # Normalize alef maqsura to ya\n",
        "    s = s.replace(\"Ù‰\", \"ÙŠ\")\n",
        "    # Collapse whitespace\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def simple_word_tokens(text: str) -> List[str]:\n",
        "    # Arabic letters + Arabic/ASCII digits\n",
        "    return re.findall(r\"[\\u0600-\\u06FF]+|[0-9Ù -Ù©]+\", text)"
      ],
      "metadata": {
        "id": "AIvhVKl8AjeE"
      },
      "id": "AIvhVKl8AjeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "\n",
        "# Pretrained MSA disambiguator (context-aware; reliable baseline)\n",
        "msa_mle = MLEDisambiguator.pretrained()\n",
        "\n",
        "def dediac(s: str) -> str:\n",
        "    return _AR_DIAC.sub(\"\", s or \"\")\n",
        "\n",
        "def _analysis_dict_from_scored(best):\n",
        "    # CAMeL returns ScoredAnalysis objects; the dict is in .analysis\n",
        "    if hasattr(best, \"analysis\"):\n",
        "        return best.analysis\n",
        "    if isinstance(best, dict):\n",
        "        return best\n",
        "    if hasattr(best, \"__dict__\"):\n",
        "        return best.__dict__\n",
        "    return {}\n",
        "\n",
        "def disambiguate_tokens(tokens: List[str]) -> Dict[str, List[str]]:\n",
        "    \"\"\"Return parallel lists: lemmas, pos, diac, plus small diagnostics.\"\"\"\n",
        "    lemmas, poss, diacs = [], [], []\n",
        "    oov_flags = []  # True if we failed to get a proper analysis\n",
        "    results = msa_mle.disambiguate(tokens)\n",
        "    for tok, res in zip(tokens, results):\n",
        "        lemma, pos, diac = tok, \"\", \"\"\n",
        "        oov = True\n",
        "        try:\n",
        "            if getattr(res, \"analyses\", None):\n",
        "                best = res.analyses[0]\n",
        "                feats = _analysis_dict_from_scored(best)\n",
        "                raw_lemma = feats.get(\"lemma\") or feats.get(\"lex\") or tok\n",
        "                lemma = dediac(raw_lemma)\n",
        "                pos   = feats.get(\"pos\") or feats.get(\"bw\") or \"\"\n",
        "                diac  = feats.get(\"diac\") or \"\"\n",
        "                oov   = False if (pos or diac or raw_lemma) else True\n",
        "        except Exception:\n",
        "            pass\n",
        "        lemmas.append(lemma); poss.append(pos); diacs.append(diac); oov_flags.append(oov)\n",
        "    return {\"lemmas\": lemmas, \"pos\": poss, \"diac\": diacs, \"oov\": oov_flags}\n"
      ],
      "metadata": {
        "id": "m-TZIVvub7B_"
      },
      "id": "m-TZIVvub7B_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class DiacSmoother:\n",
        "    \"\"\"\n",
        "    Keeps a canonical diacritization per (lemma, POS) across the document\n",
        "    to reduce flip-flops. Conservative: only applies when POS matches and we\n",
        "    already have a non-empty canonical diac for that (lemma, POS).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.canonical = {}                    # (lemma, pos) -> diac\n",
        "        self.freqs = defaultdict(lambda: defaultdict(int))  # (lemma,pos)[diac] -> count\n",
        "\n",
        "    def observe(self, lemma: str, pos: str, diac: str):\n",
        "        if not lemma or not pos or not diac:\n",
        "            return\n",
        "        key = (lemma, pos)\n",
        "        self.freqs[key][diac] += 1\n",
        "        # Update canonical to the most frequent diac so far\n",
        "        self.canonical[key] = max(self.freqs[key], key=self.freqs[key].get)\n",
        "\n",
        "    def smooth(self, lemma: str, pos: str, diac: str) -> str:\n",
        "        if not lemma or not pos:\n",
        "            return diac\n",
        "        key = (lemma, pos)\n",
        "        canon = self.canonical.get(key)\n",
        "        if canon and diac and diac != canon:\n",
        "            # Prefer the canonical diac unless current diac is empty\n",
        "            return canon\n",
        "        if canon and not diac:\n",
        "            # Fill in missing diac with canonical\n",
        "            return canon\n",
        "        return diac\n",
        "\n",
        "smoother = DiacSmoother()\n"
      ],
      "metadata": {
        "id": "H1Ox_B36cEz5"
      },
      "id": "H1Ox_B36cEz5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transcript; accept either {\"segments\":[...]} or a plain list\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "segments = raw.get(\"segments\", raw)\n",
        "assert isinstance(segments, list), \"Input JSON must be a list or have a 'segments' list.\"\n",
        "\n",
        "clean_segments = []\n",
        "for seg in segments:\n",
        "    # Flexible field names\n",
        "    start = seg.get(\"start\") if isinstance(seg, dict) else None\n",
        "    end   = seg.get(\"end\")   if isinstance(seg, dict) else None\n",
        "    text  = (seg.get(\"text\") or seg.get(\"original\") or seg.get(\"utterance\") or \"\").strip()\n",
        "\n",
        "    text_norm = normalize_ar(text)\n",
        "    tokens = simple_word_tokens(text_norm)\n",
        "\n",
        "    if tokens:\n",
        "        ana = disambiguate_tokens(tokens)\n",
        "    else:\n",
        "        ana = {\"lemmas\": [], \"pos\": [], \"diac\": [], \"oov\": []}\n",
        "\n",
        "    # Update smoother with observations and apply smoothing\n",
        "    diac_smooth = []\n",
        "    for l, p, d in zip(ana[\"lemmas\"], ana[\"pos\"], ana[\"diac\"]):\n",
        "        if d:\n",
        "            smoother.observe(l, p, d)\n",
        "        diac_smooth.append(smoother.smooth(l, p, d))\n",
        "\n",
        "    # Simple quality heuristics for a per-segment reliability flag\n",
        "    n = max(1, len(tokens))\n",
        "    diac_cov = sum(1 for d in diac_smooth if d) / n\n",
        "    oov_ratio = sum(1 for o in ana[\"oov\"] if o) / n\n",
        "    diac_reliable = (len(tokens) >= 3) and (diac_cov >= 0.6) and (oov_ratio <= 0.5)\n",
        "\n",
        "    clean_segments.append({\n",
        "        \"start\": start,\n",
        "        \"end\": end,\n",
        "        \"original\": text,          # raw ASR line (human-readable)\n",
        "        \"text_norm\": text_norm,    # normalized (search/display)\n",
        "        \"tokens\": tokens,          # cleaned tokens\n",
        "        \"lemmas\": ana[\"lemmas\"],   # context-aware lemmas\n",
        "        \"pos\": ana[\"pos\"],         # coarse POS\n",
        "        \"diac\": diac_smooth,       # smoothed diacritics (can hide if unreliable)\n",
        "        \"diac_coverage\": round(diac_cov, 3),\n",
        "        \"oov_ratio\": round(oov_ratio, 3),\n",
        "    })\n",
        "\n",
        "clean_doc = {\n",
        "    \"video\": video_name,\n",
        "    \"num_segments\": len(clean_segments),\n",
        "    \"segments\": clean_segments\n",
        "}\n",
        "\n",
        "with open(preprocessed_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(clean_doc, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Saved cleaned transcript â†’ {preprocessed_transcript_path}\")\n",
        "print(f\"Segments: {len(clean_segments)}\")\n"
      ],
      "metadata": {
        "id": "y4SynYSucHx8"
      },
      "id": "y4SynYSucHx8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "# ====== LOAD ======\n",
        "with open(preprocessed_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "segments = data.get(\"segments\", [])\n",
        "before = len(segments)\n",
        "\n",
        "def _has_content(val):\n",
        "    if val is None:\n",
        "        return False\n",
        "    if isinstance(val, list):\n",
        "        return len(val) > 0\n",
        "    if isinstance(val, str):\n",
        "        return val.strip() != \"\"\n",
        "    return False\n",
        "\n",
        "def _normalize_to_list(val):\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    if isinstance(val, str):\n",
        "        val = val.strip()\n",
        "        return [val] if val else []\n",
        "    return []\n",
        "\n",
        "def _is_only_musiqi(lemmas_list):\n",
        "    \"\"\"Return True if there's exactly one lemma and it equals 'Ù…ÙˆØ³ÙŠÙ‚ÙŠ' after stripping.\"\"\"\n",
        "    return len(lemmas_list) == 1 and (lemmas_list[0].strip() == \"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\")\n",
        "\n",
        "filtered = []\n",
        "dropped_missing_pos_lem = 0\n",
        "dropped_only_musiqi = 0\n",
        "\n",
        "for seg in segments:\n",
        "    lem = _normalize_to_list(seg.get(\"lemmas\"))\n",
        "    pos = _normalize_to_list(seg.get(\"pos\"))\n",
        "\n",
        "    # Drop if lemmas or POS missing/empty\n",
        "    if not _has_content(lem) or not _has_content(pos):\n",
        "        dropped_missing_pos_lem += 1\n",
        "        continue\n",
        "\n",
        "    # Drop if exactly one lemma and it's \"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\"\n",
        "    if _is_only_musiqi(lem):\n",
        "        dropped_only_musiqi += 1\n",
        "        continue\n",
        "\n",
        "    # Keep normalized lists for downstream consistency\n",
        "    seg[\"lemmas\"] = lem\n",
        "    seg[\"pos\"] = pos\n",
        "    filtered.append(seg)\n",
        "\n",
        "after = len(filtered)\n",
        "\n",
        "# Update and save\n",
        "data[\"segments\"] = filtered\n",
        "data[\"num_segments\"] = after\n",
        "\n",
        "base_dir  = os.path.dirname(preprocessed_transcript_path)\n",
        "base_name = os.path.splitext(os.path.basename(preprocessed_transcript_path))[0]\n",
        "out_name  = f\"{base_name}.json\"\n",
        "out_path  = os.path.join(base_dir or \".\", out_name)\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Kept {after} / {before} segments\")\n",
        "print(f\"   â€¢ Dropped (missing lemmas/POS): {dropped_missing_pos_lem}\")\n",
        "print(f\"   â€¢ Dropped (single lemma == 'Ù…ÙˆØ³ÙŠÙ‚ÙŠ'): {dropped_only_musiqi}\")\n",
        "print(\"ðŸ’¾ Saved new file:\", preprocessed_transcript_path)\n"
      ],
      "metadata": {
        "id": "hTM7e4lRNb7V"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hTM7e4lRNb7V"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}