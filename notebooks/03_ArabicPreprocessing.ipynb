{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# üß† Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools==1.5.6 --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b85e1ab8",
      "metadata": {
        "id": "b85e1ab8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üì• Upload your files: transcript (.txt) and captions (.json)\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "import os\n",
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "preprocessing_path = os.path.join(base_path, \"Preprocessed\")\n",
        "\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!camel_data -l"
      ],
      "metadata": {
        "id": "eFQI5ugul8iP"
      },
      "id": "eFQI5ugul8iP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!camel_data -i morphology-db-msa-r13\n",
        "!camel_data -i disambig-mle-calima-msa-r13"
      ],
      "metadata": {
        "id": "Ubb97UkLmWBg"
      },
      "id": "Ubb97UkLmWBg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47197d02",
      "metadata": {
        "id": "47197d02"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "# Load the disambiguator once\n",
        "disambig = MLEDisambiguator.pretrained()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "60040256",
      "metadata": {
        "id": "60040256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "00086a1f-0c7d-4bcc-f4d0-f2dffb323e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded and lemmatized 69 transcript segments.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ArabicVideoSummariser/preprocessed/PaperMaking_transcript_ar.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2719367830.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded and lemmatized {len(segments)} transcript segments.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtranscript_preprocess_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{video_name}_transcript_ar.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_preprocess_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ArabicVideoSummariser/preprocessed/PaperMaking_transcript_ar.json'"
          ]
        }
      ],
      "source": [
        "def preprocess(text, disambig):\n",
        "    tokens = simple_word_tokenize(text)\n",
        "    result = disambig.disambiguate(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for i, r in enumerate(result):\n",
        "        if r.analyses:\n",
        "            analysis = r.analyses[0][1]\n",
        "            lemma = analysis.get('lemma', r.word)\n",
        "            lemmas.append(lemma)\n",
        "        else:\n",
        "            print(f\"‚ùå No analysis for token: '{tokens[i]}'\")\n",
        "            lemmas.append(r.word)\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "# üìÑ Parse and lemmatize transcript file with timecodes\n",
        "def load_transcript(path, disambig):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    segments = []\n",
        "    pattern = re.compile(r\"\\[(\\d+\\.\\d+) - (\\d+\\.\\d+)\\]\\s+(.*)\")\n",
        "    for line in lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            start, end, text = match.groups()\n",
        "            lemmatized = preprocess(text.strip(), disambig)\n",
        "            segments.append({\n",
        "                \"start\": float(start),\n",
        "                \"end\": float(end),\n",
        "                \"text\": text.strip(),\n",
        "                \"lemmas\": lemmatized\n",
        "            })\n",
        "    return segments\n",
        "\n",
        "# ‚úÖ\n",
        "#print (f\"{transcripts_path}/{video_name}_ar_with_timecodes.txt\")\n",
        "segments = load_transcript(f\"{transcripts_path}/{video_name}_ar_with_timecodes.txt\", disambig)\n",
        "print(f\"Loaded and lemmatized {len(segments)} transcript segments.\")\n",
        "transcript_preprocess_path = os.path.join(preprocessing_path, f\"{video_name}_transcript_ar.json\")\n",
        "with open(transcript_preprocess_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(segments, f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "74bf09b8",
      "metadata": {
        "id": "74bf09b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663f6604-daa9-4a8c-f9c2-0175d3356f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded and lemmatized 54 scene captions.\n"
          ]
        }
      ],
      "source": [
        "def load_captions(path, disambig):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    captions = []\n",
        "    for scene_id, meta in data.items():\n",
        "        scene_time = meta.get(\"scene_time\", \"UNKNOWN\")\n",
        "        arabic_caption = meta.get(\"arabic\", \"\")\n",
        "        lemmatized_caption = preprocess(arabic_caption, disambig)\n",
        "\n",
        "        captions.append({\n",
        "            \"scene_id\": scene_id,\n",
        "            \"scene_time\": scene_time,\n",
        "            \"caption\": arabic_caption,\n",
        "            \"lemmas\": lemmatized_caption\n",
        "        })\n",
        "\n",
        "    return captions\n",
        "\n",
        "# ‚úÖ Run\n",
        "captions_path = [f for f in uploaded if f.endswith(\".json\")][0]\n",
        "captions = load_captions(captions_path, disambig)\n",
        "print(f\"Loaded and lemmatized {len(captions)} scene captions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_preprocess_path = os.path.join(preprocessing_path, f\"{video_name}_transcript_ar.json\")\n",
        "caption_preprocess_path = os.path.join(preprocessing_path, f\"{video_name}_caption_ar.json\")\n",
        "with open(transcript_preprocess_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(caption_preprocess_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved: $transcript_preprocess_path and $caption_preprocess_path\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e8_SVFNqswM",
        "outputId": "ca83e7c9-f6a1-4826-f553-fa84f51a9580"
      },
      "id": "1e8_SVFNqswM",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved: processed_transcript.json and processed_captions.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}