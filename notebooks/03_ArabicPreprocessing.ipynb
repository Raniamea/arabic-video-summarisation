{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization & lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the morphology DB for MSA\n",
        "!camel_data -i morphology-db-msa-r13\n",
        "\n",
        "# Download the MLE disambiguator for MSA\n",
        "!camel_data -i disambig-mle-calima-msa-r13\n",
        "\n",
        "!mkdir -p ~/.camel_tools/data/ner/arabert\n",
        "!wget -q https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa-ner/resolve/main/config.json -O ~/.camel_tools/data/ner/arabert/config.json\n",
        "!wget -q https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa-ner/resolve/main/pytorch_model.bin -O ~/.camel_tools/data/ner/arabert/pytorch_model.bin\n",
        "!wget -q https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa-ner/resolve/main/vocab.txt -O ~/.camel_tools/data/ner/arabert/vocab.txt\n"
      ],
      "metadata": {
        "id": "q-NIP7_Bw6cD"
      },
      "id": "q-NIP7_Bw6cD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-uwk3G4EZ-Wq"
      },
      "id": "-uwk3G4EZ-Wq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Project paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "param_path = os.path.join(base_path, \"params.json\")\n",
        "with open(param_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    params = json.load(f)\n",
        "video_filename = params.get(\"video_file\")\n",
        "video_filename=\"Almasbagha.mp4\"\n",
        "\n",
        "assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar.json\")\n",
        "preprocessed_transcript_path= os.path.join(preprocessed_path, f\"{video_name}_CleanTranscript.json\")\n",
        "\n",
        "print(\"Input :\", transcript_path)\n",
        "print(\"Output:\", preprocessed_transcript_path)\n"
      ],
      "metadata": {
        "id": "5sePVQVO90fy"
      },
      "id": "5sePVQVO90fy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove diacritics and standardize common forms (keep things readable but consistent)\n",
        "_AR_DIAC = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
        "_TATWEEL = \"\\u0640\"\n",
        "\n",
        "def normalize_ar(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    s = text\n",
        "    s = _AR_DIAC.sub(\"\", s)         # strip diacritics\n",
        "    s = s.replace(_TATWEEL, \"\")     # strip tatweel\n",
        "    # Normalize hamza/alef variants\n",
        "    s = s.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
        "    # Normalize alef maqsura to ya\n",
        "    s = s.replace(\"ى\", \"ي\")\n",
        "    # Collapse whitespace\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def simple_word_tokens(text: str) -> List[str]:\n",
        "    # Arabic letters + Arabic/ASCII digits\n",
        "    return re.findall(r\"[\\u0600-\\u06FF]+|[0-9٠-٩]+\", text)"
      ],
      "metadata": {
        "id": "AIvhVKl8AjeE"
      },
      "id": "AIvhVKl8AjeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "\n",
        "# Pretrained MSA disambiguator\n",
        "msa_mle = MLEDisambiguator.pretrained()\n",
        "\n",
        "from camel_tools.ner import NERecognizer\n",
        "ner = NERecognizer(model_path=\"/root/.camel_tools/data/ner/arabert/\")\n",
        "\n",
        "def dediac(s: str) -> str:\n",
        "    return _AR_DIAC.sub(\"\", s or \"\")\n",
        "\n",
        "def _analysis_dict_from_scored(best):\n",
        "    # CAMeL returns ScoredAnalysis objects; the dict is in .analysis\n",
        "    if hasattr(best, \"analysis\"):\n",
        "        return best.analysis\n",
        "    if isinstance(best, dict):\n",
        "        return best\n",
        "    if hasattr(best, \"__dict__\"):\n",
        "        return best.__dict__\n",
        "    return {}\n",
        "\n",
        "def disambiguate_tokens(tokens: List[str]) -> Dict[str, List[str]]:\n",
        "    \"\"\"Return parallel lists: lemmas, pos, diac, plus small diagnostics.\"\"\"\n",
        "    lemmas, poss, diacs = [], [], []\n",
        "    oov_flags = []  # True if we failed to get a proper analysis\n",
        "    results = msa_mle.disambiguate(tokens)\n",
        "    for tok, res in zip(tokens, results):\n",
        "        lemma, pos, diac = tok, \"\", \"\"\n",
        "        oov = True\n",
        "        try:\n",
        "            if getattr(res, \"analyses\", None):\n",
        "                best = res.analyses[0]\n",
        "                feats = _analysis_dict_from_scored(best)\n",
        "                raw_lemma = feats.get(\"lemma\") or feats.get(\"lex\") or tok\n",
        "                lemma = dediac(raw_lemma)\n",
        "                pos   = feats.get(\"pos\") or feats.get(\"bw\") or \"\"\n",
        "                diac  = feats.get(\"diac\") or \"\"\n",
        "                oov   = False if (pos or diac or raw_lemma) else True\n",
        "        except Exception:\n",
        "            pass\n",
        "        lemmas.append(lemma); poss.append(pos); diacs.append(diac); oov_flags.append(oov)\n",
        "    return {\"lemmas\": lemmas, \"pos\": poss, \"diac\": diacs, \"oov\": oov_flags}\n"
      ],
      "metadata": {
        "id": "m-TZIVvub7B_"
      },
      "id": "m-TZIVvub7B_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class DiacSmoother:\n",
        "    \"\"\"\n",
        "    Keeps a canonical diacritization per (lemma, POS) across the document\n",
        "    to reduce flip-flops. Conservative: only applies when POS matches and we\n",
        "    already have a non-empty canonical diac for that (lemma, POS).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.canonical = {}                    # (lemma, pos) -> diac\n",
        "        self.freqs = defaultdict(lambda: defaultdict(int))  # (lemma,pos)[diac] -> count\n",
        "\n",
        "    def observe(self, lemma: str, pos: str, diac: str):\n",
        "        if not lemma or not pos or not diac:\n",
        "            return\n",
        "        key = (lemma, pos)\n",
        "        self.freqs[key][diac] += 1\n",
        "        # Update canonical to the most frequent diac so far\n",
        "        self.canonical[key] = max(self.freqs[key], key=self.freqs[key].get)\n",
        "\n",
        "    def smooth(self, lemma: str, pos: str, diac: str) -> str:\n",
        "        if not lemma or not pos:\n",
        "            return diac\n",
        "        key = (lemma, pos)\n",
        "        canon = self.canonical.get(key)\n",
        "        if canon and diac and diac != canon:\n",
        "            # Prefer the canonical diac unless current diac is empty\n",
        "            return canon\n",
        "        if canon and not diac:\n",
        "            # Fill in missing diac with canonical\n",
        "            return canon\n",
        "        return diac\n",
        "\n",
        "smoother = DiacSmoother()\n"
      ],
      "metadata": {
        "id": "H1Ox_B36cEz5"
      },
      "id": "H1Ox_B36cEz5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transcript; accept either {\"segments\":[...]} or a plain list\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "segments = raw.get(\"segments\", raw)\n",
        "assert isinstance(segments, list), \"Input JSON must be a list or have a 'segments' list.\"\n",
        "\n",
        "clean_segments = []\n",
        "\n",
        "import json, os\n",
        "from camel_tools.ner import NERecognizer\n",
        "\n",
        "# --- Load transcript (accepts dict or list) ---\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "segments = raw.get(\"segments\", raw)\n",
        "assert isinstance(segments, list), \"Input JSON must be a list or have a 'segments' list.\"\n",
        "\n",
        "# --- Load NER model once ---\n",
        "ner = NERecognizer.pretrained()\n",
        "\n",
        "clean_segments = []\n",
        "\n",
        "def _normalize_to_list(val):\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    if isinstance(val, str):\n",
        "        val = val.strip()\n",
        "        return [val] if val else []\n",
        "    return []\n",
        "\n",
        "def _is_only_musiqi(seg):\n",
        "    \"\"\"Return True if the segment has only one lemma equal to 'موسيقي'.\"\"\"\n",
        "    lem = _normalize_to_list(seg.get(\"lemmas\"))\n",
        "    return len(lem) == 1 and lem[0].strip() == \"موسيقي\"\n",
        "\n",
        "# --- Preprocess each segment ---\n",
        "for seg in segments:\n",
        "    start = seg.get(\"start\") if isinstance(seg, dict) else None\n",
        "    end   = seg.get(\"end\")   if isinstance(seg, dict) else None\n",
        "    text  = (seg.get(\"text\") or seg.get(\"original\") or seg.get(\"utterance\") or \"\").strip()\n",
        "\n",
        "    # --- Normalization ---\n",
        "    text_norm = normalize_ar(text)\n",
        "\n",
        "    # --- Tokenization ---\n",
        "    tokens = simple_word_tokens(text_norm)\n",
        "\n",
        "    if tokens:\n",
        "        ana = disambiguate_tokens(tokens)\n",
        "    else:\n",
        "        ana = {\"lemmas\": [], \"pos\": [], \"diac\": [], \"oov\": []}\n",
        "\n",
        "    # --- Diacritic smoothing ---\n",
        "    diac_smooth = []\n",
        "    for l, p, d in zip(ana[\"lemmas\"], ana[\"pos\"], ana[\"diac\"]):\n",
        "        if d:\n",
        "            smoother.observe(l, p, d)\n",
        "        diac_smooth.append(smoother.smooth(l, p, d))\n",
        "\n",
        "    # --- Segment quality heuristics ---\n",
        "    n = max(1, len(tokens))\n",
        "    diac_cov = sum(1 for d in diac_smooth if d) / n\n",
        "    oov_ratio = sum(1 for o in ana[\"oov\"] if o) / n\n",
        "\n",
        "    # --- Named Entity Recognition ---\n",
        "    ner_tags = ner.predict_sentence(tokens) if tokens else []\n",
        "\n",
        "    # --- Extract named entities ---\n",
        "    def extract_named_entities(tokens, tags):\n",
        "        entities = []\n",
        "        current_entity, current_type = [], None\n",
        "        for token, tag in zip(tokens, tags):\n",
        "            if tag != 'O':\n",
        "                if tag == current_type:\n",
        "                    current_entity.append(token)\n",
        "                else:\n",
        "                    if current_entity:\n",
        "                        entities.append((' '.join(current_entity), current_type))\n",
        "                    current_entity = [token]\n",
        "                    current_type = tag\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    entities.append((' '.join(current_entity), current_type))\n",
        "                    current_entity, current_type = [], None\n",
        "        if current_entity:\n",
        "            entities.append((' '.join(current_entity), current_type))\n",
        "        return entities\n",
        "\n",
        "    named_entities = extract_named_entities(tokens, ner_tags)\n",
        "\n",
        "    clean_segments.append({\n",
        "        \"start\": start,\n",
        "        \"end\": end,\n",
        "        \"original\": text,\n",
        "        \"text_norm\": text_norm,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": ana[\"lemmas\"],\n",
        "        \"pos\": ana[\"pos\"],\n",
        "        \"diac\": diac_smooth,\n",
        "        \"diac_coverage\": round(diac_cov, 3),\n",
        "        \"oov_ratio\": round(oov_ratio, 3),\n",
        "        \"ner\": ner_tags,\n",
        "        \"named_entities\": named_entities,\n",
        "    })\n",
        "\n",
        "# --- Merge consecutive 'موسيقي' segments ---\n",
        "merged_segments = []\n",
        "i = 0\n",
        "merged_groups = 0\n",
        "\n",
        "while i < len(clean_segments):\n",
        "    seg = clean_segments[i]\n",
        "\n",
        "    if not _is_only_musiqi(seg):\n",
        "        merged_segments.append(seg)\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "    # Start group of consecutive 'موسيقي' segments\n",
        "    start_time = seg.get(\"start\")\n",
        "    end_time = seg.get(\"end\")\n",
        "    j = i + 1\n",
        "\n",
        "    while j < len(clean_segments) and _is_only_musiqi(clean_segments[j]):\n",
        "        end_time = clean_segments[j].get(\"end\")\n",
        "        j += 1\n",
        "\n",
        "    # Replace group with one empty placeholder\n",
        "    merged_groups += 1\n",
        "    merged_segments.append({\n",
        "        \"start\": start_time,\n",
        "        \"end\": end_time,\n",
        "        \"text\": \"\",\n",
        "        \"text_norm\": \"\",\n",
        "        \"tokens\": [],\n",
        "        \"lemmas\": [],\n",
        "        \"pos\": [],\n",
        "        \"diac\": [],\n",
        "        \"diac_coverage\": 0.0,\n",
        "        \"oov_ratio\": 0.0,\n",
        "        \"ner\": [],\n",
        "        \"named_entities\": [],\n",
        "        \"placeholder\": True\n",
        "    })\n",
        "\n",
        "    i = j\n",
        "\n",
        "# --- Final structured output ---\n",
        "clean_doc = {\n",
        "    \"video\": video_name,\n",
        "    \"num_segments\": len(merged_segments),\n",
        "    \"segments\": merged_segments\n",
        "}\n",
        "\n",
        "\n",
        "with open(preprocessed_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(clean_doc, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Processed {len(segments)} → {len(merged_segments)} segments\")\n",
        "print(f\"Merged groups of consecutive 'موسيقي': {merged_groups}\")\n",
        "print(f\" Saved new file: {preprocessed_transcript_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "y4SynYSucHx8"
      },
      "id": "y4SynYSucHx8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}