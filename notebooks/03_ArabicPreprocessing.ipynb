{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# 🧠 Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b",
        "outputId": "18fb4016-ce4e-4bb2-ad7f-575df59233b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.14.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "geopandas 1.1.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.10 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.90 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "contourpy 1.3.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "scipy 1.16.1 requires numpy<2.6,>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "pywavelets 1.9.0 requires numpy<3,>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "a495b6b8fd8b437287d1789714f1a643"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: camel-tools==1.5.6 in /usr/local/lib/python3.11/dist-packages (1.5.6)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools==1.5.6 --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b85e1ab8",
      "metadata": {
        "id": "b85e1ab8",
        "outputId": "e158095a-e9d0-406e-c689-0a398c858eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3477255-5c13-4757-ac38-805645068fef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3477255-5c13-4757-ac38-805645068fef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PaperMaking.json to PaperMaking (3).json\n",
            "Saving PaperMaking_ar_with_timecodes.txt to PaperMaking_ar_with_timecodes (2).txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 📥 Upload your files: transcript (.txt) and captions (.json)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!camel_data -l"
      ],
      "metadata": {
        "id": "eFQI5ugul8iP",
        "outputId": "0a9b43ca-331a-4e3e-e9a7-8126a2abb4fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eFQI5ugul8iP",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package Name                      Size  License     Description\n",
            "----------------------------  --------  ----------  -------------------------------------------------------------------------------------------------------------------------\n",
            "all                                                 All available CAMeL Tools packages\n",
            "defaults                                            Default datasets for all CAMeL Tools components\n",
            "dialectid-all                                       All available dialect identification models\n",
            "dialectid-model26             371.2 MB  MIT         Dialect identification model trained to differentiating between 25 Arabic city dialects as well as Modern Standard Arabic\n",
            "dialectid-model6              153.0 MB  MIT         Dialect identification model trained to differentiating between 5 Arabic city dialects as well as Modern Standard Arabic\n",
            "disambig-bert-unfactored-all                        All available unfactored BERT disambiguation models\n",
            "disambig-bert-unfactored-egy  445.5 MB  MIT         Unfactored BERT model for disambiguating Egyptian Arabic\n",
            "disambig-bert-unfactored-glf  442.3 MB  MIT         Unfactored BERT model for disambiguating Gulf Arabic\n",
            "disambig-bert-unfactored-lev  441.2 MB  MIT         Unfactored BERT model for disambiguating Levantine Arabic\n",
            "disambig-bert-unfactored-msa  445.0 MB  MIT         Unfactored BERT model for disambiguating Modern Standard Arabic\n",
            "disambig-mle-all                                    All available MLE disambiguation models\n",
            "disambig-mle-calima-egy-r13    27.2 MB  GPL v2      MLE model for disambiguating Egyptian Arabic\n",
            "disambig-mle-calima-msa-r13    88.7 MB  GPL v2      MLE model for disambiguating Modern Standard Arabic\n",
            "light                                               A minimal set of packages for performing morphological analysis, disambiguation, and dialect identification.\n",
            "morphology-db-all                                   All available morphology databases\n",
            "morphology-db-egy-r13          67.3 MB  GPL v2      Database for analyzing Egyptian Arabic\n",
            "morphology-db-glf-01            8.0 MB  CC BY 4.0   Database for analyzing Gulf Arabic\n",
            "morphology-db-lev-01           10.6 MB  CC BY 4.0   Database for analyzing Levantine Arabic\n",
            "morphology-db-msa-r13          40.5 MB  GPL v2      Database for analyzing Modern Standard Arabic\n",
            "morphology-db-msa-s31          44.8 MB  LDC         Database for analyzing Modern Standard Arabic\n",
            "ner-all                                             All available named-entity-recognition models\n",
            "ner-arabert                   541.6 MB  AraBERT     Named Entity Recognition model generated by fine-tuning AraBERT\n",
            "sentiment-analysis-all                              All available sentiment analysis models\n",
            "sentiment-analysis-arabert    541.6 MB  AraBERT     Sentiment Analysis model generated by fine-tuning AraBERT\n",
            "sentiment-analysis-mbert      712.5 MB  Apache 2.0  Sentiment Analysis model generated by fine-tuning Multilingual BERT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!camel_data -i morphology-db-msa-r13\n",
        "!camel_data -i disambig-mle-calima-msa-r13"
      ],
      "metadata": {
        "id": "Ubb97UkLmWBg",
        "outputId": "f9757399-5e1a-40ca-df5e-c32cc3901a11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ubb97UkLmWBg",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following packages will be installed: 'morphology-db-msa-r13'\n",
            "Downloading package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:00<00:00, 84.2MB/s]\n",
            "Extracting package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:00<00:00, 234MB/s]\n",
            "The following packages will be installed: 'disambig-mle-calima-msa-r13'\n",
            "Downloading package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:00<00:00, 114MB/s]\n",
            "Extracting package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:00<00:00, 151MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "47197d02",
      "metadata": {
        "id": "47197d02"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "# Load the disambiguator once\n",
        "disambig = MLEDisambiguator.pretrained()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "60040256",
      "metadata": {
        "id": "60040256",
        "outputId": "edd394c3-cc92-4c2a-81c0-95cdbaba9e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded and lemmatized 69 transcript segments.\n"
          ]
        }
      ],
      "source": [
        "def preprocess(text, disambig):\n",
        "    tokens = simple_word_tokenize(text)\n",
        "    result = disambig.disambiguate(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for r in result:\n",
        "        if r.analyses:\n",
        "            analysis = r.analyses[0][1]\n",
        "            lemma = analysis.get('lemma', r.word)\n",
        "            lemmas.append(lemma)\n",
        "        else:\n",
        "            lemmas.append(r.word)\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# 📄 Parse and lemmatize transcript file with timecodes\n",
        "def load_transcript(path, disambig):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    segments = []\n",
        "    pattern = re.compile(r\"\\[(\\d+\\.\\d+) - (\\d+\\.\\d+)\\]\\s+(.*)\")\n",
        "    for line in lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            start, end, text = match.groups()\n",
        "            lemmatized = preprocess(text.strip(), disambig)\n",
        "            segments.append({\n",
        "                \"start\": float(start),\n",
        "                \"end\": float(end),\n",
        "                \"text\": text.strip(),\n",
        "                \"lemmas\": lemmatized\n",
        "            })\n",
        "    return segments\n",
        "\n",
        "# ✅ Run\n",
        "transcript_path = [f for f in uploaded if f.endswith(\".txt\")][0]\n",
        "segments = load_transcript(transcript_path, disambig)\n",
        "print(f\"Loaded and lemmatized {len(segments)} transcript segments.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "74bf09b8",
      "metadata": {
        "id": "74bf09b8",
        "outputId": "399f9115-cd07-4410-d102-411b71d9170c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded and lemmatized 54 scene captions.\n"
          ]
        }
      ],
      "source": [
        "def preprocess(text, disambig):\n",
        "    tokens = simple_word_tokenize(text)\n",
        "    result = disambig.disambiguate(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for r in result:\n",
        "        if r.analyses:\n",
        "            analysis = r.analyses[0][1]\n",
        "            lemma = analysis.get('lemma', r.word)\n",
        "            lemmas.append(lemma)\n",
        "        else:\n",
        "            lemmas.append(r.word)\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "def load_captions(path, disambig):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    captions = []\n",
        "    for scene_id, meta in data.items():\n",
        "        scene_time = meta.get(\"scene_time\", \"UNKNOWN\")\n",
        "        arabic_caption = meta.get(\"arabic\", \"\")\n",
        "        lemmatized_caption = preprocess(arabic_caption, disambig)\n",
        "\n",
        "        captions.append({\n",
        "            \"scene_id\": scene_id,\n",
        "            \"scene_time\": scene_time,\n",
        "            \"caption\": arabic_caption,\n",
        "            \"lemmas\": lemmatized_caption\n",
        "        })\n",
        "\n",
        "    return captions\n",
        "\n",
        "# ✅ Run\n",
        "captions_path = [f for f in uploaded if f.endswith(\".json\")][0]\n",
        "captions = load_captions(captions_path, disambig)\n",
        "print(f\"Loaded and lemmatized {len(captions)} scene captions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575758c2",
      "metadata": {
        "id": "575758c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 🔁 Process all transcript segments\n",
        "processed_segments = []\n",
        "for seg in segments:\n",
        "    proc = preprocess(seg[\"text\"])\n",
        "    processed_segments.append({\n",
        "        \"start\": seg[\"start\"],\n",
        "        \"end\": seg[\"end\"],\n",
        "        \"original\": seg[\"text\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(\"processed_transcript.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Saved: processed_transcript.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c6d282",
      "metadata": {
        "id": "56c6d282"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 🔁 Process all captions\n",
        "processed_captions = []\n",
        "for cap in captions:\n",
        "    proc = preprocess(cap[\"caption\"])\n",
        "    processed_captions.append({\n",
        "        \"scene_id\": cap[\"scene_id\"],\n",
        "        \"scene_time\": cap[\"scene_time\"],\n",
        "        \"original\": cap[\"caption\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(\"processed_captions.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Saved: processed_captions.json\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}