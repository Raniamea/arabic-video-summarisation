{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "authorship_tag": "ABX9TyPKc9GWdITua7A4PiY691XA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/04_validate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Transcript Validation using CAMeLBERT-MSA & Arabic Captions\n",
        "Compare a cleaned Arabic ASR transcript against scene captions to improve transcript accuracy.\n",
        "- Uses `diac`, `lemma`, `pos` from transcript segments\n",
        "- Uses Arabic captions generated previously\n",
        "- CAMeLBERT-MSA for semantic validation\n",
        "- Sliding window for misalignment\n",
        "- Outputs: **replace**, **append**, and **flag** transcript versions"
      ],
      "metadata": {
        "id": "oavdO_LK9djT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset environment first\n",
        "!pip uninstall -y torch torchvision torchaudio transformers tokenizers \\\n",
        "  sentence-transformers huggingface_hub camel_tools opencv-python opencv-contrib-python \\\n",
        "  opencv-python-headless numpy\n",
        "\n",
        "# Core installs\n",
        "!pip install --no-cache-dir numpy==1.23.5\n",
        "!pip install --no-cache-dir torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
        "!pip install --no-cache-dir transformers==4.41.2 tokenizers==0.19.1\n",
        "!pip install --no-cache-dir sentence-transformers==2.2.2\n",
        "!pip install --no-cache-dir huggingface_hub==0.23.2 tqdm==4.66.5\n",
        "\n",
        "# âœ… OpenCV\n",
        "!pip install --no-cache-dir opencv-python==4.7.0.72 opencv-contrib-python==4.7.0.72\n",
        "\n",
        "# âœ… CAMeL Tools\n",
        "!pip install --no-cache-dir camel-tools==1.5.2\n"
      ],
      "metadata": {
        "id": "ghSWhb0Tljww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RkpFfleJ_7OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re\n",
        "from bisect import bisect_left, bisect_right\n",
        "from typing import List, Dict, Any, Tuple, Iterable\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# Project Paths\n",
        "# ---------------------------\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "params_path = os.path.join(base_path, \"params.json\")\n",
        "\n",
        "with open(params_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "video_filename = params.get(\"video_file\")\n",
        "video_filename = \"Almasbagha.mp4\"\n",
        "\n",
        "assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_name  = os.path.splitext(video_filename)[0]\n",
        "\n",
        "videos_path      = os.path.join(base_path, \"videos\")\n",
        "captions_path    = os.path.join(base_path, \"captions\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "validated_path   = os.path.join(base_path, \"Validated\")\n",
        "\n",
        "os.makedirs(validated_path, exist_ok=True)\n",
        "\n",
        "caption_path   = os.path.join(captions_path,   f\"{video_name}.json\")\n",
        "transcript_path= os.path.join(preprocessed_path, f\"{video_name}_CleanTranscript.json\")\n",
        "merged_file= os.path.join(validated_path, f\"{video_name}_ScenesIntervalTranscripts.json\")\n",
        "validation_file=os.path.join(validated_path, f\"{video_name}_Validation.json\")\n",
        "validation_results  = os.path.join(validated_path, f\"{video_name}_result.txt\")\n",
        "assert os.path.exists(caption_path),   f\"Missing captions file: {caption_path}\"\n",
        "assert os.path.exists(transcript_path),f\"Missing transcript file: {transcript_path}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Load multilingual SBERT model fine-tuned for semantic similarity\n",
        "# ---------------------------\n",
        "MODEL_ID = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(MODEL_ID, device=device)\n"
      ],
      "metadata": {
        "id": "5fdtGOMN9tx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Allign Scenes and Transcript Segments by timecode and cobine in a single file"
      ],
      "metadata": {
        "id": "0ANvx5Mf4hI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "# --- Load & prep ---\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    transcript_obj = json.load(f)\n",
        "with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "segments = sorted(transcript_obj[\"segments\"], key=lambda s: float(s.get(\"start\", 0.0)))\n",
        "\n",
        "# Build sorted list of (time, caption) pairs\n",
        "scene_items = sorted(\n",
        "    [\n",
        "        (\n",
        "            float(v.get(\"scene_time\", 0.0)),\n",
        "            (v.get(\"arabic\") or v.get(\"english\") or \"\").strip()\n",
        "        )\n",
        "        for v in captions_data.values()\n",
        "    ],\n",
        "    key=lambda x: x[0]\n",
        ")\n",
        "\n",
        "scene_times = [t for t, _ in scene_items]\n",
        "scene_caps  = [c for _, c in scene_items]\n",
        "\n",
        "# last scene ends at max transcript end\n",
        "max_end = max((float(s.get(\"end\", 0.0)) for s in segments), default=0.0)\n",
        "\n",
        "# --- Build scene -> transcript alignment across intervals ---\n",
        "out = []\n",
        "n_segs = len(segments)\n",
        "seg_idx = 0  # pointer sweep over transcript segments (sorted by start)\n",
        "\n",
        "for i, scene_start in enumerate(scene_times):\n",
        "    scene_end = scene_times[i+1] if (i + 1) < len(scene_times) else max_end\n",
        "    caption_text = scene_caps[i]\n",
        "\n",
        "    # Advance pointer past segments that end before the scene starts\n",
        "    while seg_idx < n_segs and float(segments[seg_idx].get(\"end\", 0.0)) <= scene_start:\n",
        "        seg_idx += 1\n",
        "\n",
        "    # Collect all segments that overlap [scene_start, scene_end)\n",
        "    cur_texts, cur_lemmas, cur_pos = [], [], []\n",
        "    j = seg_idx\n",
        "    while j < n_segs:\n",
        "        s = segments[j]\n",
        "        s_start = float(s.get(\"start\", 0.0))\n",
        "        s_end   = float(s.get(\"end\", 0.0))\n",
        "\n",
        "        if s_start >= scene_end:\n",
        "            break  # beyond interval\n",
        "\n",
        "        if (s_end > scene_start) and (s_start < scene_end):\n",
        "            cur_texts.append(s.get(\"text_norm\") or s.get(\"original\", \"\") or \"\")\n",
        "            cur_lemmas.extend(s.get(\"lemmas\", []))\n",
        "            cur_pos.extend(s.get(\"pos\", []))\n",
        "\n",
        "        j += 1\n",
        "\n",
        "    out.append({\n",
        "        \"scene_index\": i,\n",
        "        \"scene_start\": scene_start,\n",
        "        \"scene_end\": scene_end,\n",
        "        \"caption\": caption_text,\n",
        "        \"transcript_text\": \" \".join(t for t in cur_texts if t).strip(),\n",
        "        \"lemmas\": cur_lemmas,\n",
        "        \"pos\": cur_pos\n",
        "    })\n",
        "\n",
        "# --- Save ---\n",
        "with open(merged_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"scenes\": out}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Saved {len(out)} scenes:\", merged_file)\n"
      ],
      "metadata": {
        "id": "yETAg23UFWCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine scenes & captions that have empty Transcript text"
      ],
      "metadata": {
        "id": "Y9SNuadP46qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "\n",
        "with open(merged_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "scenes = data[\"scenes\"]\n",
        "\n",
        "merged = []\n",
        "buffer = None  # to collect consecutive empty scenes\n",
        "\n",
        "for scene in scenes:\n",
        "    has_text = bool(scene.get(\"transcript_text\", \"\").strip())\n",
        "\n",
        "    if not has_text:\n",
        "        if buffer is None:\n",
        "            buffer = dict(scene)  # start new empty block\n",
        "        else:\n",
        "            buffer[\"scene_end\"] = scene[\"scene_end\"]\n",
        "            buffer[\"caption\"] = (buffer.get(\"caption\", \"\") + \" \" + scene.get(\"caption\", \"\")).strip()\n",
        "            # transcript_text/lemmas/pos stay empty\n",
        "    else:\n",
        "        if buffer:\n",
        "            merged.append(buffer)\n",
        "            buffer = None\n",
        "        merged.append(scene)\n",
        "\n",
        "# Flush if leftover empty buffer\n",
        "if buffer:\n",
        "    merged.append(buffer)\n",
        "\n",
        "# Reindex\n",
        "for i, scene in enumerate(merged):\n",
        "    scene[\"scene_index\"] = i\n",
        "\n",
        "# --- Overwrite the same file ---\n",
        "data[\"scenes\"] = merged\n",
        "with open(merged_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Consecutive empty transcript scenes merged.\")\n",
        "print(f\"   Original scenes: {len(scenes)} -> Updated scenes: {len(merged)}\")\n",
        "print(\"ðŸ’¾ File updated in place:\", merged_file)\n"
      ],
      "metadata": {
        "id": "pSpIrb5T6sCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "QmyfSdfj-zRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os, re\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sentence_transformers import util\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers\n",
        "# ---------------------------\n",
        "\n",
        "AR_STOPWORDS = {\n",
        "    \"Ùˆ\",\"ÙÙŠ\",\"Ø¹Ù„Ù‰\",\"Ù…Ù†\",\"Ø¥Ù„Ù‰\",\"Ø¹Ù†\",\"Ø£Ù†\",\"Ø¥Ù†\",\"ÙƒØ§Ù†\",\"ÙƒØ§Ù†Øª\",\"ÙŠÙƒÙˆÙ†\",\"Ù…Ø¹\",\"Ù‡Ø°Ø§\",\"Ù‡Ø°Ù‡\",\n",
        "    \"Ø°Ù„Ùƒ\",\"ØªÙ„Ùƒ\",\"Ù‡Ù†Ø§Ùƒ\",\"Ù‡Ù†Ø§\",\"Ù‡Ùˆ\",\"Ù‡ÙŠ\",\"Ù‡Ù…\",\"Ù‡Ù†\",\"ÙƒÙ…Ø§\",\"Ù„ÙƒÙ†\",\"Ø¨Ù„\",\"Ù‚Ø¯\",\"ØªÙ…\",\"Ø«Ù…\",\n",
        "    \"ÙƒÙ„\",\"Ø£ÙŠ\",\"Ø£Ùˆ\",\"Ø£Ù…Ø§Ù…\",\"Ø®Ù„Ø§Ù„\",\"Ø¨Ø¹Ø¯\",\"Ù‚Ø¨Ù„\",\"Ø­ØªÙ‰\",\"Ø­ÙŠØ«\",\"Ø¥Ø°Ø§\",\"Ø¥Ù†Ù…Ø§\",\"Ø¥Ù…Ø§\",\"Ù„Ø¯Ù‰\",\n",
        "    \"Ù„Ø¯ÙŠ\",\"Ù„Ù‡Ø§\",\"Ù„Ù‡\",\"Ù„Ù‡Ù…\",\"Ù„Ù†Ø§\",\"Ù…Ø§\",\"Ù…Ø§Ø°Ø§\",\"Ù„Ù…Ø§Ø°Ø§\",\"ÙƒÙŠÙ\",\"Ù…ØªÙ‰\",\"Ø£ÙŠØ¶Ø§\",\"Ø¨Ø¯ÙˆÙ†\",\"Ø£Ù…Ø§Ù…\",\n",
        "    \"Ø¯Ø§Ø®Ù„\",\"Ø®Ø§Ø±Ø¬\",\"Ø¨ÙŠÙ†\",\"Ø£ÙƒØ«Ø±\",\"Ø£Ù‚Ù„\"\n",
        "}\n",
        "\n",
        "def ar_normalize(text: str) -> str:\n",
        "    \"\"\"Light Arabic normalization (remove diacritics, unify alef/ya/ta marbuta).\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\", \"\", text)\n",
        "    text = text.replace(\"Ø£\",\"Ø§\").replace(\"Ø¥\",\"Ø§\").replace(\"Ø¢\",\"Ø§\")\n",
        "    text = text.replace(\"Ø©\",\"Ù‡\").replace(\"Ù‰\",\"ÙŠ\")\n",
        "    return text\n",
        "\n",
        "def ar_tokens(text: str):\n",
        "    \"\"\"Tokenize + normalize Arabic text.\"\"\"\n",
        "    tokens = re.split(r\"\\s+\", ar_normalize(text))\n",
        "    return [t for t in tokens if t and t not in AR_STOPWORDS]\n",
        "\n",
        "def lexical_overlap(seg_lemmas, cap_tokens, seg_tokens, use_ngrams=True):\n",
        "    \"\"\"Compute lexical overlap score between transcript and caption tokens.\"\"\"\n",
        "    seg_set = set(seg_lemmas) | set(seg_tokens)\n",
        "    cap_set = set()\n",
        "    for toks in cap_tokens:\n",
        "        cap_set |= set(toks)\n",
        "        if use_ngrams:\n",
        "            cap_set |= set(\"\".join(toks[i:i+2]) for i in range(len(toks)-1))\n",
        "    if not seg_set or not cap_set:\n",
        "        return 0.0\n",
        "    return len(seg_set & cap_set) / len(seg_set)\n",
        "\n",
        "def fusion_score(lexical, cosine, alpha=0.5):\n",
        "    \"\"\"Weighted fusion of lexical and cosine scores.\"\"\"\n",
        "    return alpha*lexical + (1-alpha)*cosine\n",
        "\n",
        "def is_propn(pos_tag):\n",
        "    \"\"\"Check if POS tag indicates proper noun.\"\"\"\n",
        "    return \"PROPN\" in pos_tag.upper()\n",
        "\n",
        "def should_backoff_too_much_removed(kept_words, dropped_words, max_removed_ratio=0.3):\n",
        "    \"\"\"If too many words would be dropped, keep original segment.\"\"\"\n",
        "    total = len(kept_words) + len(dropped_words)\n",
        "    if total == 0:\n",
        "        return True\n",
        "    return (len(dropped_words) / total) > max_removed_ratio\n",
        "\n",
        "# ---------------------------\n",
        "# Main validation\n",
        "# ---------------------------\n",
        "\n",
        "def validate_words_by_visual_support(\n",
        "    scenes,\n",
        "    model,\n",
        "    alpha_fusion: float = 0.5,\n",
        "    sim_threshold: float = 0.55,\n",
        "    min_word_len: int = 3,\n",
        "    propn_keep_margin: float = 0.15,\n",
        "    backoff_removed_ratio: float = 0.30\n",
        "):\n",
        "    enriched = []\n",
        "    kept_total, dropped_total = 0, 0\n",
        "\n",
        "    for scene in tqdm(scenes):\n",
        "        seg_text = scene.get(\"transcript_text\", \"\").strip()\n",
        "        seg_lemmas = scene.get(\"lemmas\", [])\n",
        "        seg_pos = scene.get(\"pos\", [])\n",
        "        caption = scene.get(\"caption\", \"\").strip()\n",
        "\n",
        "        seg_out = dict(scene)\n",
        "        seg_out[\"validated_text\"] = seg_text\n",
        "        seg_out[\"support_score\"] = 0.0\n",
        "        seg_out[\"validated\"] = False\n",
        "\n",
        "        if caption and seg_text:\n",
        "            # embeddings\n",
        "            with torch.no_grad():\n",
        "                seg_emb = model.encode(seg_text, convert_to_tensor=True)\n",
        "                cap_emb = model.encode(caption, convert_to_tensor=True)\n",
        "                cos = float(util.cos_sim(seg_emb, cap_emb).item())\n",
        "\n",
        "            # lexical overlap\n",
        "            cap_tokens = [ar_tokens(caption)]\n",
        "            lex = lexical_overlap(seg_lemmas, cap_tokens, ar_tokens(seg_text), use_ngrams=True)\n",
        "\n",
        "            # fused score\n",
        "            fused = fusion_score(lex, cos, alpha=alpha_fusion)\n",
        "\n",
        "            # keep/drop decision\n",
        "            raw_words = [w for w in re.sub(r\"\\s+\", \" \", seg_text).split(\" \") if w]\n",
        "            kept_words, dropped_words = [], []\n",
        "\n",
        "            for i, w in enumerate(raw_words):\n",
        "                w_norm = ar_normalize(w)\n",
        "                if len(w_norm) < min_word_len or w_norm in AR_STOPWORDS:\n",
        "                    kept_words.append(w)\n",
        "                    continue\n",
        "\n",
        "                # proper noun safeguard\n",
        "                is_name = False\n",
        "                if isinstance(seg_pos, list) and i < len(seg_pos):\n",
        "                    is_name = is_propn(str(seg_pos[i]))\n",
        "\n",
        "                thr = sim_threshold - (propn_keep_margin if is_name else 0.0)\n",
        "                if fused >= thr:\n",
        "                    kept_words.append(w)\n",
        "                else:\n",
        "                    dropped_words.append(w)\n",
        "\n",
        "            # backoff if too many drops\n",
        "            if should_backoff_too_much_removed(kept_words, dropped_words, max_removed_ratio=backoff_removed_ratio):\n",
        "                kept_words = raw_words\n",
        "                dropped_words = []\n",
        "\n",
        "            validated_text = \" \".join(kept_words)\n",
        "\n",
        "            seg_out[\"scores\"] = {\"cosine\": cos, \"lexical\": lex, \"fused\": fused}\n",
        "            seg_out[\"validated_text\"] = validated_text\n",
        "            seg_out[\"dropped_words\"] = dropped_words\n",
        "            seg_out[\"kept_words\"] = kept_words\n",
        "            seg_out[\"support_score\"] = fused\n",
        "            seg_out[\"validated\"] = fused >= sim_threshold\n",
        "\n",
        "            kept_total += len(kept_words)\n",
        "            dropped_total += len(dropped_words)\n",
        "\n",
        "        enriched.append(seg_out)\n",
        "\n",
        "    summary = {\n",
        "        \"scenes\": len(enriched),\n",
        "        \"kept_words\": kept_total,\n",
        "        \"dropped_words\": dropped_total,\n",
        "        \"params\": {\n",
        "            \"alpha_fusion\": alpha_fusion,\n",
        "            \"sim_threshold\": sim_threshold,\n",
        "            \"min_word_len\": min_word_len,\n",
        "            \"propn_keep_margin\": propn_keep_margin,\n",
        "            \"backoff_removed_ratio\": backoff_removed_ratio\n",
        "        }\n",
        "    }\n",
        "    return enriched, summary\n",
        "\n",
        "# ---------------------------\n",
        "# Run on your aligned file\n",
        "# ---------------------------\n",
        "\n",
        "\n",
        "with open(merged_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "scenes = data[\"scenes\"]\n",
        "\n",
        "enriched, stats = validate_words_by_visual_support(\n",
        "    scenes=scenes,\n",
        "    model=model,       # assumes SentenceTransformer model is already loaded\n",
        "    alpha_fusion=0.5,\n",
        "    sim_threshold=0.55,\n",
        "    min_word_len=3,\n",
        "    propn_keep_margin=0.15,\n",
        "    backoff_removed_ratio=0.30\n",
        ")\n",
        "\n",
        "print(\"Summary:\", stats)\n",
        "\n",
        "# Save\n",
        "\n",
        "with open(validation_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"summary\": stats, \"scenes\": enriched}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Saved validated file: {validation_file}\")\n"
      ],
      "metadata": {
        "id": "MmI16EKG-3KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sentence_transformers import util, SentenceTransformer\n",
        "\n",
        "# ---------------------------\n",
        "# Validation function\n",
        "# ---------------------------\n",
        "def validate_arabic_embeddings(\n",
        "    scenes,\n",
        "    model,\n",
        "    sim_threshold: float = 0.35,   # lower threshold for Arabic cosine\n",
        "    propn_keep_margin: float = 0.15\n",
        "):\n",
        "    enriched = []\n",
        "\n",
        "    for scene in tqdm(scenes):\n",
        "        seg_text = scene.get(\"transcript_text\", \"\").strip()\n",
        "        seg_pos  = scene.get(\"pos\", [])\n",
        "        caption  = scene.get(\"caption\", \"\").strip()\n",
        "\n",
        "        scene_out = dict(scene)\n",
        "        scene_out[\"validated_text\"] = seg_text\n",
        "        scene_out[\"support_score\"] = 0.0\n",
        "        scene_out[\"validated\"] = False\n",
        "\n",
        "        if caption and seg_text:\n",
        "            with torch.no_grad():\n",
        "                seg_emb = model.encode(seg_text, convert_to_tensor=True)\n",
        "                cap_emb = model.encode(caption, convert_to_tensor=True)\n",
        "                cos = float(util.cos_sim(seg_emb, cap_emb).item())\n",
        "\n",
        "            scene_out[\"support_score\"] = cos\n",
        "\n",
        "            thr = sim_threshold - (propn_keep_margin if any(\"PROPN\" in str(p).upper() for p in seg_pos) else 0.0)\n",
        "            if cos >= thr:\n",
        "                scene_out[\"validated\"] = True\n",
        "\n",
        "        enriched.append(scene_out)\n",
        "\n",
        "    summary = {\n",
        "        \"scenes\": len(enriched),\n",
        "        \"params\": {\n",
        "            \"model\": model_name,\n",
        "            \"sim_threshold\": sim_threshold,\n",
        "            \"propn_keep_margin\": propn_keep_margin,\n",
        "        }\n",
        "    }\n",
        "    return enriched, summary\n",
        "\n",
        "# ---------------------------\n",
        "# Run validation on aligned file\n",
        "# ---------------------------\n",
        "with open(merged_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "scenes = data[\"scenes\"]\n",
        "\n",
        "enriched, stats = validate_arabic_embeddings(\n",
        "    scenes=scenes,\n",
        "    model=model,\n",
        "    sim_threshold=0.35,   # tune between 0.25â€“0.4\n",
        "    propn_keep_margin=0.15\n",
        ")\n",
        "\n",
        "print(\"Summary:\", stats)\n",
        "\n",
        "# Save\n",
        "\n",
        "with open(validation_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"summary\": stats, \"scenes\": enriched}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… Saved validated file: {validation_file}\")\n"
      ],
      "metadata": {
        "id": "UXnuXh0aC_Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "# ==== CONFIG ====\n",
        "THRESH = 0.30  # \"exceed 0.3\" -> use > 0.30\n",
        "\n",
        "# ==== LOAD ====\n",
        "with open(validation_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# The structure is assumed to be: {\"summary\": {...}, \"scenes\": [ ... ]}\n",
        "scenes = data.get(\"scenes\", data)  # fallback if file is just a list\n",
        "\n",
        "# ==== COLLECT ====\n",
        "selected = []\n",
        "for s in scenes:\n",
        "    score = float(s.get(\"support_score\", 0.0))\n",
        "    text  = (s.get(\"transcript_text\") or \"\").strip()\n",
        "    if score > THRESH and text:\n",
        "        selected.append(text)\n",
        "\n",
        "collective_text = \" \".join(selected).strip()\n",
        "\n",
        "# ==== SAVE ====\n",
        "with open(validation_results, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(collective_text)\n",
        "\n",
        "print(f\"âœ… Scenes in file: {len(scenes)}\")\n",
        "print(f\"âœ… Selected transcripts (> {THRESH}): {len(selected)}\")\n",
        "print(\"ðŸ’¾ Saved collective text to:\", validation_results)\n",
        "print(\"\\nPreview:\\n\", collective_text[:400] + (\"...\" if len(collective_text) > 400 else \"\"))\n"
      ],
      "metadata": {
        "id": "WOA9kDO3Ww54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}