{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/04_validate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oavdO_LK9djT"
      },
      "source": [
        "#  Transcript Validation using ASR Generated Transcript & Arabic Captions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJZes0KgR5_i"
      },
      "outputs": [],
      "source": [
        "!pip install -q --no-cache-dir torch==2.6.0\n",
        "!pip install -q --no-cache-dir transformers==4.44.2 sentence-transformers==2.6.1\n",
        "!pip install -q --no-cache-dir numpy==1.26.4 tqdm==4.67.1\n",
        "!pip install -q torch torchvision torchaudio sentence-transformers evaluate rouge-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RkpFfleJ_7OB"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Mount Google Drive and define base path\n",
        "# =========================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.ismount(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# Define base path for project files\n",
        "BASE_PATH = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Obtain Video File Name\n",
        "# =========================================================\n",
        "import os, json\n",
        "\n",
        "params_path = os.path.join(BASE_PATH, \"params.json\")\n",
        "\n",
        "#with open(params_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#    params = json.load(f)\n",
        "\n",
        "#video_filename = params.get(\"video_file\")\n",
        "#assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_filename=\"Almasbagha.mp4\"\n",
        "\n",
        "video_name  = os.path.splitext(video_filename)[0]"
      ],
      "metadata": {
        "id": "BrxhEq3I2FK3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Define File Paths & Names\n",
        "# =========================================================\n",
        "videos_path      = os.path.join(BASE_PATH, \"videos\")\n",
        "captions_path    = os.path.join(BASE_PATH, \"captions\")\n",
        "preprocessed_path= os.path.join(BASE_PATH, \"Preprocessed\")\n",
        "validated_path   = os.path.join(BASE_PATH, \"Validated\")\n",
        "os.makedirs(validated_path, exist_ok=True)\n",
        "\n",
        "caption_path   = os.path.join(captions_path,   f\"{video_name}.json\")\n",
        "transcript_path= os.path.join(preprocessed_path, f\"{video_name}_CleanTranscript.json\")\n",
        "merged_file= os.path.join(validated_path, f\"{video_name}_ScenesIntervalTranscripts_WA.json\")\n",
        "strict_file= os.path.join(validated_path, f\"{video_name}_ScenesIntervalTranscripts_SA.json\")\n",
        "validated_alignment_WA=os.path.join(validated_path, f\"{video_name}_Validated_WA.json\")\n",
        "validated_alignment_SA=os.path.join(validated_path, f\"{video_name}_Validated_SA.json\")\n",
        "validated_alignment=os.path.join(validated_path, f\"{video_name}_Validated.json\")\n",
        "validated_result=os.path.join(validated_path, f\"{video_name}_Validated.txt\")\n",
        "assert os.path.exists(caption_path),   f\"Missing captions file: {caption_path}\"\n",
        "assert os.path.exists(transcript_path),f\"Missing transcript file: {transcript_path}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "X_QT-vxh13Us"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5fdtGOMN9tx-"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Load multilingual SBERT model\n",
        "# ---------------------------\n",
        "import os, json, re\n",
        "from bisect import bisect_left, bisect_right\n",
        "from typing import List, Dict, Any, Tuple, Iterable\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "MODEL_ID = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(MODEL_ID, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Allignment"
      ],
      "metadata": {
        "id": "lxa-NvP3Zgd2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRLZKzGy6C6O"
      },
      "source": [
        "## Strict Allignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l7NUSL745rYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f78c0a-2125-4313-9736-db297683ff58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strict alignment saved: /content/drive/MyDrive/ArabicVideoSummariser/Validated/Almasbagha_ScenesIntervalTranscripts_SA.json\n",
            "Updated captions for 81 transcript segments.\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "\n",
        "# --- Load & prep ---\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    transcripts = json.load(f)\n",
        "\n",
        "with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# --- Extract scene times and texts from the caption file ---\n",
        "scene_times = []\n",
        "scene_texts_en = []\n",
        "scene_texts_ar = []\n",
        "\n",
        "for key, val in captions_data.items():\n",
        "    if \"scene_time\" in val:\n",
        "        scene_times.append(float(val[\"scene_time\"]))\n",
        "        scene_texts_en.append((val.get(\"english\") or \"\").strip())\n",
        "        scene_texts_ar.append((val.get(\"arabic\") or \"\").strip())\n",
        "\n",
        "# --- Add captions to each transcript segment ---\n",
        "for segment in transcripts[\"segments\"]:\n",
        "    start = float(segment.get(\"start\", 0))\n",
        "    end   = float(segment.get(\"end\", 0))\n",
        "\n",
        "    in_window = [i for i, t in enumerate(scene_times) if start <= t < end]\n",
        "    en_strings = [scene_texts_en[i] for i in in_window] if in_window else []\n",
        "    ar_strings = [scene_texts_ar[i] for i in in_window] if in_window else []\n",
        "\n",
        "    # joined strings + raw lists (non-breaking additions)\n",
        "    segment[\"captions_en\"] = \" \".join([s for s in en_strings if s]).strip()\n",
        "    segment[\"captions_ar\"] = \" \".join([s for s in ar_strings if s]).strip()\n",
        "\n",
        "# --- Save the updated transcript ---\n",
        "with open(strict_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(transcripts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Strict alignment saved: {strict_file}\")\n",
        "print(f\"Updated captions for {len(transcripts['segments'])} transcript segments.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nPhDQ1E5SG9"
      },
      "source": [
        "## Scene-Based Sliding-Window Alignment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# --- Load data ---\n",
        "with open(transcript_path, 'r', encoding='utf-8') as f:\n",
        "    transcripts = json.load(f)\n",
        "with open(caption_path, 'r', encoding='utf-8') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# --- Build sorted caption lists ---\n",
        "scene_items = sorted(\n",
        "    (\n",
        "        (\n",
        "            float(item['scene_time']),\n",
        "            (item.get('english') or '').strip(),\n",
        "            (item.get('arabic')  or '').strip(),\n",
        "        )\n",
        "        for item in captions_data.values()\n",
        "    ),\n",
        "    key=lambda x: x[0]\n",
        ")\n",
        "\n",
        "scene_times = [t for t, _, _ in scene_items]\n",
        "scene_texts_en = [en for _, en, _ in scene_items]\n",
        "scene_texts_ar = [ar for _, _, ar in scene_items]\n",
        "\n",
        "# --- Align captions with transcript segments ---\n",
        "for segment in transcripts['segments']:\n",
        "    start = float(segment.get('start', 0))\n",
        "    end   = float(segment.get('end', 0))\n",
        "\n",
        "    # Find all captions that fall within this segment’s time window\n",
        "    in_window = [i for i, t in enumerate(scene_times) if start <= t < end]\n",
        "\n",
        "    gathered_en, gathered_ar = [], []\n",
        "\n",
        "    # Always include immediate previous and next captions by index\n",
        "    if in_window:\n",
        "        first_idx = min(in_window)\n",
        "        last_idx  = max(in_window)\n",
        "    else:\n",
        "        # If no captions fall in the window, pick nearest caption by time\n",
        "        nearest_idx = min(range(len(scene_times)), key=lambda i: abs(scene_times[i] - start))\n",
        "        first_idx = last_idx = nearest_idx\n",
        "\n",
        "    # Caption immediately before\n",
        "    prev_idx = first_idx - 1 if first_idx > 0 else None\n",
        "    if prev_idx is not None:\n",
        "        gathered_en.append(scene_texts_en[prev_idx])\n",
        "        gathered_ar.append(scene_texts_ar[prev_idx])\n",
        "\n",
        "    # Captions inside the window\n",
        "    gathered_en.extend(scene_texts_en[i] for i in range(first_idx, last_idx + 1))\n",
        "    gathered_ar.extend(scene_texts_ar[i] for i in range(first_idx, last_idx + 1))\n",
        "\n",
        "    # Caption immediately after\n",
        "    next_idx = last_idx + 1 if last_idx + 1 < len(scene_texts_en) else None\n",
        "    if next_idx is not None:\n",
        "        gathered_en.append(scene_texts_en[next_idx])\n",
        "        gathered_ar.append(scene_texts_ar[next_idx])\n",
        "\n",
        "    # Join captions\n",
        "    segment['captions_en'] = ' '.join([s for s in gathered_en if s]).strip()\n",
        "    segment['captions_ar'] = ' '.join([s for s in gathered_ar if s]).strip()\n",
        "\n",
        "# --- Save updated file ---\n",
        "with open(merged_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(transcripts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Updated captions for {len(transcripts['segments'])} transcript segments\")\n",
        "print(f\"Saved to {merged_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rovY8qJSLUF5",
        "outputId": "715662dd-cb34-4e55-b589-06b09250163a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated captions for 81 transcript segments\n",
            "Saved to /content/drive/MyDrive/ArabicVideoSummariser/Validated/Almasbagha_ScenesIntervalTranscripts_WA.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "dcSaT9HOQ-1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# CONFIG\n",
        "# ==============================\n",
        "KEEP_MARGIN = 0.15\n",
        "ALFA_FUSION = 0.35\n",
        "SIM_THRESHOLD = 0.25\n"
      ],
      "metadata": {
        "id": "xBx1xYJKMj_d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Validation"
      ],
      "metadata": {
        "id": "-3sOtjNTRIIE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmyfSdfj-zRm"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "SC0Ez9-sX18Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "# ==============================\n",
        "# Arabic  ROUGE\n",
        "# ==============================\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "\n",
        "def arabic_rouge_score(pred_text, ref_text):\n",
        "    from evaluate import load\n",
        "    rouge_metric = load(\"rouge\")\n",
        "    scores = rouge_metric.compute(\n",
        "        predictions=[pred_text],\n",
        "        references=[ref_text],\n",
        "        tokenizer=lambda x: list(x)\n",
        "    )\n",
        "    return (scores[\"rouge1\"] + scores[\"rouge2\"] + scores[\"rougeL\"]) / 3.0\n",
        "\n",
        "def compute_fusion_score(lexical, cosine):\n",
        "    return ALFA_FUSION * lexical + (1 - ALFA_FUSION) * cosine\n",
        "\n",
        "# ==============================\n",
        "# Field getters\n",
        "# ==============================\n",
        "def first_nonempty(rec, keys):\n",
        "    for k in keys:\n",
        "        v = rec.get(k)\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            return v.strip()\n",
        "    return \"\"\n",
        "\n",
        "def get_seg_text(rec):\n",
        "    return first_nonempty(rec, [\n",
        "        \"transcript_text\", \"text_norm\", \"original\", \"text\", \"segment_text\"\n",
        "    ])\n",
        "\n",
        "def get_caption_en(rec):\n",
        "    \"\"\"English captions for semantic validation.\"\"\"\n",
        "    return first_nonempty(rec, [\n",
        "        \"captions_en\", \"english_caption\", \"caption_en\", \"captionsEnglish\"\n",
        "    ])\n",
        "\n",
        "def get_caption_ar(rec):\n",
        "    \"\"\"Arabic captions for lexical validation.\"\"\"\n",
        "    return first_nonempty(rec, [\n",
        "        \"captions_ar\", \"caption_ar\", \"captionsArabic\", \"captions\", \"scene_caption\",\n",
        "        \"visual_caption\", \"blip_caption\", \"clip_caption\", \"yolo_caption\"\n",
        "    ])\n",
        "\n",
        "def get_lemma_text(rec):\n",
        "    lemmas = rec.get(\"lemmas\")\n",
        "    if isinstance(lemmas, list) and lemmas:\n",
        "        return \" \".join(map(str, lemmas))\n",
        "    return \"\"\n",
        "\n",
        "# ==============================\n",
        "# Stopwords & POS helpers\n",
        "# ==============================\n",
        "AR_STOPWORDS = {\n",
        "    \"و\",\"في\",\"على\",\"من\",\"إلى\",\"عن\",\"أن\",\"إن\",\"كان\",\"كانت\",\"يكون\",\"مع\",\"هذا\",\"هذه\",\n",
        "    \"ذلك\",\"تلك\",\"هناك\",\"هنا\",\"هو\",\"هي\",\"هم\",\"هن\",\"كما\",\"لكن\",\"بل\",\"قد\",\"تم\",\"ثم\",\n",
        "    \"كل\",\"أي\",\"أو\",\"أمام\",\"خلال\",\"بعد\",\"قبل\",\"حتى\",\"حيث\",\"إذا\",\"إنما\",\"إما\",\"لدى\",\n",
        "    \"لدي\",\"لها\",\"له\",\"لهم\",\"لنا\",\"ما\",\"ماذا\",\"لماذا\",\"كيف\",\"متى\",\"أيضا\",\"بدون\",\"أمام\",\n",
        "    \"داخل\",\"خارج\",\"بين\",\"أكثر\",\"أقل\"\n",
        "}\n",
        "\n",
        "def is_propn(pos_tag):\n",
        "    return \"NOUN\" in str(pos_tag).upper()\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Safe JSON serialization\n",
        "# ==============================\n",
        "def to_serializable(obj):\n",
        "    if isinstance(obj, (np.integer,)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.floating,)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, (np.bool_,)):\n",
        "        return bool(obj)\n",
        "    elif isinstance(obj, torch.Tensor):\n",
        "        return obj.item()\n",
        "    return str(obj)\n"
      ],
      "metadata": {
        "id": "mccbFU8MXx_m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Validation Function"
      ],
      "metadata": {
        "id": "_-b7FxHYghW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Main validation\n",
        "# ==============================\n",
        "def validate_words_by_visual_support(\n",
        "    records\n",
        "):\n",
        "    enriched = []\n",
        "\n",
        "    for rec in tqdm(records):\n",
        "        seg_text = get_seg_text(rec)\n",
        "        seg_lemma_text = get_lemma_text(rec) or seg_text\n",
        "\n",
        "        cap_en = get_caption_en(rec)   # English for semantic\n",
        "        cap_ar = get_caption_ar(rec)   # Arabic for lexical\n",
        "\n",
        "        rec_out = dict(rec)\n",
        "        rec_out[\"validated_text\"] = seg_text\n",
        "        rec_out[\"cosine_score\"] = 0.0\n",
        "\n",
        "        # --- Semantic similarity ---\n",
        "        if cap_en and seg_lemma_text:\n",
        "            with torch.no_grad():\n",
        "                seg_emb = model.encode(seg_lemma_text, convert_to_tensor=True)\n",
        "                cap_emb = model.encode(cap_en, convert_to_tensor=True)\n",
        "                cosine_val = float(util.cos_sim(seg_emb, cap_emb).item())\n",
        "            rec_out[\"cosine_score\"] = cosine_val\n",
        "        else:\n",
        "            cosine_val = 0.0\n",
        "\n",
        "        # --- Lexical similarity ---\n",
        "        tokens = rec.get(\"tokens\", [])\n",
        "        seg_joined = \" \".join(tokens) if tokens else seg_lemma_text\n",
        "        lex = arabic_rouge_score(seg_joined, cap_ar) if cap_ar and seg_joined else 0.0\n",
        "\n",
        "\n",
        "        # --- Fusion score ---\n",
        "        fused = compute_fusion_score(lex, cosine_val)\n",
        "        rec_out[\"scores\"] = {\n",
        "            \"cosine\": float(cosine_val),\n",
        "            \"lexical_rouge\": float(lex),\n",
        "            \"fused\": float(fused)\n",
        "        }\n",
        "\n",
        "        #Named Entity flag (using CAMeL NER output) + margin value ---\n",
        "        named_entities = rec.get(\"named_entities\") or []\n",
        "        has_named_entity = bool(named_entities)  # True if any named entity present\n",
        "        #Numeric margin per record -> KEEP_MARGIN if NOUN present, else 0.0\n",
        "        rec_out[\"Propn_Keep_Margin\"] = float(KEEP_MARGIN) if has_named_entity else 0.0\n",
        "\n",
        "        enriched.append(rec_out)\n",
        "\n",
        "    summary = {\n",
        "        \"records\": len(enriched),\n",
        "        \"Proper_Keep_true\": sum(1 for r in enriched if r.get(\"Proper_Keep\")),\n",
        "        \"params\": {\n",
        "            \"alpha_fusion\": ALFA_FUSION,\n",
        "            \"keep_margin\": KEEP_MARGIN,\n",
        "            \"model\": MODEL_ID\n",
        "        }\n",
        "    }\n",
        "    return enriched, summary\n"
      ],
      "metadata": {
        "id": "o-8GgBx-geSA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Stict Allignment Validation"
      ],
      "metadata": {
        "id": "2k26fuZH9H3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(strict_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "records = data.get(\"scenes\") or data.get(\"segments\") or data\n",
        "\n",
        "enriched_records, stats = validate_words_by_visual_support(\n",
        "    records=records\n",
        ")\n",
        "\n",
        "# --- Save full output---\n",
        "with open(validated_alignment_SA, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(\n",
        "        {\"summary\": stats, \"records\": enriched_records},\n",
        "        f,\n",
        "        ensure_ascii=False,\n",
        "        indent=2,\n",
        "        default=to_serializable\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Summary:\", stats)\n",
        "\n",
        "print(f\"Strict Allignment Validation complete : {validated_alignment_SA}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGZWlD4vqfwg",
        "outputId": "b22f220d-7dea-4fed-e0cb-a61821529497"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [00:45<00:00,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: {'records': 81, 'Proper_Keep_true': 0, 'params': {'alpha_fusion': 0.35, 'keep_margin': 0.15, 'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'}}\n",
            "Strict Allignment Validation complete : /content/drive/MyDrive/ArabicVideoSummariser/Validated/Almasbagha_Validated_SA.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scene Window Allignment Validation"
      ],
      "metadata": {
        "id": "AsH51wW1AGa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(merged_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "records = data.get(\"scenes\") or data.get(\"segments\") or data\n",
        "\n",
        "enriched_records, stats = validate_words_by_visual_support(\n",
        "    records=records,\n",
        ")\n",
        "\n",
        "print(\"Summary:\", stats)\n",
        "\n",
        "# --- Save full output ---\n",
        "with open(validated_alignment_WA, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(\n",
        "        {\"summary\": stats, \"records\": enriched_records},\n",
        "        f,\n",
        "        ensure_ascii=False,\n",
        "        indent=2,\n",
        "        default=to_serializable\n",
        "    )\n",
        "\n",
        "print(f\"Window Allignment Validation complete : {validated_alignment_WA}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ1Kxy55vC36",
        "outputId": "a5b3b949-5db0-430d-95b5-14c4623cbf39"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:54<00:00,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: {'records': 81, 'Proper_Keep_true': 0, 'params': {'alpha_fusion': 0.35, 'keep_margin': 0.15, 'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'}}\n",
            "Window Allignment Validation complete : /content/drive/MyDrive/ArabicVideoSummariser/Validated/Almasbagha_Validated_WA.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining & Final Output"
      ],
      "metadata": {
        "id": "u1ezkvK1AUpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Combine Strict + Window by Highest Fusion\n",
        "# ==========================================\n",
        "import json, os\n",
        "\n",
        "# --- Load both validated alignment files ---\n",
        "with open(validated_alignment_SA, \"r\", encoding=\"utf-8\") as f:\n",
        "    strict_data = json.load(f)\n",
        "with open(validated_alignment_WA, \"r\", encoding=\"utf-8\") as f:\n",
        "    window_data = json.load(f)\n",
        "\n",
        "# --- Extract record lists ---\n",
        "strict_segments = strict_data.get(\"records\") or strict_data.get(\"segments\") or strict_data.get(\"scenes\") or []\n",
        "window_segments = window_data.get(\"records\") or window_data.get(\"segments\") or window_data.get(\"scenes\") or []\n",
        "\n",
        "# --- Combine based on highest fusion score ---\n",
        "combined_records = []\n",
        "common_len = min(len(strict_segments), len(window_segments))\n",
        "\n",
        "num_from_strict = 0\n",
        "num_from_window = 0\n",
        "\n",
        "for i in range(common_len):\n",
        "    s_rec = strict_segments[i]\n",
        "    w_rec = window_segments[i]\n",
        "\n",
        "    s_scores = s_rec.get(\"scores\") or {}\n",
        "    w_scores = w_rec.get(\"scores\") or {}\n",
        "\n",
        "    s_cosine = float(s_scores.get(\"cosine\", 0.0) or 0.0)\n",
        "    w_cosine = float(w_scores.get(\"cosine\", 0.0) or 0.0)\n",
        "\n",
        "    # Choose record with higher fused score\n",
        "    choose_window = (w_cosine > s_cosine)\n",
        "    best = w_rec if choose_window else s_rec\n",
        "\n",
        "    best_out = dict(best)  # shallow copy so we don't modify the original\n",
        "    num_from_window += int(choose_window)\n",
        "    num_from_strict += int(not choose_window)\n",
        "\n",
        "    combined_records.append(best_out)\n",
        "\n",
        "# --- Summary metadata ---\n",
        "combined_summary = {\n",
        "    \"records_total\": len(combined_records),\n",
        "    \"source_files\": {\n",
        "        \"strict\": validated_alignment_SA,\n",
        "        \"window\": validated_alignment_WA\n",
        "    },\n",
        "    \"selection_criterion\": \"highest fused score per segment\",\n",
        "    \"chosen_source_counts\": {\n",
        "        \"from_strict\": num_from_strict,\n",
        "        \"from_window\": num_from_window\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- Save combined JSON file ---\n",
        "with open(validated_alignment, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(\n",
        "        {\"summary\": combined_summary, \"records\": combined_records},\n",
        "        f,\n",
        "        ensure_ascii=False,\n",
        "        indent=2\n",
        "    )\n",
        "\n",
        "print(f\"Combined alignment saved: {validated_alignment}\")\n",
        "print(f\"Total segments compared: {common_len}\")\n",
        "print(f\"Chosen from Strict Alignment: {num_from_strict}\")\n",
        "print(f\"Chosen from Window Alignment: {num_from_window}\")\n",
        "print(\"Selection criteria: Highest cosine score per segment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce9dLLKZiCFT",
        "outputId": "c33bf5ab-dc19-4d52-aa06-686297d913c2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined alignment saved: /content/drive/MyDrive/ArabicVideoSummariser/Validated/Almasbagha_Validated.json\n",
            "Total segments compared: 81\n",
            "Chosen from Strict Alignment: 11\n",
            "Chosen from Window Alignment: 70\n",
            "Selection criteria: Highest cosine score per segment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Select and Concatenate Validated Segments\n",
        "# ==========================================\n",
        "import json, os\n",
        "\n",
        "# --- Load validated alignment file ---\n",
        "with open(validated_alignment, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "records = data.get(\"records\") or data.get(\"segments\") or data.get(\"scenes\") or []\n",
        "\n",
        "selected_segments = []\n",
        "\n",
        "# --- Iterate and select segments based on condition ---\n",
        "for rec in records:\n",
        "    scores = rec.get(\"scores\", {})\n",
        "    fusion_score = float(scores.get(\"fused\", 0.0))\n",
        "    margin = float(rec.get(\"Propn_Keep_Margin\", 0.0) or 0.0)\n",
        "\n",
        "    text = (\n",
        "        rec.get(\"validated_text\")\n",
        "        or rec.get(\"transcript_text\")\n",
        "        or rec.get(\"text\")\n",
        "        or \"\"\n",
        "    ).strip()\n",
        "\n",
        "    if fusion_score>SIM_THRESHOLD-margin:\n",
        "        if text:\n",
        "            selected_segments.append(text)\n",
        "    else:\n",
        "        if text:\n",
        "            print(f\" Rejected segment: {text[:80]}...\")\n",
        "\n",
        "# --- Concatenate selected text ---\n",
        "validated_text_all = \" \".join(selected_segments).strip()\n",
        "\n",
        "with open(validated_result, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(validated_text_all)\n",
        "\n",
        "# --- Print stats ---\n",
        "print(f\"Saved concatenated validated text → {validated_result}\")\n",
        "print(f\"Segments meeting condition: {len(selected_segments)} / {len(records)}\")\n",
        "print(f\"Threshold applied: SIM_THRESHOLD - Propn_Keep_Margin ({SIM_THRESHOLD})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8lkLOqYmba0",
        "outputId": "c202b12a-0654-4d09-c160-2d5d61d28b9a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Rejected segment: مصبعة...\n",
            " Rejected segment: ستشعر فور دخولك ان حقبة زمانية عاثرها هذا المكان...\n",
            " Rejected segment: منها ما زال يروي حتي الان...\n",
            " Rejected segment: التي لا تزال تعمل...\n",
            " Rejected segment: كي يوم...\n",
            " Rejected segment: اربعة واربعين. اه مواليد سبعة وتلاتين شهر اربعة يوم اربعة....\n",
            " Rejected segment: عندي حوالي مشي بالتلاتة وتمانين سنة. اه كنت شغال متعلم...\n",
            " Rejected segment: دي ايه؟ دي حاجة...\n",
            " Rejected segment: ده فوق كل شيء...\n",
            " Rejected segment: دخلت الجيش ععدت فيه 9 سنين...\n",
            " Rejected segment: حضرت حاجة...\n",
            " Rejected segment: حرب سبعة وشتين وحضرت حرب...\n",
            " Rejected segment: فاخدت المسبغة دي المسبغة دي عمرها...\n",
            " Rejected segment: 119 سنة من سنة واحدة...\n",
            " Rejected segment: الروح صبت عيش...\n",
            " Rejected segment: تقول كده...\n",
            " Rejected segment: انا حاجة جيدة...\n",
            " Rejected segment: ها هم اولاد عميقين...\n",
            " Rejected segment: هتغرب عليك؟...\n",
            " Rejected segment: في فترة انا مسكتي فيها المزبغة...\n",
            " Rejected segment: وفترة بعد كده انا قعدت ودخلت جيشي ونظامي...\n",
            " Rejected segment: الاول كان كلها يدوية...\n",
            " Rejected segment: بدل ما كان يودي شغلة عندي ولا عندي ولا عند حد...\n",
            " Rejected segment: لهو عايز....\n",
            " Rejected segment: التي تهدد تلك المهنة من الانقراض...\n",
            " Rejected segment: ولكن ال سلامة لهم راي اخر...\n",
            " Rejected segment: لا لا لا عمرة ما فيش حاجة اسمها عشر سنة...\n",
            " Rejected segment: ولو متين سنة كمان يفضل برضو اليدوي زي ما هو...\n",
            " Rejected segment: لازم كل حاجة تفضل لاصلها زي ما هي...\n",
            " Rejected segment: مهما يبقي في شركات ومصانع لابد...\n",
            " Rejected segment: لابدين يبقي فيه...\n",
            " Rejected segment: ما تقولي ايه يعني كل واحد في شغلته سلطة...\n",
            " Rejected segment: ما فيش شغلة بتبطل ابدا...\n",
            " Rejected segment: تري...\n",
            " Rejected segment: هل تظنون انه سيمتهن هذه الحرفة...\n",
            " Rejected segment: ويتحدث عنها بنفس الشغف...\n",
            "Saved concatenated validated text → /content/drive/MyDrive/ArabicVideoSummariser/Validated/Almasbagha_Validated.txt\n",
            "Segments meeting condition: 44 / 81\n",
            "Threshold applied: SIM_THRESHOLD - Propn_Keep_Margin (0.25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "VzXiCfczQgwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Segments meeting condition: {len(selected_segments)} / {len(records)}\")\n",
        "print(f\"Threshold applied: SIM_THRESHOLD - Propn_Keep_Margin ({SIM_THRESHOLD})\")\n",
        "print(f\"Threshold applied: AlFA_Fusion ({ALFA_FUSION})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMUFEhgGQfGG",
        "outputId": "df93ce83-1f9f-40c4-e1fa-e48eb51d82f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segments meeting condition: 44 / 81\n",
            "Threshold applied: SIM_THRESHOLD - Propn_Keep_Margin (0.25)\n",
            "Threshold applied: AlFA_Fusion (0.35)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lxa-NvP3Zgd2",
        "GRLZKzGy6C6O",
        "9nPhDQ1E5SG9",
        "dcSaT9HOQ-1B",
        "QmyfSdfj-zRm",
        "SC0Ez9-sX18Y",
        "_-b7FxHYghW6",
        "VzXiCfczQgwW"
      ],
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyN3EXY+M+ZKr/f/T9iW4UXj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}