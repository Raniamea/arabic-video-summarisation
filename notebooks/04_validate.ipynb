{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "authorship_tag": "ABX9TyNXu4o4QfY6rj3GP33kAM3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/04_validate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Transcript Validation using CAMeLBERT-MSA & Arabic Captions\n",
        "Compare a cleaned Arabic ASR transcript against scene captions to improve transcript accuracy.\n",
        "- Uses `diac`, `lemma`, `pos` from transcript segments\n",
        "- Uses Arabic captions generated previously\n",
        "- CAMeLBERT-MSA for semantic validation\n",
        "- Sliding window for misalignment\n",
        "- Outputs: **replace**, **append**, and **flag** transcript versions"
      ],
      "metadata": {
        "id": "oavdO_LK9djT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import os, json, re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Project paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "param_path = os.path.join(base_path, \"params.json\")\n",
        "with open(param_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    params = json.load(f)\n",
        "video_filename = params.get(\"video_file\")\n",
        "video_filename=\"Almasbagha.mp4\"\n",
        "\n",
        "assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "validated_path= os.path.join(base_path, \"Validated\")\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "caption_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "transcript_path= os.path.join(preprocessed_path, f\"{video_name}_CleanTranscript.json\")\n",
        "\n",
        "with open(transcript_path, encoding='utf-8') as f:\n",
        "    transcript_data = json.load(f)\n",
        "\n",
        "with open(caption_path, encoding='utf-8') as f:\n",
        "    captions_data = json.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkpFfleJ_7OB",
        "outputId": "b4d64f25-08a5-4c63-8851-fda730388364"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ Setup Model\n",
        "model = SentenceTransformer(\"CAMeL-Lab/bert-base-arabic-camelbert-msa\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "5fdtGOMN9tx-",
        "outputId": "358f2598-320b-46b7-c50b-9f9179a2aea1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'VersionComparison' from 'transformers.utils.import_utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-31237914.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ✅ Setup Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAMeL-Lab/bert-base-arabic-camelbert-msa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from sentence_transformers.backend import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mexport_dynamic_quantized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexport_optimized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_openvino_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_optimized_onnx_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_dynamic_quantized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_static_quantized_openvino_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/backend/load.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_save_pretrained_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_should_export\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_warn_to_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodeling_gguf_pytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_gguf_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m from .utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersionComparison\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_package_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'VersionComparison' from 'transformers.utils.import_utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bisect import bisect_left\n",
        "import torch\n",
        "from sentence_transformers import util\n",
        "\n",
        "def _as_text(x, prefer_keys):\n",
        "    \"\"\"Return the first existing key's value (string) from dict x; else stringify x.\"\"\"\n",
        "    if isinstance(x, dict):\n",
        "        for k in prefer_keys:\n",
        "            if k in x and x[k] is not None:\n",
        "                return str(x[k])\n",
        "        return \"\"\n",
        "    return str(x)\n",
        "\n",
        "def _as_joined_list(x, key):\n",
        "    \"\"\"Return space-joined list if dict[key] is a list; if str -> return it; else ''.\"\"\"\n",
        "    if isinstance(x, dict):\n",
        "        v = x.get(key, \"\")\n",
        "        if isinstance(v, list):\n",
        "            return \" \".join(map(str, v))\n",
        "        elif isinstance(v, str):\n",
        "            return v\n",
        "    return \"\"\n",
        "\n",
        "def validate_segments(\n",
        "    transcript_data,\n",
        "    captions_data,\n",
        "    model,\n",
        "    window_size=3,\n",
        "    threshold_replace=0.82,\n",
        "    threshold_append=0.75\n",
        "):\n",
        "    replace, append, flag = [], [], []\n",
        "\n",
        "    # --- Prepare captions: sort by time and keep Arabic text ---\n",
        "    # captions_data is a dict keyed by filename -> each value has 'scene_time' and 'arabic'\n",
        "    captions_sorted = sorted(\n",
        "        [{\"t\": v.get(\"scene_time\", 0.0), \"text\": v.get(\"arabic\", \"\")} for v in captions_data.values()],\n",
        "        key=lambda x: x[\"t\"]\n",
        "    )\n",
        "    caption_texts = [c[\"text\"] for c in captions_sorted]\n",
        "    caption_times = [c[\"t\"] for c in captions_sorted]\n",
        "\n",
        "    # Precompute caption embeddings once (big speed + stability win)\n",
        "    if len(caption_texts) == 0:\n",
        "        # Nothing to compare against\n",
        "        return replace, append, flag\n",
        "    cap_emb = model.encode(caption_texts, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "    # --- Helper: find nearest caption index for a given time ---\n",
        "    def nearest_caption_index(t):\n",
        "        j = bisect_left(caption_times, t)\n",
        "        if j <= 0:\n",
        "            return 0\n",
        "        if j >= len(caption_times):\n",
        "            return len(caption_times) - 1\n",
        "        # pick closer one of j-1 or j\n",
        "        return j if abs(caption_times[j] - t) < abs(caption_times[j - 1] - t) else (j - 1)\n",
        "\n",
        "    # --- Iterate segments ---\n",
        "    for idx, segment in tqdm(enumerate(transcript_data), total=len(transcript_data)):\n",
        "\n",
        "        # Ensure we have a dict to attach results to\n",
        "        if isinstance(segment, dict):\n",
        "            seg_dict = segment\n",
        "        else:\n",
        "            seg_dict = {\"text\": str(segment)}  # wrap strings to allow attaching results\n",
        "\n",
        "        # Pull text from transcript: prefer normalized > original > text\n",
        "        # Your transcript JSON uses 'text_norm' and also has lists for pos/lemmas/diac\n",
        "        seg_text  = _as_text(seg_dict, [\"text_norm\", \"text\", \"original\"])\n",
        "        seg_pos   = _as_joined_list(seg_dict, \"pos\")\n",
        "        seg_lemma = _as_joined_list(seg_dict, \"lemmas\")\n",
        "        seg_diac  = _as_joined_list(seg_dict, \"diac\")\n",
        "\n",
        "        # Build enriched string exactly like your logic intends\n",
        "        enriched_segment = f\"{seg_text} | POS: {seg_pos} | Lemma: {seg_lemma} | Diac: {seg_diac}\"\n",
        "\n",
        "        # Segment time (use midpoint if available; otherwise fall back to index)\n",
        "        if \"start\" in seg_dict and \"end\" in seg_dict:\n",
        "            seg_time = float(seg_dict.get(\"start\", 0.0) + seg_dict.get(\"end\", 0.0)) / 2.0\n",
        "            center = nearest_caption_index(seg_time)\n",
        "        else:\n",
        "            # If no timing in transcript, fall back to aligning roughly by index\n",
        "            center = min(idx, len(caption_texts) - 1)\n",
        "\n",
        "        # Candidate caption window by index (like your original logic)\n",
        "        lo = max(0, center - window_size)\n",
        "        hi = min(len(caption_texts), center + window_size + 1)\n",
        "        cand_slice = slice(lo, hi)\n",
        "\n",
        "        # Compute segment embedding\n",
        "        seg_emb = model.encode(enriched_segment, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "        # Cosine scores vs candidate captions\n",
        "        scores_vec = util.cos_sim(seg_emb, cap_emb[cand_slice]).squeeze(0)\n",
        "        best_local_idx = int(torch.argmax(scores_vec).item())\n",
        "        best_score = float(scores_vec[best_local_idx].item())\n",
        "        best_caption = caption_texts[lo + best_local_idx] if len(caption_texts) > 0 else None\n",
        "\n",
        "        # Attach results (preserving your logic)\n",
        "        seg_dict[\"match_score\"] = best_score\n",
        "        seg_dict[\"best_caption\"] = best_caption\n",
        "\n",
        "        if best_caption is None:\n",
        "            seg_dict[\"validation_action\"] = \"flag\"\n",
        "            flag.append(seg_dict)\n",
        "            continue\n",
        "\n",
        "        if best_score >= threshold_replace:\n",
        "            seg_dict[\"validation_action\"] = \"replace\"\n",
        "            seg_dict[\"revised_text\"] = best_caption\n",
        "            replace.append(seg_dict)\n",
        "        elif best_score >= threshold_append:\n",
        "            seg_dict[\"validation_action\"] = \"append\"\n",
        "            seg_dict[\"revised_text\"] = (seg_text + \" \" + best_caption).strip()\n",
        "            append.append(seg_dict)\n",
        "        else:\n",
        "            seg_dict[\"validation_action\"] = \"flag\"\n",
        "            flag.append(seg_dict)\n",
        "\n",
        "    return replace, append, flag\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "ZZAd1oYJ91o0",
        "outputId": "73b7fb1b-1b9e-4665-980f-d4a4c9046236"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-2698015616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbisect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbisect_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_as_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.2.2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentence-transformers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_hub_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Run matching\n",
        "\n",
        "replace, append, flag = validate_segments(transcript_data, captions_data, model)\n",
        "\n",
        "base_name = os.path.splitext(os.path.basename(transcript_path))[0]\n",
        "def save_json(data, suffix):\n",
        "    output_path = os.path.join(validated_path, f\"{base_name}_{suffix}.json\")\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✅ Saved: {output_path}\")\n",
        "\n",
        "\n",
        "save_json(replace, \"replace\")\n",
        "save_json(append, \"append\")\n",
        "save_json(flag, \"flag\")\n",
        "print(\"✅ Validation results saved.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8A9pFLxP95YW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}