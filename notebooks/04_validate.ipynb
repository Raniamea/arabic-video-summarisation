{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfYAXBGZENNwEXF+cs8X2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/04_validate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arabic Transcript Validation: CAMeLBERT + CLIP\n",
        "This notebook validates transcript segments by comparing:\n",
        "1. Textual semantics via [CAMeLBERT](https://huggingface.co/CAMeL-Lab/bert-base-camelbert-mix)\n",
        "2. Visual alignment via [CLIP multilingual model](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1)\n",
        "\n",
        "Outputs three versions of the transcript: `replace`, `append`, and `flag`, based on match confidence."
      ],
      "metadata": {
        "id": "oavdO_LK9djT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl1XTTNUvuKv"
      },
      "outputs": [],
      "source": [
        "# ✅ Install dependencies\n",
        "!pip install -q transformers==4.35.2 camel-tools==1.5.0 numpy==1.23.5 sentence-transformers==2.2.2 opencv-python-headless ftfy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "camelbert = AutoModel.from_pretrained(\"CAMeL-Lab/bert-base-camelbert-mix\")\n",
        "camelbert_tokenizer = AutoTokenizer.from_pretrained(\"CAMeL-Lab/bert-base-camelbert-mix\")\n",
        "\n",
        "clip_model = SentenceTransformer(\"clip-ViT-B-32-multilingual-v1\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "camelbert = camelbert.to(device)\n"
      ],
      "metadata": {
        "id": "5fdtGOMN9tx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "uploaded = files.upload()\n",
        "transcript_file = [f for f in uploaded if \"Transcript\" in f][0]\n",
        "captions_file = [f for f in uploaded if \"captions\" in f][0]\n",
        "\n",
        "with open(transcript_file, encoding='utf-8') as f:\n",
        "    transcript = json.load(f)\n",
        "\n",
        "with open(captions_file, encoding='utf-8') as f:\n",
        "    captions = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(transcript)} transcript segments, {len(captions)} captions.\")\n"
      ],
      "metadata": {
        "id": "C_qHk_PR9xxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def embed_text_camelbert(text):\n",
        "    tokens = camelbert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = camelbert(**tokens)\n",
        "    return output.last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "def embed_text_clip(text):\n",
        "    return clip_model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "def embed_image_clip(image_path):\n",
        "    return clip_model.encode(Image.open(image_path), convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "ZZAd1oYJ91o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scene_folder = \"scenes\"\n",
        "validated = {\"replace\": [], \"append\": [], \"flag\": []}\n",
        "\n",
        "for seg in tqdm(transcript):\n",
        "    scene_id = seg.get(\"scene_id\") or seg.get(\"id\") or seg.get(\"scene\")\n",
        "    seg_text = seg.get(\"diac\") or seg.get(\"text\")\n",
        "    image_path = os.path.join(scene_folder, f\"{scene_id}.jpg\")\n",
        "\n",
        "    caption = next((c[\"arabic\"] for c in captions if str(c.get(\"scene_id\")) == str(scene_id)), None)\n",
        "\n",
        "    if not caption or not os.path.exists(image_path):\n",
        "        validated[\"flag\"].append({**seg, \"reason\": \"Missing caption or image\"})\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        emb_text = embed_text_camelbert(seg_text)\n",
        "        emb_caption = embed_text_camelbert(caption)\n",
        "        sem_sim = cosine_similarity(emb_text, emb_caption, dim=0).item()\n",
        "\n",
        "        emb_img = embed_image_clip(image_path)\n",
        "        emb_txt_clip = embed_text_clip(seg_text)\n",
        "        vis_sim = cosine_similarity(emb_img, emb_txt_clip, dim=0).item()\n",
        "\n",
        "        result = {\n",
        "            \"scene_id\": scene_id,\n",
        "            \"start\": seg.get(\"start\"),\n",
        "            \"end\": seg.get(\"end\"),\n",
        "            \"text\": seg_text,\n",
        "            \"caption\": caption,\n",
        "            \"semantic_sim\": round(sem_sim, 4),\n",
        "            \"visual_sim\": round(vis_sim, 4)\n",
        "        }\n",
        "\n",
        "        if sem_sim >= 0.75 and vis_sim >= 0.7:\n",
        "            validated[\"replace\"].append(result)\n",
        "        elif sem_sim >= 0.5 or vis_sim >= 0.6:\n",
        "            validated[\"append\"].append(result)\n",
        "        else:\n",
        "            validated[\"flag\"].append(result)\n",
        "    except Exception as e:\n",
        "        validated[\"flag\"].append({**seg, \"error\": str(e)})\n"
      ],
      "metadata": {
        "id": "8A9pFLxP95YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in validated:\n",
        "    with open(f\"Validated_{k}_CAMEL_CLIP.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(validated[k], f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Output files saved:\")\n",
        "for k in validated:\n",
        "    print(f\"  - Validated_{k}_CAMEL_CLIP.json\")"
      ],
      "metadata": {
        "id": "xeSKfQL29_iy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}