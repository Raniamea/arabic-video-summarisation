{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "authorship_tag": "ABX9TyMRW5GJdwzIK329Rm3XAZCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/04_validate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Transcript Validation using CAMeLBERT-MSA & Arabic Captions\n",
        "Compare a cleaned Arabic ASR transcript against scene captions to improve transcript accuracy.\n",
        "- Uses `diac`, `lemma`, `pos` from transcript segments\n",
        "- Uses Arabic captions generated previously\n",
        "- CAMeLBERT-MSA for semantic validation\n",
        "- Sliding window for misalignment\n",
        "- Outputs: **replace**, **append**, and **flag** transcript versions"
      ],
      "metadata": {
        "id": "oavdO_LK9djT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Reset environment first\n",
        "!pip uninstall -y torch torchvision torchaudio transformers tokenizers \\\n",
        "  sentence-transformers huggingface_hub camel_tools opencv-python opencv-contrib-python \\\n",
        "  opencv-python-headless numpy\n",
        "\n",
        "# ✅ Core installs (compatible versions, no auto-upgrades)\n",
        "!pip install --no-cache-dir numpy==1.23.5\n",
        "!pip install --no-cache-dir torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
        "!pip install --no-cache-dir transformers==4.41.2 tokenizers==0.19.1\n",
        "!pip install --no-cache-dir sentence-transformers==2.2.2\n",
        "!pip install --no-cache-dir huggingface_hub==0.23.2 tqdm==4.66.5\n",
        "\n",
        "# ✅ OpenCV (avoid headless conflicts)\n",
        "!pip install --no-cache-dir opencv-python==4.7.0.72 opencv-contrib-python==4.7.0.72\n",
        "\n",
        "# ✅ CAMeL Tools (needs old numpy, already pinned to 1.23.5)\n",
        "!pip install --no-cache-dir camel-tools==1.5.2\n"
      ],
      "metadata": {
        "id": "ghSWhb0Tljww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, torch, transformers, tokenizers, sentence_transformers, huggingface_hub, cv2\n",
        "print(\"NumPy:\", numpy.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"Tokenizers:\", tokenizers.__version__)\n",
        "print(\"Sentence-Transformers:\", sentence_transformers.__version__)\n",
        "print(\"HF Hub:\", huggingface_hub.__version__)\n",
        "print(\"OpenCV:\", cv2.__version__)\n",
        "\n",
        "# Smoke tests\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tok = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"Tokenizer OK\")\n"
      ],
      "metadata": {
        "id": "yOiKBsdMl-z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RkpFfleJ_7OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re\n",
        "from bisect import bisect_left, bisect_right\n",
        "from typing import List, Dict, Any, Tuple, Iterable\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# Project Paths\n",
        "# ---------------------------\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "params_path = os.path.join(base_path, \"params.json\")\n",
        "\n",
        "with open(params_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "video_filename = params.get(\"video_file\")\n",
        "video_filename = \"Almasbagha.mp4\"\n",
        "\n",
        "assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_name  = os.path.splitext(video_filename)[0]\n",
        "\n",
        "videos_path      = os.path.join(base_path, \"videos\")\n",
        "captions_path    = os.path.join(base_path, \"captions\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "validated_path   = os.path.join(base_path, \"Validated\")\n",
        "os.makedirs(validated_path, exist_ok=True)\n",
        "\n",
        "caption_path   = os.path.join(captions_path,   f\"{video_name}.json\")\n",
        "transcript_path= os.path.join(preprocessed_path, f\"{video_name}_CleanTranscript.json\")\n",
        "\n",
        "assert os.path.exists(caption_path),   f\"Missing captions file: {caption_path}\"\n",
        "assert os.path.exists(transcript_path),f\"Missing transcript file: {transcript_path}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Model (recommended)\n",
        "# ---------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=device)\n"
      ],
      "metadata": {
        "id": "5fdtGOMN9tx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Arabic helpers\n",
        "# ---------------------------\n",
        "\n",
        "_AR_DIAC = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
        "_AR_PUNCT = re.compile(r\"[^\\w\\u0600-\\u06FF]+\", re.UNICODE)\n",
        "\n",
        "AR_STOPWORDS = {\n",
        "    \"و\", \"في\", \"على\", \"من\", \"إلى\", \"عن\", \"أن\", \"إن\", \"كان\", \"كانت\", \"يكون\", \"مع\", \"هذا\", \"هذه\",\n",
        "    \"ذلك\", \"تلك\", \"هناك\", \"هنا\", \"هو\", \"هي\", \"هم\", \"هن\", \"كما\", \"لكن\", \"بل\", \"قد\", \"تم\", \"ثم\",\n",
        "    \"كل\", \"أي\", \"أو\", \"أمام\", \"خلال\", \"بعد\", \"قبل\", \"حتى\", \"حيث\", \"إذا\", \"إنما\", \"إما\", \"لدى\",\n",
        "    \"لدي\", \"لها\", \"له\", \"لهم\", \"لنا\", \"ما\", \"ماذا\", \"لماذا\", \"كيف\", \"متى\", \"أيضا\", \"بدون\", \"أمام\",\n",
        "    \"داخل\", \"خارج\", \"بين\", \"أكثر\", \"أقل\"\n",
        "}\n",
        "\n",
        "# very light clitic/affix list (heuristic, safe)\n",
        "CLITIC_PREFIXES = (\"و\", \"ف\", \"ب\", \"ك\", \"ل\", \"س\")     # single-letter clitics\n",
        "DEF_ART = \"ال\"                                       # definite article\n",
        "CLITIC_SUFFIXES = (\"ه\", \"ها\", \"هم\", \"هن\", \"كما\", \"كم\", \"نا\", \"ي\")  # pronoun suffixes (approx)\n",
        "\n",
        "def ar_normalize(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "    t = _AR_DIAC.sub(\"\", text)\n",
        "    t = t.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
        "    t = t.replace(\"ة\",\"ه\").replace(\"ى\",\"ي\")\n",
        "    t = _AR_PUNCT.sub(\" \", t)\n",
        "    t = re.sub(r\"\\s+\",\" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def split_clitics(token: str) -> List[str]:\n",
        "    \"\"\"Split very common prefixes and remove the definite article if present.\"\"\"\n",
        "    tok = token\n",
        "    pieces = []\n",
        "\n",
        "    # peel single-letter prefixes repeatedly (e.g., و + ب + ل + ...)\n",
        "    while tok and tok[0] in CLITIC_PREFIXES:\n",
        "        pieces.append(tok[0])\n",
        "        tok = tok[1:]\n",
        "\n",
        "    # remove 'ال' if present (definite article)\n",
        "    if tok.startswith(DEF_ART) and len(tok) > 2:\n",
        "        pieces.append(DEF_ART)\n",
        "        tok = tok[2:]\n",
        "\n",
        "    # heuristic suffix stripping (only one pass; we keep core meaning)\n",
        "    for suf in CLITIC_SUFFIXES:\n",
        "        if tok.endswith(suf) and len(tok) > len(suf) + 1:\n",
        "            tok = tok[: -len(suf)]\n",
        "            break\n",
        "\n",
        "    # Return content part last (so tokens = [prefixes..., core])\n",
        "    if tok:\n",
        "        pieces.append(tok)\n",
        "    return pieces\n",
        "\n",
        "def ar_tokens(text: str) -> List[str]:\n",
        "    \"\"\"Normalize then split into tokens; keep the core (post-clitic) token for content scoring.\"\"\"\n",
        "    norm = ar_normalize(text)\n",
        "    raw = [w for w in norm.split() if w]\n",
        "    # keep both raw tokens (for exact) and stripped cores for content/lemma-ish\n",
        "    cores = []\n",
        "    for w in raw:\n",
        "        parts = split_clitics(w)\n",
        "        if parts:\n",
        "            cores.append(parts[-1])  # last piece = core\n",
        "    return cores\n",
        "\n",
        "def ngrams(tokens: List[str], n: int) -> Iterable[Tuple[str, ...]]:\n",
        "    return zip(*[tokens[i:] for i in range(n)])\n"
      ],
      "metadata": {
        "id": "ZZAd1oYJ91o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Captions prep & time windows\n",
        "# ---------------------------\n",
        "\n",
        "def prep_captions(captions_data: Dict[str, Any]):\n",
        "    \"\"\"Return sorted lists by scene_time: times[], raw_texts[], norm_tokens(core)[], joined_texts_for_embed.\"\"\"\n",
        "    rows = []\n",
        "    for v in captions_data.values():\n",
        "        rows.append({\n",
        "            \"t\": float(v.get(\"scene_time\", 0.0)),\n",
        "            \"text\": v.get(\"arabic\", \"\") or \"\"\n",
        "        })\n",
        "    rows.sort(key=lambda x: x[\"t\"])\n",
        "\n",
        "    times = [r[\"t\"] for r in rows]\n",
        "    texts = [r[\"text\"] for r in rows]\n",
        "    tokens_core = [ar_tokens(r[\"text\"]) for r in rows]  # core tokens (pseudo-lemmas)\n",
        "    return times, texts, tokens_core\n",
        "\n",
        "def time_window_indices(times: List[float], center_time: float, half_window_sec: float) -> Tuple[int, int]:\n",
        "    \"\"\"Return slice [lo:hi) covering captions within ±half_window_sec around center_time.\"\"\"\n",
        "    if not times:\n",
        "        return 0, 0\n",
        "    lo = bisect_left(times, center_time - half_window_sec)\n",
        "    hi = bisect_right(times, center_time + half_window_sec)\n",
        "    lo = max(0, lo); hi = min(len(times), hi)\n",
        "    if lo >= hi:\n",
        "        # fallback: nearest single index\n",
        "        j = min(max(bisect_left(times, center_time), 0), len(times)-1)\n",
        "        return j, j+1\n",
        "    return lo, hi\n"
      ],
      "metadata": {
        "id": "8A9pFLxP95YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Lexical overlap & fusion scoring\n",
        "# ---------------------------\n",
        "\n",
        "def jaccard(set_a: set, set_b: set) -> float:\n",
        "    if not set_a and not set_b: return 0.0\n",
        "    inter = len(set_a & set_b)\n",
        "    union = len(set_a | set_b)\n",
        "    return inter / union if union else 0.0\n",
        "\n",
        "def lexical_overlap(\n",
        "    seg_lemmas: List[str],\n",
        "    cap_core_tokens_window: List[List[str]],  # list per caption in window\n",
        "    seg_tokens_core: List[str],\n",
        "    use_ngrams: bool = True\n",
        ") -> float:\n",
        "    # Caption bag of core tokens\n",
        "    cap_bag = set()\n",
        "    for toks in cap_core_tokens_window:\n",
        "        cap_bag.update(toks)\n",
        "\n",
        "    # Lemma overlap (transcript side has true lemmas)\n",
        "    lem_bag = set([l for l in seg_lemmas if l and l not in AR_STOPWORDS])\n",
        "    lemma_j = jaccard(lem_bag, cap_bag)\n",
        "\n",
        "    if not use_ngrams:\n",
        "        return lemma_j\n",
        "\n",
        "    # n-gram overlap from core content tokens (2-gram and 3-gram)\n",
        "    seg_bi = set(ngrams([t for t in seg_tokens_core if t not in AR_STOPWORDS], 2))\n",
        "    seg_tri = set(ngrams([t for t in seg_tokens_core if t not in AR_STOPWORDS], 3))\n",
        "\n",
        "    # Build caption n-grams\n",
        "    cap_tokens_linear = []\n",
        "    for toks in cap_core_tokens_window:\n",
        "        cap_tokens_linear.extend(toks)\n",
        "\n",
        "    cap_bi  = set(ngrams([t for t in cap_tokens_linear if t not in AR_STOPWORDS], 2))\n",
        "    cap_tri = set(ngrams([t for t in cap_tokens_linear if t not in AR_STOPWORDS], 3))\n",
        "\n",
        "    # Weight trigrams a bit more than bigrams\n",
        "    bi_overlap  = jaccard(set(map(tuple, seg_bi)),  set(map(tuple, cap_bi)))\n",
        "    tri_overlap = jaccard(set(map(tuple, seg_tri)), set(map(tuple, cap_tri)))\n",
        "\n",
        "    # Weighted lexical score\n",
        "    return 0.6 * lemma_j + 0.25 * bi_overlap + 0.15 * tri_overlap\n",
        "\n",
        "def fusion_score(lex: float, cos: float, alpha: float = 0.5) -> float:\n",
        "    # alpha: weight for lexical; (1-alpha): for cosine\n",
        "    return alpha * lex + (1.0 - alpha) * cos\n"
      ],
      "metadata": {
        "id": "A8gd5p1NkH8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Safeguards\n",
        "# ---------------------------\n",
        "\n",
        "def is_propn(pos_tag: str) -> bool:\n",
        "    \"\"\"Heuristic: consider these as proper nouns.\"\"\"\n",
        "    if not pos_tag: return False\n",
        "    tag = pos_tag.upper()\n",
        "    return any(k in tag for k in (\"PROPN\", \"NNP\", \"NOUN_PROP\", \"PROPER\"))\n",
        "\n",
        "def should_backoff_too_much_removed(kept: List[str], dropped: List[str], max_removed_ratio: float = 0.3) -> bool:\n",
        "    # Only consider content words for the ratio\n",
        "    kept_content    = [w for w in kept    if len(w) >= 3 and w not in AR_STOPWORDS]\n",
        "    dropped_content = [w for w in dropped if len(w) >= 3 and w not in AR_STOPWORDS]\n",
        "    total = len(kept_content) + len(dropped_content)\n",
        "    if total == 0: return False\n",
        "    return (len(dropped_content) / total) > max_removed_ratio\n"
      ],
      "metadata": {
        "id": "IR0oQJbJkKXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Main validator\n",
        "# ---------------------------\n",
        "\n",
        "def validate_words_by_visual_support(\n",
        "    transcript_data: List[Dict[str, Any]],\n",
        "    captions_data: Dict[str, Any],\n",
        "    model: SentenceTransformer,\n",
        "    half_window_sec: float = 10.0,     # time-based window (± seconds)\n",
        "    alpha_fusion: float = 0.5,         # weight of lexical vs embedding\n",
        "    sim_threshold: float = 0.55,       # cosine threshold for window vs segment\n",
        "    min_word_len: int = 3,             # ignore very short tokens\n",
        "    propn_keep_margin: float = 0.15,   # keep PROPN unless score < (sim_threshold - margin)\n",
        "    backoff_removed_ratio: float = 0.30 # backoff if >30% content words would be removed\n",
        "):\n",
        "    # Prep captions once\n",
        "    cap_times, cap_texts, cap_core_tokens = prep_captions(captions_data)\n",
        "    if not cap_texts:\n",
        "        raise ValueError(\"No captions available for comparison.\")\n",
        "\n",
        "    enriched = []\n",
        "    kept_total, dropped_total = 0, 0\n",
        "\n",
        "    for idx, seg in tqdm(enumerate(transcript_data), total=len(transcript_data)):\n",
        "        seg_dict = seg if isinstance(seg, dict) else {\"text\": str(seg)}\n",
        "\n",
        "        # Prefer text_norm > text > original\n",
        "        seg_text   = seg_dict.get(\"text_norm\") or seg_dict.get(\"text\") or seg_dict.get(\"original\") or \"\"\n",
        "        seg_lemmas = seg_dict.get(\"lemmas\", []) or seg_dict.get(\"lemma\", [])\n",
        "        seg_pos    = seg_dict.get(\"pos\", [])  # can be list or string\n",
        "        seg_start  = float(seg_dict.get(\"start\", 0.0)) if \"start\" in seg_dict else None\n",
        "        seg_end    = float(seg_dict.get(\"end\", 0.0)) if \"end\" in seg_dict else None\n",
        "        seg_mid    = ((seg_start + seg_end)/2.0) if (seg_start is not None and seg_end is not None) else None\n",
        "\n",
        "        # Time window around segment midpoint (or index fallback)\n",
        "        if seg_mid is not None:\n",
        "            lo, hi = time_window_indices(cap_times, seg_mid, half_window_sec)\n",
        "        else:\n",
        "            # fallback to a narrow window by index (kept for robustness)\n",
        "            center = min(idx, len(cap_times) - 1)\n",
        "            lo, hi = max(0, center-3), min(len(cap_times), center+4)\n",
        "\n",
        "        # Build window text for embeddings\n",
        "        window_text = \" \".join(cap_texts[lo:hi])\n",
        "\n",
        "        # Segment embedding vs window\n",
        "        with torch.no_grad():\n",
        "            seg_emb = model.encode(seg_text, convert_to_tensor=True, show_progress_bar=False)\n",
        "            win_emb = model.encode(window_text, convert_to_tensor=True, show_progress_bar=False)\n",
        "            cos = float(util.cos_sim(seg_emb, win_emb).item())\n",
        "\n",
        "        # Lexical overlap (lemmas + ngrams) vs caption cores in the window\n",
        "        lex = lexical_overlap(\n",
        "            seg_lemmas,\n",
        "            cap_core_tokens[lo:hi],\n",
        "            ar_tokens(seg_text),\n",
        "            use_ngrams=True\n",
        "        )\n",
        "\n",
        "        fused = fusion_score(lex, cos, alpha=alpha_fusion)\n",
        "\n",
        "        # Tokenize segment words for keep/drop decision\n",
        "        # Keep original surface words to reconstruct validated_text\n",
        "        raw_words = [w for w in re.sub(r\"\\s+\", \" \", seg_text).split(\" \") if w]\n",
        "\n",
        "        kept_words, dropped_words = [], []\n",
        "        for i, w in enumerate(raw_words):\n",
        "            w_norm = ar_normalize(w)\n",
        "            if len(w_norm) < min_word_len or w_norm in AR_STOPWORDS:\n",
        "                kept_words.append(w)\n",
        "                continue\n",
        "\n",
        "            # If POS exists and says PROPN, safeguard (don't drop unless very low support)\n",
        "            is_name = False\n",
        "            if isinstance(seg_pos, list) and i < len(seg_pos):\n",
        "                is_name = is_propn(str(seg_pos[i]))\n",
        "            elif isinstance(seg_pos, str):\n",
        "                # crude: if whole segment POS tag string contains PROPN\n",
        "                is_name = is_propn(seg_pos)\n",
        "\n",
        "            # Decision by fused score; for PROPN, allow a margin\n",
        "            thr = sim_threshold - (propn_keep_margin if is_name else 0.0)\n",
        "            if fused >= thr:\n",
        "                kept_words.append(w)\n",
        "            else:\n",
        "                dropped_words.append(w)\n",
        "\n",
        "        # Backoff if we removed too much\n",
        "        if should_backoff_too_much_removed(kept_words, dropped_words, max_removed_ratio=backoff_removed_ratio):\n",
        "            kept_words = raw_words\n",
        "            dropped_words = []\n",
        "\n",
        "        validated_text = \" \".join(kept_words)\n",
        "\n",
        "        seg_out = dict(seg_dict)\n",
        "        seg_out[\"visual_window_idx\"] = [lo, hi-1]\n",
        "        seg_out[\"visual_context\"] = window_text\n",
        "        seg_out[\"scores\"] = {\"cosine\": cos, \"lexical\": lex, \"fused\": fused}\n",
        "        seg_out[\"validated_text\"] = validated_text\n",
        "        seg_out[\"dropped_words\"] = dropped_words\n",
        "        seg_out[\"kept_words\"] = kept_words\n",
        "\n",
        "        enriched.append(seg_out)\n",
        "        kept_total += len(kept_words)\n",
        "        dropped_total += len(dropped_words)\n",
        "\n",
        "    summary = {\n",
        "        \"segments\": len(enriched),\n",
        "        \"kept_words\": kept_total,\n",
        "        \"dropped_words\": dropped_total,\n",
        "        \"params\": {\n",
        "            \"half_window_sec\": half_window_sec,\n",
        "            \"alpha_fusion\": alpha_fusion,\n",
        "            \"sim_threshold\": sim_threshold,\n",
        "            \"min_word_len\": min_word_len,\n",
        "            \"propn_keep_margin\": propn_keep_margin,\n",
        "            \"backoff_removed_ratio\": backoff_removed_ratio\n",
        "        }\n",
        "    }\n",
        "    return enriched, summary\n"
      ],
      "metadata": {
        "id": "iPZiwI2ukOPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Load data\n",
        "# ---------------------------\n",
        "with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    transcript_data = json.load(f)\n",
        "\n",
        "# ---------------------------\n",
        "# Run validation\n",
        "# ---------------------------\n",
        "enriched, stats = validate_words_by_visual_support(\n",
        "    transcript_data=transcript_data,\n",
        "    captions_data=captions_data,\n",
        "    model=model,\n",
        "    half_window_sec=10.0,     # tune 8–12s\n",
        "    alpha_fusion=0.5,         # 0.4–0.6 usually stable\n",
        "    sim_threshold=0.55,       # raise for stricter, lower for more permissive\n",
        "    min_word_len=3,\n",
        "    propn_keep_margin=0.15,\n",
        "    backoff_removed_ratio=0.30\n",
        ")\n",
        "\n",
        "print(\"Summary:\", stats)\n",
        "\n",
        "# ---------------------------\n",
        "# Save outputs into Validated/\n",
        "# ---------------------------\n",
        "base_name = os.path.splitext(os.path.basename(transcript_path))[0]\n",
        "out_json = os.path.join(validated_path, f\"{base_name}_ValidatedWords.json\")\n",
        "\n",
        "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"summary\": stats, \"segments\": enriched}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Saved:\", out_json)\n"
      ],
      "metadata": {
        "id": "Y4Nrn9PTkS7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}