{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOBNseRqwa3Q7udVQBrEl9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/CustomTrainingOnTranscripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrOGJcCOQBGL"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Mount Google Drive and define base path\n",
        "# =========================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Safe mount: avoids duplicate mount warnings\n",
        "if not os.path.ismount(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# Define base path for project files\n",
        "BASE_PATH = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "print(f\"Base path set to: {BASE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all broken HF packages\n",
        "!pip uninstall -y transformers huggingface_hub tokenizers accelerate diffusers gradio"
      ],
      "metadata": {
        "id": "PPGgueOfQRKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  \"transformers==4.46.3\" \\\n",
        "  \"huggingface-hub==0.35.3\" \\\n",
        "  \"tokenizers==0.20.3\" \\\n",
        "  \"datasets==2.19.1\" \\\n",
        "  \"evaluate>=0.4.2,<0.5.0\"  \\\n",
        "  \"rouge-score==0.1.2\" \\\n",
        "  \"bert-score==0.3.13\" \\\n",
        "  \"matplotlib==3.8.4\" \\\n",
        "  \"pandas==2.2.2\" \\\n",
        "  \"accelerate>=0.30.0,<0.35.0\""
      ],
      "metadata": {
        "id": "hLfEYAxnp-tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Custom Fine-Tuning on Arabic Transcript–Summary Pairs\n",
        "# (ROUGE-L + BERTScore + SemanticSim)\n",
        "# ============================================================\n",
        "\n",
        "import os, torch, evaluate, numpy as np, warnings\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from bert_score import score as bert_score_fn\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "EXCEL_PATH = os.path.join(BASE_DIR, \"transcripts.xlsx\")\n",
        "\n",
        "# ---- Clean up warning clutter ----\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ---- Load dataset ----\n",
        "df = pd.read_excel(EXCEL_PATH)\n",
        "df = df.dropna(subset=['transcript', 'summary']).reset_index(drop=True)\n",
        "\n",
        "# ---- Split into train / val / test (80 / 10 / 10) ----\n",
        "train_df, val_df, test_df = np.split(\n",
        "    df.sample(frac=1, random_state=42),\n",
        "    [int(0.8 * len(df)), int(0.9 * len(df))]\n",
        ")\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "    \"test\": Dataset.from_pandas(test_df)\n",
        "})\n",
        "\n",
        "# ---- Choose base model ----\n",
        "#BASE_MODEL = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "#BASE_MODEL = \"moussaKam/AraBART\"\n",
        "\n",
        "BASE_MODEL = \"ahmeddbahaa/AraBART-finetuned-ar\"\n",
        "MODEL_NAME = BASE_MODEL.split(\"/\")[-1]\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess(batch):\n",
        "    model_inputs = tokenizer(batch[\"transcript\"], max_length=max_input_length, truncation=True)\n",
        "    labels = tokenizer(batch[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_ds = dataset.map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# ---- Metrics ----\n",
        "def load_safe_metrics():\n",
        "    \"\"\"Load ROUGE-L and BERTScore safely, even if HF evaluate fails.\"\"\"\n",
        "    try:\n",
        "        rouge = evaluate.load(\"rouge\")\n",
        "    except Exception:\n",
        "        print(\"Using local fallback for ROUGE-L\")\n",
        "        class RougeFallback:\n",
        "            def compute(self, predictions, references, use_stemmer=True):\n",
        "                scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=use_stemmer)\n",
        "                aggregator = scoring.BootstrapAggregator()\n",
        "                for p, r in zip(predictions, references):\n",
        "                    aggregator.add_scores(scorer.score(r, p))\n",
        "                result = aggregator.aggregate()\n",
        "                return {\"rougeL\": result[\"rougeL\"].mid.fmeasure}\n",
        "        rouge = RougeFallback()\n",
        "\n",
        "    try:\n",
        "        bertscore = evaluate.load(\"bertscore\")\n",
        "    except Exception:\n",
        "        print(\"Using local fallback for BERTScore\")\n",
        "        class BertFallback:\n",
        "            def compute(self, predictions, references, lang=\"ar\"):\n",
        "                P, R, F1 = bert_score_fn(predictions, references, lang=lang, verbose=False)\n",
        "                return {\"f1\": F1.tolist()}\n",
        "        bertscore = BertFallback()\n",
        "\n",
        "    return rouge, bertscore\n",
        "\n",
        "rouge, bertscore = load_safe_metrics()\n",
        "\n",
        "# ---- Load LaBSE model for semantic similarity ----\n",
        "sem_model = SentenceTransformer(\"sentence-transformers/LaBSE\").to(device)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    preds = np.where(preds < 0, 0, preds)\n",
        "    preds = np.where(preds >= tokenizer.vocab_size, 0, preds)\n",
        "    labels = np.where(labels < 0, 0, labels)\n",
        "    labels = np.where(labels >= tokenizer.vocab_size, 0, labels)\n",
        "\n",
        "    decoded_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds  = [p.strip() if p.strip() else \"[EMPTY]\" for p in decoded_preds]\n",
        "    decoded_labels = [l.strip() if l.strip() else \"[EMPTY]\" for l in decoded_labels]\n",
        "\n",
        "    # ---- ROUGE-L ----\n",
        "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    rougeL = float(r.get(\"rougeL\", 0.0))\n",
        "\n",
        "    # ---- BERTScore ----\n",
        "    b = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"ar\")\n",
        "    f1_values = b.get(\"f1\", 0.0)\n",
        "    bert_f1_mean = float(np.mean(f1_values)) if isinstance(f1_values, (list, np.ndarray)) else float(f1_values)\n",
        "\n",
        "    # ---- Semantic similarity (LaBSE) ----\n",
        "    emb_pred = sem_model.encode(decoded_preds, convert_to_tensor=True, show_progress_bar=False)\n",
        "    emb_ref  = sem_model.encode(decoded_labels, convert_to_tensor=True, show_progress_bar=False)\n",
        "    cosine_scores = util.cos_sim(emb_pred, emb_ref).diagonal().detach().cpu().numpy()\n",
        "    semantic_mean = float(np.mean(cosine_scores))\n",
        "\n",
        "    return {\n",
        "        \"rougeL\": rougeL,\n",
        "        \"bertscore_f1\": bert_f1_mean,\n",
        "        \"semantic_sim\": semantic_mean\n",
        "    }\n",
        "\n",
        "# ---- Load model ----\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(device)\n",
        "\n",
        "# ---- Training setup ----\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f\"/content/{MODEL_NAME}_finetuned_ROUGEL_SEMANTIC\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"semantic_sim\",\n",
        "    greater_is_better=True,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=max_target_length,\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    disable_tqdm=True\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# ---- Train ----\n",
        "trainer.train()\n",
        "\n",
        "# ---- Evaluate on test set ----\n",
        "metrics = trainer.evaluate(tokenized_ds[\"test\"])\n",
        "wanted = {k: v for k, v in metrics.items() if k in (\n",
        "    \"eval_rougeL\", \"eval_bertscore_f1\", \"eval_semantic_sim\"\n",
        ")}\n",
        "print(\"\\n===== Final Test Metrics =====\")\n",
        "for k in [\"eval_rougeL\", \"eval_bertscore_f1\", \"eval_semantic_sim\"]:\n",
        "    if k in wanted and isinstance(wanted[k], (int, float)):\n",
        "        print(f\"{k}: {wanted[k]:.4f}\")\n"
      ],
      "metadata": {
        "id": "o1r3L8mVQgSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 📦 Save Fine-Tuned Model to Drive\n",
        "# Target: /content/drive/MyDrive/Videosummarisation/models/<timestamped_model_name>\n",
        "# ============================================================\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ---- Define target path ----\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/Videosummarisation/models\"\n",
        "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
        "\n",
        "# Create timestamped folder\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "save_dir = os.path.join(SAVE_ROOT, f\"{MODEL_NAME}_finetuned_{timestamp}\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"💾 Saving fine-tuned model to: {save_dir}\")\n",
        "\n",
        "# ---- Save model, tokenizer, and generation config ----\n",
        "try:\n",
        "    model.save_pretrained(save_dir, safe_serialization=False)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    if hasattr(model, \"generation_config\"):\n",
        "        model.generation_config.save_pretrained(save_dir)\n",
        "    print(\"✅ Model, tokenizer, and generation config saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\" Save failed: {e}\")\n",
        "\n",
        "# ---- Optional: save metrics summary ----\n",
        "metrics_path = os.path.join(save_dir, \"final_test_metrics.txt\")\n",
        "try:\n",
        "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"===== Final Test Metrics =====\\n\")\n",
        "        for k, v in wanted.items():\n",
        "            f.write(f\"{k}: {v:.4f}\\n\")\n",
        "    print(f\"📄 Metrics saved to {metrics_path}\")\n",
        "except Exception as e:\n",
        "    print(f\" Metrics file not written: {e}\")\n",
        "\n",
        "# ---- Optional: Zip the folder (for backup or download) ----\n",
        "# import shutil\n",
        "# zip_path = f\"{save_dir}.zip\"\n",
        "# shutil.make_archive(save_dir, \"zip\", save_dir)\n",
        "# print(f\" Zipped model saved as: {zip_path}\")\n",
        "\n",
        "print(\"\\n Model Saved in  'Videosummarisation/models/'\")\n"
      ],
      "metadata": {
        "id": "N5LrmEJK98Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dOAbjNINYkZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FULL TEST-SET EVALUATION (ROUGE-L + BERTScore + Semantic-Sim)\n",
        "# ============================================================\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt, textwrap, random, os, evaluate, warnings, torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ---------- 0) Helpers ----------\n",
        "def safe_batch_decode(tokenizer, sequences):\n",
        "    vocab = tokenizer.vocab_size\n",
        "    clean = []\n",
        "    for seq in sequences:\n",
        "        if isinstance(seq, np.ndarray): seq = seq.tolist()\n",
        "        seq = [int(i) for i in seq if 0 <= i < vocab]\n",
        "        clean.append(seq or [tokenizer.pad_token_id])\n",
        "    return tokenizer.batch_decode(clean, skip_special_tokens=True)\n",
        "\n",
        "def load_metrics():\n",
        "    os.environ.pop(\"HF_EVALUATE_OFFLINE\", None)\n",
        "    os.environ.pop(\"HF_DATASETS_OFFLINE\", None)\n",
        "    try:\n",
        "        rouge = evaluate.load(\"rouge\")\n",
        "    except Exception:\n",
        "        from rouge_score import rouge_scorer, scoring\n",
        "        class RougeFallback:\n",
        "            def compute(self, predictions, references, use_stemmer=True):\n",
        "                scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=use_stemmer)\n",
        "                agg = scoring.BootstrapAggregator()\n",
        "                for p,r in zip(predictions,references): agg.add_scores(scorer.score(r,p))\n",
        "                res = agg.aggregate()\n",
        "                return {\"rougeL\": res[\"rougeL\"].mid.fmeasure}\n",
        "        rouge = RougeFallback()\n",
        "    try:\n",
        "        bertscore = evaluate.load(\"bertscore\")\n",
        "    except Exception:\n",
        "        from bert_score import score\n",
        "        class BertFallback:\n",
        "            def compute(self, predictions, references, lang=\"ar\"):\n",
        "                P,R,F1 = score(predictions,references,lang=lang,verbose=False)\n",
        "                return {\"precision\":P.tolist(),\"recall\":R.tolist(),\"f1\":F1.tolist()}\n",
        "        bertscore = BertFallback()\n",
        "    return rouge, bertscore\n",
        "\n",
        "rouge, bertscore = load_metrics()\n",
        "\n",
        "# ---------- 1) Generate predictions ----------\n",
        "print(\"🚀 Generating summaries for the entire test set...\")\n",
        "pred_out = trainer.predict(tokenized_ds[\"test\"], max_length=128)\n",
        "pred_ids = pred_out.predictions[0] if isinstance(pred_out.predictions, tuple) else pred_out.predictions\n",
        "pred_texts = safe_batch_decode(tokenizer, pred_ids)\n",
        "\n",
        "ref_texts = list(dataset[\"test\"][\"summary\"])\n",
        "src_texts = list(dataset[\"test\"][\"transcript\"])\n",
        "\n",
        "# ---------- 2) Compute per-sample metrics ----------\n",
        "print(\"⚙️ Computing ROUGE-L, BERTScore, and Semantic-Sim (LaBSE)...\")\n",
        "\n",
        "# --- Initialize Semantic Model (LaBSE) ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sem_model = SentenceTransformer(\"sentence-transformers/LaBSE\").to(device)\n",
        "\n",
        "# --- ROUGE & BERTScore ---\n",
        "rouge_scores = rouge.compute(predictions=pred_texts, references=ref_texts)\n",
        "bert_scores = bertscore.compute(predictions=pred_texts, references=ref_texts, lang=\"ar\")\n",
        "\n",
        "rougeL_per_sample = []\n",
        "semantic_sim_per_sample = []\n",
        "\n",
        "for p, r in zip(pred_texts, ref_texts):\n",
        "    # ROUGE-L\n",
        "    tmp = rouge.compute(predictions=[p], references=[r])\n",
        "    rougeL = tmp.get(\"rougeL\", 0.0) * 100\n",
        "    rougeL_per_sample.append(rougeL)\n",
        "\n",
        "    # Semantic Similarity (cosine over LaBSE embeddings)\n",
        "    emb_pred = sem_model.encode(p, convert_to_tensor=True)\n",
        "    emb_ref  = sem_model.encode(r, convert_to_tensor=True)\n",
        "    sim = float(util.cos_sim(emb_pred, emb_ref).cpu().item()) * 100\n",
        "    semantic_sim_per_sample.append(sim)\n",
        "\n",
        "bert_f1_per_sample = np.array(bert_scores[\"f1\"]) * 100.0\n",
        "\n",
        "# ---------- 3) Build a per-sample dataframe ----------\n",
        "def clip(s, n=320):\n",
        "    s = (s or \"\").replace(\"\\n\",\" \")\n",
        "    return (s[:n]+\"…\") if len(s)>n else s\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"idx\": np.arange(len(pred_texts)),\n",
        "    \"ROUGE_L_%\": np.round(rougeL_per_sample,2),\n",
        "    \"BERT_F1_%\": np.round(bert_f1_per_sample,2),\n",
        "    \"Semantic_Sim_%\": np.round(semantic_sim_per_sample,2),\n",
        "    \"Prediction\": [clip(t) for t in pred_texts],\n",
        "    \"Reference\":  [clip(t) for t in ref_texts],\n",
        "    \"Transcript_snip\": [clip(t) for t in src_texts]\n",
        "})\n",
        "\n",
        "OUT_DIR = \"/content/eval_outputs_full\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "out_csv = os.path.join(OUT_DIR,\"test_full_metrics.csv\")\n",
        "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"✅ Saved per-sample metrics → {out_csv}\")\n",
        "\n",
        "# ---------- 4) Summary statistics ----------\n",
        "mean_rougeL = float(df[\"ROUGE_L_%\"].mean())\n",
        "mean_bert   = float(df[\"BERT_F1_%\"].mean())\n",
        "mean_sem    = float(df[\"Semantic_Sim_%\"].mean())\n",
        "\n",
        "print(f\"\\n===== {MODEL_NAME} =====\")\n",
        "print(f\"Mean ROUGE-L: {mean_rougeL:.2f}%\")\n",
        "print(f\"Mean BERTScore F1: {mean_bert:.2f}%\")\n",
        "print(f\"Mean Semantic-Sim: {mean_sem:.2f}%\")\n",
        "\n",
        "# ---------- 5) Plot distributions ----------\n",
        "plt.figure(figsize=(14,4))\n",
        "\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(df[\"BERT_F1_%\"], bins=20, color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.title(\"BERTScore F1 Distribution\"); plt.xlabel(\"F1 (%)\"); plt.ylabel(\"Count\"); plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(df[\"Semantic_Sim_%\"], bins=20, color=\"mediumseagreen\", edgecolor=\"black\")\n",
        "plt.title(\"Semantic Similarity (LaBSE) Distribution\"); plt.xlabel(\"Cosine Similarity (%)\"); plt.ylabel(\"Count\"); plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.hist(df[\"ROUGE_L_%\"], bins=20, color=\"lightcoral\", edgecolor=\"black\")\n",
        "plt.title(\"ROUGE-L Distribution\"); plt.xlabel(\"ROUGE-L (%)\"); plt.ylabel(\"Count\"); plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,\"metric_distributions.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# ---------- 6) Show representative samples ----------\n",
        "best_i  = int(df[\"Semantic_Sim_%\"].idxmax())\n",
        "worst_i = int(df[\"Semantic_Sim_%\"].idxmin())\n",
        "\n",
        "def show_case(row, title):\n",
        "    print(\"\\n\"+\"=\"*80)\n",
        "    print(title)\n",
        "    print(\"=\"*80)\n",
        "    print(f\"[idx={int(row['idx'])}] | ROUGE-L: {row['ROUGE_L_%']:.2f}% | BERT F1: {row['BERT_F1_%']:.2f}% | Semantic-Sim: {row['Semantic_Sim_%']:.2f}%\")\n",
        "    print(\"\\nTranscript (snippet):\\n\"+textwrap.fill(row[\"Transcript_snip\"],width=100))\n",
        "    print(\"\\nReference:\\n\"+textwrap.fill(row[\"Reference\"],width=100))\n",
        "    print(\"\\nPrediction:\\n\"+textwrap.fill(row[\"Prediction\"],width=100))\n",
        "\n",
        "print(\"\\n===== Qualitative Test Samples =====\")\n",
        "show_case(df.loc[best_i],  \"BEST SAMPLE (Semantic Similarity)\")\n",
        "show_case(df.loc[worst_i], \"WORST SAMPLE (Semantic Similarity)\")\n",
        "for rnd in random.sample(list(df.index), k=min(2,len(df))):\n",
        "    show_case(df.loc[rnd],  \"RANDOM SAMPLE\")\n",
        "\n",
        "print(f\"\\n🖼️ Figures & CSV saved under: {OUT_DIR}\")\n"
      ],
      "metadata": {
        "id": "1yGC3GVj0yRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}