{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOUr5gMwrS+JL7S1X5syAtu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/01_transcribe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg\n",
        "!pip install -q pydub"
      ],
      "metadata": {
        "id": "DIUtvDSNHQxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your params.json on Google Drive\n",
        "param_path = \"/content/drive/MyDrive/ArabicVideoSummariser/params.json\"\n",
        "\n",
        "# Load it\n",
        "with open(param_path, \"r\") as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "# Get the filename\n",
        "video_filename = params.get(\"video_file\")\n",
        "print(\"üé• Transcribing video file:\", video_filename)\n",
        "\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "\n",
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar.txt\")\n",
        "trascription_json_path = os.path.join(transcripts_path, f\"{video_name}_ar.json\")\n",
        "\n",
        "# Convert video to audio\n",
        "audio_path = os.path.join(videos_path, f\"{video_name}.wav\")\n",
        "!ffmpeg -y -i \"{video_path}\" -ar 16000 -ac 1 \"{audio_path}\"  # Resample to 16kHz mono\n",
        "\n",
        "# Load audio using pydub\n",
        "audio = AudioSegment.from_wav(audio_path)\n",
        "chunk_length_ms = 30 * 1000  # 30 seconds\n",
        "total_chunks = math.ceil(len(audio) / chunk_length_ms)\n",
        "\n",
        "print(f\"üîä Audio duration: {len(audio) / 1000:.1f}s, Chunks: {total_chunks}\")\n"
      ],
      "metadata": {
        "id": "MaUj-uJnyqz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31fa5413"
      },
      "outputs": [],
      "source": [
        "import torch, whisper, json, gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model = whisper.load_model(\"large\", device=\"cuda\", in_memory=True)\n",
        "\n",
        "results_ar = []\n",
        "results_en = []\n",
        "\n",
        "for i in range(total_chunks):\n",
        "    start_ms = i * chunk_length_ms\n",
        "    end_ms = min((i + 1) * chunk_length_ms, len(audio))\n",
        "    chunk = audio[start_ms:end_ms]\n",
        "    chunk_file = f\"/content/chunk_{i}.wav\"\n",
        "    chunk.export(chunk_file, format=\"wav\")\n",
        "\n",
        "    print(f\"‚è±Ô∏è Transcribing chunk {i+1}/{total_chunks} ({start_ms/1000:.1f}s - {end_ms/1000:.1f}s)\")\n",
        "\n",
        "    # Arabic transcription\n",
        "    result_ar = model.transcribe(\n",
        "        chunk_file, language=\"ar\", task=\"transcribe\", verbose=False, fp16=False\n",
        "    )\n",
        "    for segment in result_ar[\"segments\"]:\n",
        "        segment[\"start\"] += start_ms / 1000\n",
        "        segment[\"end\"] += start_ms / 1000\n",
        "        results_ar.append(segment)\n",
        "\n",
        "    # English translation\n",
        "    result_en = model.transcribe(\n",
        "        chunk_file, language=\"ar\", task=\"translate\", verbose=False, fp16=False\n",
        "    )\n",
        "    for segment in result_en[\"segments\"]:\n",
        "        segment[\"start\"] += start_ms / 1000\n",
        "        segment[\"end\"] += start_ms / 1000\n",
        "        results_en.append(segment)\n",
        "\n",
        "# === Arabic Output ===\n",
        "# Save text transcript\n",
        "with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join([seg[\"text\"] for seg in results_ar]))\n",
        "\n",
        "# Save time-coded transcript\n",
        "with open(transcript_path.replace(\".txt\", \"_with_timecodes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for seg in results_ar:\n",
        "        f.write(f\"[{seg['start']:.2f} - {seg['end']:.2f}] {seg['text']}\\n\")\n",
        "\n",
        "# Save JSON\n",
        "with open(trascription_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"segments\": results_ar}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# === English Output ===\n",
        "en_txt_path = transcript_path.replace(\"ar.txt\", \"en.txt\")\n",
        "with open(en_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join([seg[\"text\"] for seg in results_en]))\n",
        "\n",
        "with open(en_txt_path.replace(\".txt\", \"_with_timecodes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for seg in results_en:\n",
        "        f.write(f\"[{seg['start']:.2f} - {seg['end']:.2f}] {seg['text']}\\n\")\n",
        "\n",
        "with open(trascription_json_path.replace(\"ar.json\", \"en.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"segments\": results_en}, f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "qeXd-Xmm16To"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}