{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNAYMNfqCv3gJWWlljr2qep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/01_transcribe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Whisper and Torch\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!apt-get install ffmpeg\n",
        "!pip install -q pydub"
      ],
      "metadata": {
        "id": "DIUtvDSNHQxV",
        "outputId": "6a44e1ea-ee61-46ce-af3f-b6abf40c09c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m218.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Path to your params.json on Google Drive\n",
        "param_path = \"/content/drive/MyDrive/ArabicVideoSummariser/params.json\"\n",
        "\n",
        "# Load it\n",
        "with open(param_path, \"r\") as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "# Get the filename\n",
        "video_filename = params.get(\"video_file\")\n",
        "print(\"ğŸ¥ Transcribing video file:\", video_filename)\n",
        "\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "\n",
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar.txt\")\n",
        "trascription_json_path = os.path.join(transcripts_path, f\"{video_name}_ar.json\")\n",
        "\n",
        "# Convert video to audio\n",
        "audio_path = os.path.join(videos_path, f\"{video_name}.wav\")\n",
        "!ffmpeg -y -i \"{video_path}\" -ar 16000 -ac 1 \"{audio_path}\"  # Resample to 16kHz mono\n",
        "\n",
        "# Load audio using pydub\n",
        "audio = AudioSegment.from_wav(audio_path)\n",
        "chunk_length_ms = 30 * 1000  # 30 seconds\n",
        "total_chunks = math.ceil(len(audio) / chunk_length_ms)\n",
        "\n",
        "print(f\"ğŸ”Š Audio duration: {len(audio) / 1000:.1f}s, Chunks: {total_chunks}\")\n"
      ],
      "metadata": {
        "id": "MaUj-uJnyqz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31fa5413"
      },
      "outputs": [],
      "source": [
        "import torch, whisper, json, gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model = whisper.load_model(\"large\", device=\"cuda\", in_memory=True)\n",
        "\n",
        "results_ar = []\n",
        "results_en = []\n",
        "\n",
        "for i in range(total_chunks):\n",
        "    start_ms = i * chunk_length_ms\n",
        "    end_ms = min((i + 1) * chunk_length_ms, len(audio))\n",
        "    chunk = audio[start_ms:end_ms]\n",
        "    chunk_file = f\"/content/chunk_{i}.wav\"\n",
        "    chunk.export(chunk_file, format=\"wav\")\n",
        "\n",
        "    print(f\"â±ï¸ Transcribing chunk {i+1}/{total_chunks} ({start_ms/1000:.1f}s - {end_ms/1000:.1f}s)\")\n",
        "\n",
        "    # Arabic transcription\n",
        "    result_ar = model.transcribe(\n",
        "        chunk_file, language=\"ar\", task=\"transcribe\", verbose=False, fp16=False\n",
        "    )\n",
        "    for segment in result_ar[\"segments\"]:\n",
        "        segment[\"start\"] += start_ms / 1000\n",
        "        segment[\"end\"] += start_ms / 1000\n",
        "        results_ar.append(segment)\n",
        "\n",
        "    # English translation\n",
        "    result_en = model.transcribe(\n",
        "        chunk_file, language=\"ar\", task=\"translate\", verbose=False, fp16=False\n",
        "    )\n",
        "    for segment in result_en[\"segments\"]:\n",
        "        segment[\"start\"] += start_ms / 1000\n",
        "        segment[\"end\"] += start_ms / 1000\n",
        "        results_en.append(segment)\n",
        "\n",
        "# === Arabic Output ===\n",
        "# Save text transcript\n",
        "with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join([seg[\"text\"] for seg in results_ar]))\n",
        "\n",
        "# Save time-coded transcript\n",
        "with open(transcript_path.replace(\".txt\", \"_with_timecodes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for seg in results_ar:\n",
        "        f.write(f\"[{seg['start']:.2f} - {seg['end']:.2f}] {seg['text']}\\n\")\n",
        "\n",
        "# Save JSON\n",
        "with open(trascription_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"segments\": results_ar}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# === English Output ===\n",
        "en_txt_path = transcript_path.replace(\"ar.txt\", \"en.txt\")\n",
        "with open(en_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join([seg[\"text\"] for seg in results_en]))\n",
        "\n",
        "with open(en_txt_path.replace(\".txt\", \"_with_timecodes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for seg in results_en:\n",
        "        f.write(f\"[{seg['start']:.2f} - {seg['end']:.2f}] {seg['text']}\\n\")\n",
        "\n",
        "with open(trascription_json_path.replace(\"ar.json\", \"en.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"segments\": results_en}, f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "qeXd-Xmm16To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Check if both Arabic and English results are non-empty\n",
        "if results_ar and results_en:\n",
        "    links_html = \"\"\"\n",
        "    <h3>âœ… Transcription complete!</h3>\n",
        "    <p>Continue to the next steps:</p>\n",
        "    <ul>\n",
        "      <li><a href=\"https://colab.research.google.com/drive/SCENE_NOTEBOOK_ID\" target=\"_blank\">â–¶ï¸ Run Scene Detection</a></li>\n",
        "      <li><a href=\"https://colab.research.google.com/drive/CAPTIONS_NOTEBOOK_ID\" target=\"_blank\">ğŸ“ Generate Scene Captions</a></li>\n",
        "      <li><a href=\"https://colab.research.google.com/drive/VALIDATE_NOTEBOOK_ID\" target=\"_blank\">ğŸ” Validate Summary</a></li>\n",
        "    </ul>\n",
        "    \"\"\"\n",
        "    display(HTML(links_html))\n",
        "else:\n",
        "    print(\"âš ï¸ Transcription incomplete or failed. Links not shown.\")\n"
      ],
      "metadata": {
        "id": "KhlN1c0WzcZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}