{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMojNZzbM3UHhO2vu8YsvU6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/02_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If already mounted, unmount it safely\n",
        "!fusermount -u /content/drive\n",
        "\n",
        "# Then try mounting again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dP0l4sd4MXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scenedetect[opencv] transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "jC_2s9KtHZka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "from PIL import Image\n",
        "from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "# ============ SETUP ============\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# BLIP-2 model\n",
        "caption_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Translation model (EN â†’ AR)\n",
        "translator_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "translator_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\").to(device)\n",
        "\n",
        "# Base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "keyframes_base = os.path.join(base_path, \"keyframes\")\n",
        "captions_base = os.path.join(base_path, \"captions\")\n",
        "os.makedirs(keyframes_base, exist_ok=True)\n",
        "os.makedirs(captions_base, exist_ok=True)\n",
        "\n",
        "# ============ FUNCTION ============\n",
        "def extract_and_caption(video_path, video_name):\n",
        "    keyframe_dir = os.path.join(keyframes_base, video_name)\n",
        "    os.makedirs(keyframe_dir, exist_ok=True)\n",
        "    captions = {}\n",
        "\n",
        "    # --- Scene detection ---\n",
        "    video_manager = VideoManager([video_path])\n",
        "    scene_manager = SceneManager()\n",
        "    scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
        "    video_manager.set_downscale_factor()\n",
        "    video_manager.start()\n",
        "    scene_manager.detect_scenes(video_manager)\n",
        "    scene_list = scene_manager.get_scene_list()\n",
        "    video_manager.release()\n",
        "\n",
        "    # --- Extract frames ---\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    for i, (start, _) in enumerate(scene_list):\n",
        "        frame_num = int(start.get_seconds() * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "\n",
        "        frame_name = f\"scene_{i:03}.jpg\"\n",
        "        frame_path = os.path.join(keyframe_dir, frame_name)\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "\n",
        "        # Convert to PIL\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # --- Captioning with BLIP-2 ---\n",
        "        inputs = caption_processor(images=image, return_tensors=\"pt\").to(device, torch.float16 if device == \"cuda\" else torch.float32)\n",
        "        generated_ids = caption_model.generate(**inputs, max_new_tokens=50)\n",
        "        english_caption = caption_processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # --- Translate to Arabic ---\n",
        "        translation_inputs = translator_tokenizer(english_caption, return_tensors=\"pt\", padding=True).to(device)\n",
        "        translated = translator_model.generate(**translation_inputs)\n",
        "        arabic_caption = translator_tokenizer.decode(translated[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # --- Save result with scene start time ---\n",
        "        captions[frame_name] = {\n",
        "            \"scene_time\": round(start.get_seconds(), 2),  # Time in seconds, rounded for readability\n",
        "            \"english\": english_caption,\n",
        "            \"arabic\": arabic_caption\n",
        "        }\n",
        "\n",
        "        print(f\"âœ“ {frame_name} @ {start.get_seconds():.2f}s | EN: {english_caption} | AR: {arabic_caption}\")\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Save JSON\n",
        "    json_path = os.path.join(captions_base, f\"{video_name}.json\")\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"âœ… Captions saved to: {json_path}\")\n",
        "\n",
        "# ============ MAIN LOOP ============\n",
        "video_files = [f for f in os.listdir(videos_path) if f.lower().endswith(('.mp4', '.mov', '.avi', '.mkv'))]\n",
        "print(f\"ðŸŽ¬ Found {len(video_files)} videos.\")\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_name = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(videos_path, video_file)\n",
        "    print(f\"\\nðŸ”„ Processing: {video_file}\")\n",
        "    extract_and_caption(video_path, video_name)\n"
      ],
      "metadata": {
        "id": "hBfFY4xeKa5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}