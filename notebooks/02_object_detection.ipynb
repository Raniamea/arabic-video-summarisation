{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJXea/b1vql9WOF4Iakq6m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/notebooks/02_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If already mounted, unmount it safely\n",
        "!fusermount -u /content/drive\n",
        "\n",
        "# Then try mounting again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dP0l4sd4MXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install YOLOv8\n",
        "#!pip install -q ultralytics\n",
        "#from ultralytics import YOLO\n"
      ],
      "metadata": {
        "id": "NNKARG5VJqua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8 model\n",
        "#model = YOLO(\"yolov8n.pt\")  # Use 'n' for speed, 's' or 'm' for better accuracy\n"
      ],
      "metadata": {
        "id": "X0Ub8WAjJwjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "# Set paths\n",
        "video_path = \"/content/drive/MyDrive/ArabicVideoSummariser/videos/KhanElkhalili.mp4\"\n",
        "output_dir = \"/content/drive/MyDrive/ArabicVideoSummariser/keyframes\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 1. Extract keyframes (simple interval)\n",
        "def extract_keyframes(video_path, output_folder, interval=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    saved = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_count % interval == 0:\n",
        "            path = os.path.join(output_folder, f\"frame_{saved}.jpg\")\n",
        "            cv2.imwrite(path, frame)\n",
        "            saved += 1\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "extract_keyframes(video_path, output_dir, interval=60)\n",
        "\n",
        "# 2. Caption model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# 3. Translation model\n",
        "en_ar_model = \"Helsinki-NLP/opus-mt-en-ar\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(en_ar_model)\n",
        "translator = MarianMTModel.from_pretrained(en_ar_model).to(device)\n",
        "\n",
        "# 4. Process keyframes\n",
        "captions = {}\n",
        "\n",
        "for filename in os.listdir(output_dir):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        path = os.path.join(output_dir, filename)\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Captioning\n",
        "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
        "        output = model.generate(**inputs)\n",
        "        english = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Translation\n",
        "        translated = translator.generate(**tokenizer(english, return_tensors=\"pt\", padding=True).to(device))\n",
        "        arabic = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "        captions[filename] = {\"english\": english, \"arabic\": arabic}\n",
        "        print(f\"{filename}: EN: {english} | AR: {arabic}\")\n"
      ],
      "metadata": {
        "id": "hBfFY4xeKa5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}