{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/VideoProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa355c80",
      "metadata": {
        "id": "aa355c80"
      },
      "source": [
        "# ğŸ¥ Arabic Video Multimodal Validator and Summarizer\n",
        "\n",
        "This Colab notebook lets you input the name of an Arabic video file and automatically performs:\n",
        "- Audio transcription (Arabic)\n",
        "- Scene/keyframe caption validation using Sentence-BERT and CLIP\n",
        "- (Optional) Abstractive summarization with mBART"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”Š 1. Setup Environment"
      ],
      "metadata": {
        "id": "_z2ibBr631mn"
      },
      "id": "_z2ibBr631mn"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ebf7362d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf7362d",
        "outputId": "4d842564-6abd-43fb-8a4a-f36f52c9cd61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting scenedetect[opencv]\n",
            "  Downloading scenedetect-0.6.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (8.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (2.0.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.3.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scenedetect-0.6.6-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scenedetect, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.1 scenedetect-0.6.6\n"
          ]
        }
      ],
      "source": [
        "# Install Whisper and Torch\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install scenedetect[opencv] transformers accelerate bitsandbytes\n",
        "!pip install -q transformers sentence-transformers torchaudio opencv-python Pillow\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dTq6hem-Mwn",
        "outputId": "d5f48038-79e0-43a2-dc36-b7698434c955"
      },
      "id": "-dTq6hem-Mwn",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "Already unmounted\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "keyframes_path = os.path.join(base_path, \"keyframes\")\n",
        "os.makedirs(transcripts_path, exist_ok=True)\n",
        "os.makedirs(captions_path, exist_ok=True)\n",
        "os.makedirs(keyframes_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "MtOIIp158Tdz"
      },
      "id": "MtOIIp158Tdz",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”Š 2. Input Video Filename"
      ],
      "metadata": {
        "id": "kJfBHPBq3nqm"
      },
      "id": "kJfBHPBq3nqm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Video Filename\n",
        "video_filename = input(\"Enter the name of the video file (e.g., MyVideo.mp4): \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x1rnSV02iD4",
        "outputId": "38603875-1b17-40be-a3d3-f31ac24ec5d8"
      },
      "id": "-x1rnSV02iD4",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the name of the video file (e.g., MyVideo.mp4): Calligraphy.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n"
      ],
      "metadata": {
        "id": "zsXhIfDs4kO5"
      },
      "id": "zsXhIfDs4kO5",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ”Š 3. Transcribe Arabic Audio using Whisper\n",
        "\n"
      ],
      "metadata": {
        "id": "e6PjCMXNBaHf"
      },
      "id": "e6PjCMXNBaHf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31fa5413",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31fa5413",
        "outputId": "adde5d2b-d25d-43c2-d373-34dec652d77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.88G/2.88G [01:19<00:00, 38.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved Arabic transcript to: /content/drive/MyDrive/ArabicVideoSummariser/transcripts/Calligraphy_ar.txt\n",
            "âœ… Saved English translation to: /content/drive/MyDrive/ArabicVideoSummariser/transcripts/Calligraphy_en.txt\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"large\", device=\"cuda\")\n",
        "\n",
        "# transcribe (Arabic)\n",
        "result = model.transcribe(video_path, language=\"ar\", task=\"transcribe\")\n",
        "transcript_txt = os.path.join(transcripts_path, f\"{video_name}_ar.txt\")\n",
        "with open(transcript_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result['text'])\n",
        "print(f\"âœ… Saved Arabic transcript to: {transcript_txt}\")\n",
        "\n",
        "\n",
        "# Translate (Arabic â†’ English)\n",
        "result_en = model.transcribe(video_path, language=\"ar\", task=\"translate\")\n",
        "translation_txt = os.path.join(transcripts_path, f\"{video_name}_en.txt\")\n",
        "with open(translation_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result_en[\"text\"])\n",
        "print(f\"âœ… Saved English translation to: {translation_txt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ–¼ï¸ 4. KeyFrame Detection & Captioning"
      ],
      "metadata": {
        "id": "o1HpAGrQQ7ro"
      },
      "id": "o1HpAGrQQ7ro"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eonKPbj2UDHu"
      },
      "id": "eonKPbj2UDHu"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import json\n",
        "from PIL import Image\n",
        "from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# ============ SETUP ============\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# BLIP-2 model\n",
        "caption_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Translation model (EN â†’ AR)\n",
        "translator_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "translator_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\").to(device)\n",
        "\n",
        "\n",
        "keyframe_dir = os.path.join(keyframes_path, video_name)\n",
        "os.makedirs(keyframe_dir, exist_ok=True)\n",
        "captions = {}\n",
        "\n",
        "# --- Scene detection ---\n",
        "video_manager = VideoManager([video_path])\n",
        "scene_manager = SceneManager()\n",
        "scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
        "video_manager.set_downscale_factor()\n",
        "video_manager.start()\n",
        "scene_manager.detect_scenes(video_manager)\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "video_manager.release()\n",
        "\n",
        "# --- Extract frames ---\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "for i, (start, _) in enumerate(scene_list):\n",
        "  frame_num = int(start.get_seconds() * fps)\n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    continue\n",
        "\n",
        "  frame_name = f\"scene_{i:03}.jpg\"\n",
        "  frame_path = os.path.join(keyframe_dir, frame_name)\n",
        "  cv2.imwrite(frame_path, frame)\n",
        "\n",
        "  # Convert to PIL\n",
        "  image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  # --- Captioning with BLIP-2 ---\n",
        "  inputs = caption_processor(images=image, return_tensors=\"pt\").to(device, torch.float16 if device == \"cuda\" else torch.float32)\n",
        "  generated_ids = caption_model.generate(**inputs, max_new_tokens=50)\n",
        "  english_caption = caption_processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Translate to Arabic ---\n",
        "  translation_inputs = translator_tokenizer(english_caption, return_tensors=\"pt\", padding=True).to(device)\n",
        "  translated = translator_model.generate(**translation_inputs)\n",
        "  arabic_caption = translator_tokenizer.decode(translated[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Save result with scene start time ---\n",
        "  captions[frame_name] = {\n",
        "    \"scene_time\": round(start.get_seconds(), 2),  # Time in seconds, rounded for readability\n",
        "    \"english\": english_caption,\n",
        "    \"arabic\": arabic_caption\n",
        "    }\n",
        "\n",
        "  print(f\"âœ“ {frame_name} @ {start.get_seconds():.2f}s | EN: {english_caption} | AR: {arabic_caption}\")\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save JSON\n",
        "json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… Captions saved to: {json_path}\")"
      ],
      "metadata": {
        "id": "hBfFY4xeKa5n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hBfFY4xeKa5n"
    },
    {
      "cell_type": "markdown",
      "id": "eef4a65b",
      "metadata": {
        "id": "eef4a65b"
      },
      "source": [
        "## ğŸ§  5. Process Transcript into Overlapping Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa7a3d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aa7a3d4",
        "outputId": "01d63498-cd60-404d-da99-2ec215dac024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1667 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Chunk 1 ---\n",
            "ÙÙŠ Ù‚Ù„Ø¨ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ø­ÙŠØ« ØªØªØ¬Ø³Ø¯ Ø§Ù„Ø­Ø¶Ø§Ø±Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙÙŠ ÙƒÙ„ Ø²Ø§ÙˆÙŠØ© ÙŠØ±ÙˆÙŠ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø­ÙƒØ§ÙŠØ© Ø£Ø²Ù„ÙŠØ© Ø¹Ù† Ø§Ù„Ø¬Ù…Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ø¥Ù†Ù‡ Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø¬Ø±Ø¯ Ø®Ø· Ø¥Ù†Ù‡ ÙÙ† ØŒ Ø¥Ù†Ù‡ Ù‡ÙˆÙŠØ© Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… ÙŠØ¹Ù†ÙŠ Ù‡Ùˆ Ø¨Ù‚Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¨ ÙˆÙ„ÙƒÙ† ØªØ·ÙˆØ±Ù‡ Ø§Ø®ØªÙ„Ù Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… ÙØ§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…ÙˆØ¬ÙˆØ¯ ÙˆÙ„ÙƒÙ† Ù„Ø£Ù†Ù‡ Ù‡Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø¨Ø³Ø§Ø·Ø© Ø´Ø¯ÙŠØ¯Ø© Ù‡Ùˆ Ù…Ù† Ø¹Ø§Ø¦Ù„Ø© Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ù†Ø¨Ø§ØªÙŠØ© ÙˆÙ„ÙƒÙ† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù„ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù†Ø¯Ù†Ø§ ÙØ¹Ù„Ø§ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ø´ÙŠØ¡ ÙˆÙ…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø´ÙŠØ¡ ØªØ§Ù†ÙŠ Ø®Ø§Ù„Øµ ÙÙÙŠ Ø­Ø§Ø¬\n",
            "\n",
            "--- Chunk 2 ---\n",
            "##ÙƒÙ† ØªØ·ÙˆØ±Ù‡ Ø§Ø®ØªÙ„Ù Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… ÙØ§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…ÙˆØ¬ÙˆØ¯ ÙˆÙ„ÙƒÙ† Ù„Ø£Ù†Ù‡ Ù‡Ùˆ ÙŠØ¹Ù†ÙŠ Ø¨Ø¨Ø³Ø§Ø·Ø© Ø´Ø¯ÙŠØ¯Ø© Ù‡Ùˆ Ù…Ù† Ø¹Ø§Ø¦Ù„Ø© Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ù†Ø¨Ø§ØªÙŠØ© ÙˆÙ„ÙƒÙ† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù„ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù†Ø¯Ù†Ø§ ÙØ¹Ù„Ø§ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ø´ÙŠØ¡ ÙˆÙ…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø´ÙŠØ¡ ØªØ§Ù†ÙŠ Ø®Ø§Ù„Øµ ÙÙÙŠ Ø­Ø§Ø¬Ø© Ø­ØµÙ„Øª Ù…Ø¹ Ù†Ø²ÙˆÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙˆØªØªÙ†ÙˆØ¹ Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø­ÙŠØ« ÙŠØ±ÙˆÙŠ ÙƒÙ„ Ø®Ø· Ø¹Ù† Ø¹ØµØ± Ø¥ØµØ¯Ø§Ø±Ù‡ ÙÙŠÙ‡ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª ØªÙ…ÙŠØ² Ø¨Ù‡Ø§ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø·Ø¨Ø¹Ø§ Ù‡Ù… ÙƒØ°Ø§ Ù†ÙˆØ¹ ÙƒØªÙŠØ± Ù…Ù‡Ù…Ø§Ø´ Ø¨Ø³ Ù†Ø³Ø® ÙˆØ§Ù„Ø±Ù‚Ø¹ Ø²ÙŠ Ù…Ø§ Ø¨ÙŠØ¹Ù„Ù…ÙˆÙ†Ø§ ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ÙˆØ­ØªÙ‰ Ø§Ù„Ù†\n",
            "\n",
            "--- Chunk 3 ---\n",
            "##Ø© Ø­ØµÙ„Øª Ù…Ø¹ Ù†Ø²ÙˆÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙˆØªØªÙ†ÙˆØ¹ Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø­ÙŠØ« ÙŠØ±ÙˆÙŠ ÙƒÙ„ Ø®Ø· Ø¹Ù† Ø¹ØµØ± Ø¥ØµØ¯Ø§Ø±Ù‡ ÙÙŠÙ‡ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª ØªÙ…ÙŠØ² Ø¨Ù‡Ø§ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø·Ø¨Ø¹Ø§ Ù‡Ù… ÙƒØ°Ø§ Ù†ÙˆØ¹ ÙƒØªÙŠØ± Ù…Ù‡Ù…Ø§Ø´ Ø¨Ø³ Ù†Ø³Ø® ÙˆØ§Ù„Ø±Ù‚Ø¹ Ø²ÙŠ Ù…Ø§ Ø¨ÙŠØ¹Ù„Ù…ÙˆÙ†Ø§ ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ÙˆØ­ØªÙ‰ Ø§Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ø±Ù‚Ø¹ Ø¨ØªÙ‚ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø´ÙŠØ¡ ÙˆØ®Ø· Ø§Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ø±Ù‚Ø§Ø¹ ÙÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø´ÙŠØ¡ ØªØ§Ù†ÙŠ Ù‡Ù… Ø§Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ø±ÙŠØ­Ø§Ù† ÙˆØ§Ù„Ù…Ø­Ù‚Ù‚ ÙˆØ§Ù„Ø«Ù„Ø« ÙˆØ§Ù„Ø±Ù‚Ø§Ø¹ ÙˆØ§Ù„ØªÙˆÙ‚ÙŠØ¹ Ø¯ÙˆÙ„ Ø§Ù„Ø³ØªØ© Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù„ÙŠ Ù…Ù†Ù‡Ù… Ø·Ù„Ø¹ Ø­Ø§Ø¬Ø§Øª ÙƒØªÙŠØ±Ø© Ø¬Ø¯Ø§ ÙˆÙ„Ø£Ù† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„ÙŠØ³ Ù…Ø¬Ø±Ø¯\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Load tokenizer ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# === Load transcript text ===\n",
        "with open(transcript_txt, encoding=\"utf-8\") as f:\n",
        "    full_transcript = f.read()\n",
        "\n",
        "# === Tokenize in small overlapping windows ===\n",
        "tokens = tokenizer.tokenize(full_transcript)\n",
        "\n",
        "# Define chunking parameters\n",
        "chunk_size = 128\n",
        "step = 64\n",
        "\n",
        "# Generate safe overlapping chunks (as token strings)\n",
        "token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens) - chunk_size + 1, step)]\n",
        "\n",
        "# Convert token chunks back to readable strings\n",
        "text_chunks = [tokenizer.convert_tokens_to_string(chunk) for chunk in token_chunks]\n",
        "\n",
        "# Prepare model-ready input (â‰¤512 tokens, padded)\n",
        "tokenized_chunks = [\n",
        "    tokenizer(chunk_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
        "    for chunk_text in text_chunks\n",
        "]\n",
        "\n",
        "# Preview\n",
        "for i, chunk in enumerate(text_chunks[:3]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd49d45",
      "metadata": {
        "id": "8fd49d45"
      },
      "source": [
        "## ğŸ–¼ï¸ 6. Load Scene Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32077c3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "32077c3a",
        "outputId": "b631cf1c-7aba-46de-9a4e-a8f49e989e5d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ArabicVideoSummariser/captions/Calligraphy.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-3469840728.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcaptions_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{os.path.splitext(video_filename)[0]}.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mscenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscene_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arabic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscenes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ArabicVideoSummariser/captions/Calligraphy.json'"
          ]
        }
      ],
      "source": [
        "# Load captions from JSON\n",
        "import json\n",
        "captions_json = os.path.join(captions_path, f\"{os.path.splitext(video_filename)[0]}.json\")\n",
        "with open(captions_json, encoding='utf-8') as f:\n",
        "    scenes = json.load(f)\n",
        "scene_captions = [(scene, data[\"arabic\"]) for scene, data in scenes.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fea4337",
      "metadata": {
        "id": "9fea4337"
      },
      "source": [
        "## ğŸ”¡ 7. Embed Captions and Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20958ab",
      "metadata": {
        "id": "d20958ab"
      },
      "outputs": [],
      "source": [
        "# Encode using multilingual Sentence-BERT\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "caption_texts = [text for _, text in scene_captions]\n",
        "caption_embeddings = model.encode(caption_texts, convert_to_tensor=True)\n",
        "transcript_embeddings = model.encode(transcript_chunks, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501f8e8c",
      "metadata": {
        "id": "501f8e8c"
      },
      "source": [
        "## ğŸ”— 8. Match Captions to Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80225615",
      "metadata": {
        "id": "80225615"
      },
      "outputs": [],
      "source": [
        "# Find best transcript match for each caption\n",
        "results = []\n",
        "similarities = util.cos_sim(caption_embeddings, transcript_embeddings)\n",
        "for i, (scene_id, caption_text) in enumerate(scene_captions):\n",
        "    sim_scores = similarities[i]\n",
        "    top_idx = sim_scores.argmax().item()\n",
        "    results.append({\n",
        "        \"scene_id\": scene_id,\n",
        "        \"caption\": caption_text,\n",
        "        \"best_transcript_chunk\": transcript_chunks[top_idx],\n",
        "        \"similarity_score\": float(sim_scores[top_idx])\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c0e9ff",
      "metadata": {
        "id": "00c0e9ff"
      },
      "source": [
        "## ğŸ“¥ 9. Output Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458b6474",
      "metadata": {
        "id": "458b6474"
      },
      "outputs": [],
      "source": [
        "# Display a few matches\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df[['scene_id', 'caption', 'best_transcript_chunk', 'similarity_score']].head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e6PjCMXNBaHf"
      ],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}