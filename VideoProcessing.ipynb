{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/VideoProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa355c80",
      "metadata": {
        "id": "aa355c80"
      },
      "source": [
        "# üé• Arabic Video Multimodal Validator and Summarizer\n",
        "\n",
        "This Colab notebook lets you input the name of an Arabic video file and automatically performs:\n",
        "- Audio transcription (Arabic)\n",
        "- Scene/keyframe caption validation using Sentence-BERT and CLIP\n",
        "- (Optional) Abstractive summarization with mBART"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚ôªÔ∏è 1. Setup Environment"
      ],
      "metadata": {
        "id": "_z2ibBr631mn"
      },
      "id": "_z2ibBr631mn"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ebf7362d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf7362d",
        "outputId": "890eebff-2d9a-41af-cd60-0b2530a20d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/CAMeL-Lab/camel_tools.git@master\n",
            "  Cloning https://github.com/CAMeL-Lab/camel_tools.git (to revision master) to /tmp/pip-req-build-eku8vvt1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/CAMeL-Lab/camel_tools.git /tmp/pip-req-build-eku8vvt1\n",
            "  Resolved https://github.com/CAMeL-Lab/camel_tools.git to commit 17b4d31c4d1909f05dafeaf7e147f747bec31256\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (1.17.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (5.5.2)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (1.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (0.3.8)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers<4.44.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (4.38.2)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (2.32.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (2.14.1)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (0.20.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (4.67.1)\n",
            "Requirement already satisfied: muddler in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (0.1.3)\n",
            "Requirement already satisfied: camel-kenlm>=2025.4.8 in /usr/local/lib/python3.11/dist-packages (from camel_tools==1.5.6) (2025.4.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel_tools==1.5.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->camel_tools==1.5.6) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel_tools==1.5.6) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel_tools==1.5.6) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel_tools==1.5.6) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel_tools==1.5.6) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel_tools==1.5.6) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel_tools==1.5.6) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->camel_tools==1.5.6) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->camel_tools==1.5.6) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->camel_tools==1.5.6) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->camel_tools==1.5.6) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->camel_tools==1.5.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->camel_tools==1.5.6) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->camel_tools==1.5.6) (2025.7.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel_tools==1.5.6) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel_tools==1.5.6) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers<4.44.0,>=4.0->camel_tools==1.5.6) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->camel_tools==1.5.6) (3.0.2)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install \"numpy<2.0\" \"tokenizers>=0.14,<0.19\" --quiet\n",
        "!pip install git+https://github.com/CAMeL-Lab/camel_tools.git@master\n",
        "!pip install scenedetect==0.6.6 opencv-python==4.7.0.72 --quiet\n",
        "\n",
        "!pip install \"transformers==4.38.2\" \"sentence-transformers\" \"sacremoses\" Pillow --quiet\n",
        "!pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "!pip install git+https://github.com/openai/whisper.git --quiet\n",
        "\n",
        "!pip install bitsandbytes accelerate --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "7TimDWmjOC2b"
      },
      "id": "7TimDWmjOC2b",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dTq6hem-Mwn",
        "outputId": "b15371e4-1e9e-47e9-d01d-670c716d2b79"
      },
      "id": "-dTq6hem-Mwn",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "keyframes_path = os.path.join(base_path, \"keyframes\")\n",
        "os.makedirs(transcripts_path, exist_ok=True)\n",
        "os.makedirs(captions_path, exist_ok=True)\n",
        "os.makedirs(keyframes_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "MtOIIp158Tdz"
      },
      "id": "MtOIIp158Tdz",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ñ∂Ô∏è 2. Input Video Filename"
      ],
      "metadata": {
        "id": "kJfBHPBq3nqm"
      },
      "id": "kJfBHPBq3nqm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Video Filename\n",
        "video_filename = input(\"Enter the name of the video file (e.g., MyVideo.mp4): \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x1rnSV02iD4",
        "outputId": "fd02247b-123a-4b44-fc4d-c7d2e917aab5"
      },
      "id": "-x1rnSV02iD4",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the name of the video file (e.g., MyVideo.mp4): Calligraphy.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar.txt\")\n",
        "translation_path = os.path.join(transcripts_path, f\"{video_name}_en.txt\")\n",
        "keyframe_dir = os.path.join(keyframes_path, video_name)\n",
        "os.makedirs(keyframe_dir, exist_ok=True)\n",
        "json_path = os.path.join(captions_path, f\"{video_name}.json\")\n"
      ],
      "metadata": {
        "id": "zsXhIfDs4kO5"
      },
      "id": "zsXhIfDs4kO5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üîä 3. Transcribe Arabic Audio using Whisper\n",
        "\n"
      ],
      "metadata": {
        "id": "e6PjCMXNBaHf"
      },
      "id": "e6PjCMXNBaHf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31fa5413",
      "metadata": {
        "id": "31fa5413"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"large\", device=\"cuda\")\n",
        "\n",
        "# transcribe (Arabic)\n",
        "result = model.transcribe(video_path, language=\"ar\", task=\"transcribe\")\n",
        "\n",
        "with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result['text'])\n",
        "print(f\"‚úÖ Saved Arabic transcript to: {transcript_path}\")\n",
        "\n",
        "\n",
        "# Translate (Arabic ‚Üí English)\n",
        "result_en = model.transcribe(video_path, language=\"ar\", task=\"translate\")\n",
        "with open(translation_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result_en[\"text\"])\n",
        "print(f\"‚úÖ Saved English translation to: {translation_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñºÔ∏è 4. KeyFrame Detection & Captioning"
      ],
      "metadata": {
        "id": "o1HpAGrQQ7ro"
      },
      "id": "o1HpAGrQQ7ro"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import json\n",
        "from PIL import Image\n",
        "from scenedetect import open_video, SceneManager\n",
        "#from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "\n",
        "# ============ SETUP ============\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# BLIP-2 model\n",
        "caption_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\",use_fast=False)\n",
        "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Translation model (EN ‚Üí AR)\n",
        "translator_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "translator_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\").to(device)\n",
        "\n",
        "\n",
        "\n",
        "captions = {}\n",
        "\n",
        "scene_manager = SceneManager()\n",
        "scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
        "video = open_video(video_path)\n",
        "scene_manager.detect_scenes(video)\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "# --- Extract frames ---\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "for i, (start, _) in enumerate(scene_list):\n",
        "  frame_num = int(start.get_seconds() * fps)\n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    continue\n",
        "\n",
        "  frame_name = f\"scene_{i:03}.jpg\"\n",
        "  frame_path = os.path.join(keyframe_dir, frame_name)\n",
        "  cv2.imwrite(frame_path, frame)\n",
        "\n",
        "  # Convert to PIL\n",
        "  image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  # --- Captioning with BLIP-2 ---\n",
        "  inputs = caption_processor(images=image, return_tensors=\"pt\").to(device, torch.float16 if device == \"cuda\" else torch.float32)\n",
        "  generated_ids = caption_model.generate(**inputs, max_new_tokens=50)\n",
        "  english_caption = caption_processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Translate to Arabic ---\n",
        "  translation_inputs = translator_tokenizer(english_caption, return_tensors=\"pt\", padding=True).to(device)\n",
        "  translated = translator_model.generate(**translation_inputs)\n",
        "  arabic_caption = translator_tokenizer.decode(translated[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Save result with scene start time ---\n",
        "  captions[frame_name] = {\n",
        "    \"scene_time\": round(start.get_seconds(), 2),  # Time in seconds, rounded for readability\n",
        "    \"english\": english_caption,\n",
        "    \"arabic\": arabic_caption\n",
        "    }\n",
        "\n",
        "  print(f\"‚úì {frame_name} @ {start.get_seconds():.2f}s | EN: {english_caption} | AR: {arabic_caption}\")\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save JSON\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úÖ Captions saved to: {json_path}\")"
      ],
      "metadata": {
        "id": "hBfFY4xeKa5n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hBfFY4xeKa5n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßπ 5. Clean the Script"
      ],
      "metadata": {
        "id": "ThznxocQv7uh"
      },
      "id": "ThznxocQv7uh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Clean the Transcript"
      ],
      "metadata": {
        "id": "4EykBV4P8EBl"
      },
      "id": "4EykBV4P8EBl"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.utils.charmap import CharMapper\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "\n",
        "# ==== Setup ====\n",
        "normalizer = CharMapper.builtin_mapper('arclean')\n",
        "\n",
        "disambig = MLEDisambiguator.pretrained()\n",
        "\n",
        "# ==== Function: Clean, Segment, POS tag ====\n",
        "def arabic_segment_and_analyze(text):\n",
        "    # Normalize Arabic text (removes Tatweel, unifies alef/ya, strips diacritics)\n",
        "    normalized_text = normalizer.map_string(text)\n",
        "\n",
        "    # Insert sentence breaks based on conjunctions and commas\n",
        "    segmented = re.sub(r'(?<=\\S)\\s+(?=(Ÿà|ÿ´ŸÖ|ŸÑŸÉŸÜ|ŸÅ|ÿ®ÿπÿØ|ÿ•ŸÑÿß ÿ£ŸÜ|ÿ∫Ÿäÿ± ÿ£ŸÜ)\\s)', r'. ', normalized_text)\n",
        "    segmented = re.sub(r'ÿå|\\.\\s*', '.\\n', segmented)\n",
        "    sentences = [s.strip() for s in segmented.split('\\n') if s.strip()]\n",
        "\n",
        "    # POS tagging\n",
        "    results = []\n",
        "    for sent in sentences:\n",
        "        tokens = simple_word_tokenize(sent)\n",
        "        disambig_results = disambig.disambiguate(tokens)\n",
        "        tagged = [(tok, d.analyses[0].analysis['pos']) for tok, d in zip(tokens, disambig_results)]\n",
        "        results.append({\n",
        "            \"sentence\": sent,\n",
        "            \"tokens_with_pos\": tagged\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# ==== File Paths ====\n",
        "output_txt = os.path.join(transcripts_path, f\"{video_name}_segmented_ar.txt\")\n",
        "\n",
        "# ==== Run segmentation ====\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "segmented = arabic_segment_and_analyze(raw_text)\n",
        "\n",
        "# ==== Save output ====\n",
        "with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in segmented:\n",
        "        f.write(item['sentence'] + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Segmented transcript saved to: {output_txt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-lXmHon_n5G",
        "outputId": "3e637f11-0002-49d4-eab3-3f0807ace201"
      },
      "id": "m-lXmHon_n5G",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Segmented transcript saved to: /content/drive/MyDrive/ArabicVideoSummariser/transcripts/Calligraphy_segmented_ar.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Clean the captions"
      ],
      "metadata": {
        "id": "AGZ5Ydcc8MQa"
      },
      "id": "AGZ5Ydcc8MQa"
    },
    {
      "cell_type": "markdown",
      "id": "eef4a65b",
      "metadata": {
        "id": "eef4a65b"
      },
      "source": [
        "## üß† 6. Process Transcript into Overlapping Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa7a3d4",
      "metadata": {
        "id": "1aa7a3d4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Load tokenizer ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# === Load transcript text ===\n",
        "with open(transcript_txt, encoding=\"utf-8\") as f:\n",
        "    full_transcript = f.read()\n",
        "\n",
        "# === Tokenize in small overlapping windows ===\n",
        "tokens = tokenizer.tokenize(full_transcript)\n",
        "\n",
        "# Define chunking parameters\n",
        "chunk_size = 128\n",
        "step = 64\n",
        "\n",
        "# Generate safe overlapping chunks (as token strings)\n",
        "token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens) - chunk_size + 1, step)]\n",
        "\n",
        "# Convert token chunks back to readable strings\n",
        "text_chunks = [tokenizer.convert_tokens_to_string(chunk) for chunk in token_chunks]\n",
        "\n",
        "# Prepare model-ready input (‚â§512 tokens, padded)\n",
        "tokenized_chunks = [\n",
        "    tokenizer(chunk_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
        "    for chunk_text in text_chunks\n",
        "]\n",
        "\n",
        "# Preview\n",
        "for i, chunk in enumerate(text_chunks[:3]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd49d45",
      "metadata": {
        "id": "8fd49d45"
      },
      "source": [
        "## üñºÔ∏è 7. Load Scene Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32077c3a",
      "metadata": {
        "id": "32077c3a"
      },
      "outputs": [],
      "source": [
        "# Load captions from JSON\n",
        "import json\n",
        "captions_json = os.path.join(captions_path, f\"{os.path.splitext(video_filename)[0]}.json\")\n",
        "with open(captions_json, encoding='utf-8') as f:\n",
        "    scenes = json.load(f)\n",
        "scene_captions = [(scene, data[\"arabic\"]) for scene, data in scenes.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fea4337",
      "metadata": {
        "id": "9fea4337"
      },
      "source": [
        "## üî° 8. Embed Captions and Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20958ab",
      "metadata": {
        "id": "d20958ab"
      },
      "outputs": [],
      "source": [
        "# Encode using multilingual Sentence-BERT\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "caption_texts = [text for _, text in scene_captions]\n",
        "caption_embeddings = model.encode(caption_texts, convert_to_tensor=True)\n",
        "transcript_embeddings = model.encode(transcript_chunks, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501f8e8c",
      "metadata": {
        "id": "501f8e8c"
      },
      "source": [
        "## üîó 9. Match Captions to Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80225615",
      "metadata": {
        "id": "80225615"
      },
      "outputs": [],
      "source": [
        "# Find best transcript match for each caption\n",
        "results = []\n",
        "similarities = util.cos_sim(caption_embeddings, transcript_embeddings)\n",
        "for i, (scene_id, caption_text) in enumerate(scene_captions):\n",
        "    sim_scores = similarities[i]\n",
        "    top_idx = sim_scores.argmax().item()\n",
        "    results.append({\n",
        "        \"scene_id\": scene_id,\n",
        "        \"caption\": caption_text,\n",
        "        \"best_transcript_chunk\": transcript_chunks[top_idx],\n",
        "        \"similarity_score\": float(sim_scores[top_idx])\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c0e9ff",
      "metadata": {
        "id": "00c0e9ff"
      },
      "source": [
        "## üì• 10. Output Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458b6474",
      "metadata": {
        "id": "458b6474"
      },
      "outputs": [],
      "source": [
        "# Display a few matches\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df[['scene_id', 'caption', 'best_transcript_chunk', 'similarity_score']].head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_z2ibBr631mn",
        "kJfBHPBq3nqm",
        "e6PjCMXNBaHf",
        "o1HpAGrQQ7ro",
        "eef4a65b",
        "9fea4337",
        "501f8e8c",
        "00c0e9ff"
      ],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}