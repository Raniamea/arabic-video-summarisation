{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/VideoProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa355c80",
      "metadata": {
        "id": "aa355c80"
      },
      "source": [
        "# ğŸ¥ Arabic Video Multimodal Validator and Summarizer\n",
        "\n",
        "This Colab notebook lets you input the name of an Arabic video file and automatically performs:\n",
        "- Audio transcription (Arabic)\n",
        "- Scene/keyframe caption validation using Sentence-BERT and CLIP\n",
        "- (Optional) Abstractive summarization with mBART"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##â™»ï¸ 1. Setup Environment"
      ],
      "metadata": {
        "id": "_z2ibBr631mn"
      },
      "id": "_z2ibBr631mn"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ebf7362d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ebf7362d",
        "outputId": "614d463e-38ee-4278-b764-43cb0424dc78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting scenedetect[opencv]\n",
            "  Downloading scenedetect-0.6.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (8.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (2.0.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.3.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scenedetect-0.6.6-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scenedetect, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.1 scenedetect-0.6.6\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n",
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.5.6-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.17.0)\n",
            "Collecting docopt (from camel-tools)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel-tools) (5.5.2)\n",
            "Collecting numpy<2 (from camel-tools)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.3.8)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.6.0+cu124)\n",
            "Collecting transformers<4.44.0,>=4.0 (from camel-tools)\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.32.3)\n",
            "Collecting emoji (from camel-tools)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyrsistent (from camel-tools)\n",
            "  Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from camel-tools) (4.67.1)\n",
            "Collecting muddler (from camel-tools)\n",
            "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting camel-kenlm>=2025.4.8 (from camel-tools)\n",
            "  Downloading camel-kenlm-2025.4.8.zip (556 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.5/556.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.5.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers<4.44.0,>=4.0->camel-tools)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2025.7.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers<4.44.0,>=4.0->camel-tools) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n",
            "Downloading camel_tools-1.5.6-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
            "Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: camel-kenlm, docopt\n",
            "  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2025.4.8-cp311-cp311-linux_x86_64.whl size=3455595 sha256=324c548e64f9c9f5b1e2c6ca3341fe681ac3126a2b311aefce0a59c41db24614\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/b9/62/8559aee1915ae6690fcc902a972a9ba0ff46d3ee67fea2aa44\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6a1560e2da812761624b646f6cd2bf26533e39af4ad8dc1c99346669836b89df\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built camel-kenlm docopt\n",
            "Installing collected packages: docopt, camel-kenlm, pyrsistent, numpy, muddler, emoji, tokenizers, transformers, camel-tools\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.3\n",
            "    Uninstalling transformers-4.53.3:\n",
            "      Successfully uninstalled transformers-4.53.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed camel-kenlm-2025.4.8 camel-tools-1.5.6 docopt-0.6.2 emoji-2.14.1 muddler-0.1.3 numpy-1.26.4 pyrsistent-0.20.0 tokenizers-0.19.1 transformers-4.43.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e66785d2bf0b4a0fb4503616c4cc7cc7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install Whisper and Torch\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install scenedetect[opencv] transformers accelerate bitsandbytes\n",
        "!pip install -q transformers sentence-transformers torchaudio opencv-python Pillow\n",
        "!pip install sacremoses\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-dTq6hem-Mwn"
      },
      "id": "-dTq6hem-Mwn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "keyframes_path = os.path.join(base_path, \"keyframes\")\n",
        "os.makedirs(transcripts_path, exist_ok=True)\n",
        "os.makedirs(captions_path, exist_ok=True)\n",
        "os.makedirs(keyframes_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "MtOIIp158Tdz"
      },
      "id": "MtOIIp158Tdz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â–¶ï¸ 2. Input Video Filename"
      ],
      "metadata": {
        "id": "kJfBHPBq3nqm"
      },
      "id": "kJfBHPBq3nqm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Video Filename\n",
        "video_filename = input(\"Enter the name of the video file (e.g., MyVideo.mp4): \")"
      ],
      "metadata": {
        "id": "-x1rnSV02iD4"
      },
      "id": "-x1rnSV02iD4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n"
      ],
      "metadata": {
        "id": "zsXhIfDs4kO5"
      },
      "id": "zsXhIfDs4kO5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ”Š 3. Transcribe Arabic Audio using Whisper\n",
        "\n"
      ],
      "metadata": {
        "id": "e6PjCMXNBaHf"
      },
      "id": "e6PjCMXNBaHf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31fa5413",
      "metadata": {
        "id": "31fa5413"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"large\", device=\"cuda\")\n",
        "\n",
        "# transcribe (Arabic)\n",
        "result = model.transcribe(video_path, language=\"ar\", task=\"transcribe\")\n",
        "transcript_txt = os.path.join(transcripts_path, f\"{video_name}_ar.txt\")\n",
        "with open(transcript_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result['text'])\n",
        "print(f\"âœ… Saved Arabic transcript to: {transcript_txt}\")\n",
        "\n",
        "\n",
        "# Translate (Arabic â†’ English)\n",
        "result_en = model.transcribe(video_path, language=\"ar\", task=\"translate\")\n",
        "translation_txt = os.path.join(transcripts_path, f\"{video_name}_en.txt\")\n",
        "with open(translation_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result_en[\"text\"])\n",
        "print(f\"âœ… Saved English translation to: {translation_txt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ–¼ï¸ 4. KeyFrame Detection & Captioning"
      ],
      "metadata": {
        "id": "o1HpAGrQQ7ro"
      },
      "id": "o1HpAGrQQ7ro"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import json\n",
        "from PIL import Image\n",
        "from scenedetect import open_video, SceneManager\n",
        "#from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "\n",
        "# ============ SETUP ============\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# BLIP-2 model\n",
        "caption_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\",use_fast=False)\n",
        "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Translation model (EN â†’ AR)\n",
        "translator_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "translator_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\").to(device)\n",
        "\n",
        "\n",
        "keyframe_dir = os.path.join(keyframes_path, video_name)\n",
        "os.makedirs(keyframe_dir, exist_ok=True)\n",
        "captions = {}\n",
        "\n",
        "scene_manager = SceneManager()\n",
        "scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
        "video = open_video(video_path)\n",
        "scene_manager.detect_scenes(video)\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "# --- Extract frames ---\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "for i, (start, _) in enumerate(scene_list):\n",
        "  frame_num = int(start.get_seconds() * fps)\n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    continue\n",
        "\n",
        "  frame_name = f\"scene_{i:03}.jpg\"\n",
        "  frame_path = os.path.join(keyframe_dir, frame_name)\n",
        "  cv2.imwrite(frame_path, frame)\n",
        "\n",
        "  # Convert to PIL\n",
        "  image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  # --- Captioning with BLIP-2 ---\n",
        "  inputs = caption_processor(images=image, return_tensors=\"pt\").to(device, torch.float16 if device == \"cuda\" else torch.float32)\n",
        "  generated_ids = caption_model.generate(**inputs, max_new_tokens=50)\n",
        "  english_caption = caption_processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Translate to Arabic ---\n",
        "  translation_inputs = translator_tokenizer(english_caption, return_tensors=\"pt\", padding=True).to(device)\n",
        "  translated = translator_model.generate(**translation_inputs)\n",
        "  arabic_caption = translator_tokenizer.decode(translated[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Save result with scene start time ---\n",
        "  captions[frame_name] = {\n",
        "    \"scene_time\": round(start.get_seconds(), 2),  # Time in seconds, rounded for readability\n",
        "    \"english\": english_caption,\n",
        "    \"arabic\": arabic_caption\n",
        "    }\n",
        "\n",
        "  print(f\"âœ“ {frame_name} @ {start.get_seconds():.2f}s | EN: {english_caption} | AR: {arabic_caption}\")\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save JSON\n",
        "json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… Captions saved to: {json_path}\")"
      ],
      "metadata": {
        "id": "hBfFY4xeKa5n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hBfFY4xeKa5n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§¹ 5. Clean the Script"
      ],
      "metadata": {
        "id": "ThznxocQv7uh"
      },
      "id": "ThznxocQv7uh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Clean the Transcript"
      ],
      "metadata": {
        "id": "4EykBV4P8EBl"
      },
      "id": "4EykBV4P8EBl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Clean the captions"
      ],
      "metadata": {
        "id": "AGZ5Ydcc8MQa"
      },
      "id": "AGZ5Ydcc8MQa"
    },
    {
      "cell_type": "markdown",
      "id": "eef4a65b",
      "metadata": {
        "id": "eef4a65b"
      },
      "source": [
        "## ğŸ§  6. Process Transcript into Overlapping Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa7a3d4",
      "metadata": {
        "id": "1aa7a3d4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Load tokenizer ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# === Load transcript text ===\n",
        "with open(transcript_txt, encoding=\"utf-8\") as f:\n",
        "    full_transcript = f.read()\n",
        "\n",
        "# === Tokenize in small overlapping windows ===\n",
        "tokens = tokenizer.tokenize(full_transcript)\n",
        "\n",
        "# Define chunking parameters\n",
        "chunk_size = 128\n",
        "step = 64\n",
        "\n",
        "# Generate safe overlapping chunks (as token strings)\n",
        "token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens) - chunk_size + 1, step)]\n",
        "\n",
        "# Convert token chunks back to readable strings\n",
        "text_chunks = [tokenizer.convert_tokens_to_string(chunk) for chunk in token_chunks]\n",
        "\n",
        "# Prepare model-ready input (â‰¤512 tokens, padded)\n",
        "tokenized_chunks = [\n",
        "    tokenizer(chunk_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
        "    for chunk_text in text_chunks\n",
        "]\n",
        "\n",
        "# Preview\n",
        "for i, chunk in enumerate(text_chunks[:3]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd49d45",
      "metadata": {
        "id": "8fd49d45"
      },
      "source": [
        "## ğŸ–¼ï¸ 7. Load Scene Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32077c3a",
      "metadata": {
        "id": "32077c3a"
      },
      "outputs": [],
      "source": [
        "# Load captions from JSON\n",
        "import json\n",
        "captions_json = os.path.join(captions_path, f\"{os.path.splitext(video_filename)[0]}.json\")\n",
        "with open(captions_json, encoding='utf-8') as f:\n",
        "    scenes = json.load(f)\n",
        "scene_captions = [(scene, data[\"arabic\"]) for scene, data in scenes.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fea4337",
      "metadata": {
        "id": "9fea4337"
      },
      "source": [
        "## ğŸ”¡ 8. Embed Captions and Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20958ab",
      "metadata": {
        "id": "d20958ab"
      },
      "outputs": [],
      "source": [
        "# Encode using multilingual Sentence-BERT\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "caption_texts = [text for _, text in scene_captions]\n",
        "caption_embeddings = model.encode(caption_texts, convert_to_tensor=True)\n",
        "transcript_embeddings = model.encode(transcript_chunks, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501f8e8c",
      "metadata": {
        "id": "501f8e8c"
      },
      "source": [
        "## ğŸ”— 9. Match Captions to Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80225615",
      "metadata": {
        "id": "80225615"
      },
      "outputs": [],
      "source": [
        "# Find best transcript match for each caption\n",
        "results = []\n",
        "similarities = util.cos_sim(caption_embeddings, transcript_embeddings)\n",
        "for i, (scene_id, caption_text) in enumerate(scene_captions):\n",
        "    sim_scores = similarities[i]\n",
        "    top_idx = sim_scores.argmax().item()\n",
        "    results.append({\n",
        "        \"scene_id\": scene_id,\n",
        "        \"caption\": caption_text,\n",
        "        \"best_transcript_chunk\": transcript_chunks[top_idx],\n",
        "        \"similarity_score\": float(sim_scores[top_idx])\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c0e9ff",
      "metadata": {
        "id": "00c0e9ff"
      },
      "source": [
        "## ğŸ“¥ 10. Output Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458b6474",
      "metadata": {
        "id": "458b6474"
      },
      "outputs": [],
      "source": [
        "# Display a few matches\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df[['scene_id', 'caption', 'best_transcript_chunk', 'similarity_score']].head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_z2ibBr631mn",
        "kJfBHPBq3nqm",
        "e6PjCMXNBaHf",
        "o1HpAGrQQ7ro",
        "eef4a65b",
        "9fea4337",
        "501f8e8c",
        "00c0e9ff"
      ],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}