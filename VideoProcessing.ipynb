{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/VideoProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa355c80",
      "metadata": {
        "id": "aa355c80"
      },
      "source": [
        "# ğŸ¥ Arabic Video Multimodal Validator and Summarizer\n",
        "\n",
        "This Colab notebook lets you input the name of an Arabic video file and automatically performs:\n",
        "- Audio transcription (Arabic)\n",
        "- Scene/keyframe caption validation using Sentence-BERT and CLIP\n",
        "- (Optional) Abstractive summarization with mBART"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##â™»ï¸ 1. Setup Environment"
      ],
      "metadata": {
        "id": "_z2ibBr631mn"
      },
      "id": "_z2ibBr631mn"
    },
    {
      "cell_type": "code",
      "source": [
        "# === CORE BUILD TOOLS ===\n",
        "!pip install -U pip setuptools wheel --quiet\n",
        "# === TORCH (CUDA 11.8) ===\n",
        "!pip install torch==2.0.1 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "# === NUMPY (compatible with OpenCV, Whisper, SceneDetect) ===\n",
        "!pip install numpy==1.24.4 --quiet\n",
        "# === OpenCV + SceneDetect ===\n",
        "!pip install opencv-python==4.7.0.72 opencv-contrib-python==4.7.0.72 scenedetect==0.6.6 --quiet\n",
        "# === Whisper for ASR ===\n",
        "!pip install git+https://github.com/openai/whisper.git --quiet\n",
        "# === Transformers for BLIP-2, MarianMT, etc. ===\n",
        "!pip install transformers==4.38.2 tokenizers>=0.14,<0.19 sentence-transformers sacremoses --quiet\n",
        "# === CAMeL Tools (for Arabic sentence tokenization etc.) ===\n",
        "!pip install git+https://github.com/CAMeL-Lab/camel_tools.git@master --quiet\n",
        "# === Audio & Image processing ===\n",
        "!pip install librosa==0.10.0.post2 soundfile Pillow --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m_XLlNaQJ7k",
        "outputId": "aa125d23-966a-4ac2-9dbc-b872f7477c28"
      },
      "id": "6m_XLlNaQJ7k",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.18.0 requires numpy>=1.12.0, which is not installed.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cufflinks 0.17.3 requires numpy>=1.9.2, which is not installed.\n",
            "pandas-gbq 0.29.2 requires numpy>=1.18.1, which is not installed.\n",
            "pytensor 2.31.7 requires numpy>=1.17.0, which is not installed.\n",
            "spanner-graph-notebook 1.1.6 requires numpy, which is not installed.\n",
            "spacy 3.8.7 requires numpy>=1.19.0; python_version >= \"3.9\", which is not installed.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "bqplot 0.12.45 requires numpy>=1.10.4, which is not installed.\n",
            "osqp 1.0.4 requires numpy>=1.7, which is not installed.\n",
            "bigframes 2.12.0 requires numpy>=1.24.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m148.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'lit' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'lit'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [torchaudio]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.9.0 requires numpy<3.0.0,>=1.17, which is not installed.\n",
            "peft 0.16.0 requires numpy>=1.17, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.10.6 requires jax>=0.5.1, which is not installed.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "optax 0.2.5 requires jax>=0.4.27, which is not installed.\n",
            "optax 0.2.5 requires jaxlib>=0.4.27, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "chex 0.1.90 requires jax>=0.4.27, which is not installed.\n",
            "chex 0.1.90 requires jaxlib>=0.4.27, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "orbax-checkpoint 0.11.19 requires jax>=0.5.0, which is not installed.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "scipy 1.16.0 requires numpy<2.6,>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [opencv-contrib-python]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/bin/bash: line 1: 0.19: No such file or directory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.5/556.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for camel_tools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9/9\u001b[0m [camel_tools]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.10.6 requires jax>=0.5.1, which is not installed.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "optax 0.2.5 requires jax>=0.4.27, which is not installed.\n",
            "optax 0.2.5 requires jaxlib>=0.4.27, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "chex 0.1.90 requires jax>=0.4.27, which is not installed.\n",
            "chex 0.1.90 requires jaxlib>=0.4.27, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "orbax-checkpoint 0.11.19 requires jax>=0.5.0, which is not installed.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [librosa]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Mount Google Drive & Define Folder Paths"
      ],
      "metadata": {
        "id": "e_VWwOEolSNk"
      },
      "id": "e_VWwOEolSNk"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "7TimDWmjOC2b"
      },
      "id": "7TimDWmjOC2b",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dTq6hem-Mwn",
        "outputId": "c4f15a56-ee2f-447b-f80c-1b98733c590d"
      },
      "id": "-dTq6hem-Mwn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base paths\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "keyframes_path = os.path.join(base_path, \"keyframes\")\n",
        "os.makedirs(transcripts_path, exist_ok=True)\n",
        "os.makedirs(captions_path, exist_ok=True)\n",
        "os.makedirs(keyframes_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "MtOIIp158Tdz"
      },
      "id": "MtOIIp158Tdz",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â–¶ï¸ 3. Input Video Filename"
      ],
      "metadata": {
        "id": "kJfBHPBq3nqm"
      },
      "id": "kJfBHPBq3nqm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Video Filename\n",
        "video_filename = input(\"Enter the name of the video file (e.g., MyVideo.mp4): \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x1rnSV02iD4",
        "outputId": "9df760e6-d786-4a6b-ef1d-b149bdf05183"
      },
      "id": "-x1rnSV02iD4",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the name of the video file (e.g., MyVideo.mp4): Calligraphy.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar.txt\")\n",
        "translation_path = os.path.join(transcripts_path, f\"{video_name}_en.txt\")\n",
        "keyframe_dir = os.path.join(keyframes_path, video_name)\n",
        "os.makedirs(keyframe_dir, exist_ok=True)\n",
        "captions_json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "trascription_json_path = os.path.join(transcripts_path, f\"{video_name}.json\")\n"
      ],
      "metadata": {
        "id": "zsXhIfDs4kO5"
      },
      "id": "zsXhIfDs4kO5",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ”Š 4. Transcribe Arabic Audio using Whisper\n",
        "\n"
      ],
      "metadata": {
        "id": "e6PjCMXNBaHf"
      },
      "id": "e6PjCMXNBaHf"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31fa5413",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31fa5413",
        "outputId": "70f13816-5f5b-4ade-87a9-570c777cc2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numba/__init__.py:48: UserWarning: A NumPy version >=1.25.2 and <2.6.0 is required for this version of SciPy (detected version 1.24.4)\n",
            "  import scipy\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.88G/2.88G [01:06<00:00, 46.2MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:10.620]  ÙÙŠ Ù‚Ù„Ø¨ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ø­ÙŠØ« ØªØªØ¬Ø³Ø¯ Ø§Ù„Ø­Ø¶Ø§Ø±Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙÙŠ ÙƒÙ„ Ø²Ø§ÙˆÙŠØ©\n",
            "[00:10.620 --> 00:16.380]  ÙŠØ±ÙˆÙŠ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø­ÙƒØ§ÙŠØ© Ø£Ø²Ù„ÙŠØ© Ø¹Ù† Ø§Ù„Ø¬Ù…Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¨Ø¯Ø§Ø¹\n",
            "[00:16.380 --> 00:19.300]  Ø¥Ù†Ù‡ Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø¬Ø±Ø¯ Ø®Ø·\n",
            "[00:19.300 --> 00:23.180]  Ø¥Ù†Ù‡ ÙÙ†ØŒ Ø¥Ù†Ù‡ Ù‡ÙˆÙŠØ© Ø¹Ø±Ø¨ÙŠØ©\n",
            "[00:23.180 --> 00:26.760]  Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…\n",
            "[00:26.760 --> 00:29.580]  ÙŠØ¹Ù†ÙŠ Ù‡Ùˆ Ø¨Ù‚Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¨\n",
            "[00:29.580 --> 00:32.180]  ÙˆÙ„ÙƒÙ† ØªØ·ÙˆØ±Ù‡ Ø§Ø®ØªÙ„Ù Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…\n",
            "[00:32.180 --> 00:35.020]  ÙØ§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…ÙˆØ¬ÙˆØ¯ ÙˆÙ„ÙƒÙ† Ù„Ø£Ù†Ù‡ Ù‡Ùˆ\n",
            "[00:35.020 --> 00:40.380]  ÙŠØ¹Ù†ÙŠ Ø¨Ø¨Ø³Ø§Ø·Ø© Ø´Ø¯ÙŠØ¯Ø© Ù‡Ùˆ Ù…Ù† Ø¹ÙŠÙ„Ø© Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ù†Ø¨Ø§ØªÙŠØ©\n",
            "[00:40.380 --> 00:44.180]  ÙˆÙ„ÙƒÙ† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù„ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù†Ø¯Ù†Ø§ ÙØ¹Ù„Ø§ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ø´ÙŠØ¡\n",
            "[00:44.180 --> 00:46.560]  ÙˆÙ…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø´ÙŠØ¡ ØªØ§Ù†ÙŠ Ø®Ø§Ù„Øµ\n",
            "[00:46.560 --> 00:50.060]  ÙÙÙŠ Ø­Ø§Ø¬Ø© Ø­ØµÙ„Øª Ù…Ø¹ Ù†Ø²ÙˆÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø©\n",
            "[00:50.060 --> 00:56.300]  ÙˆØªØªÙ†ÙˆØ¹ Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±\n",
            "[00:56.300 --> 01:00.240]  Ø­ÙŠØ« ÙŠØ±ÙˆÙŠ ÙƒÙ„ Ø®Ø· Ø¹Ù† Ø¹ØµØ± Ø¥ØµØ¯Ø§Ø±Ù‡ ÙÙŠÙ‡\n",
            "[01:00.240 --> 01:02.480]  ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª ØªÙ…ÙŠØ² Ø¨Ù‡Ø§\n",
            "[01:02.480 --> 01:07.320]  Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø·Ø¨Ø¹Ø§ Ù‡Ù… ÙƒØ°Ø§ Ù†ÙˆØ¹ ÙƒØªÙŠØ±\n",
            "[01:07.320 --> 01:10.040]  Ù…Ù‡Ù…Ø§Ø´ Ø¨Ø³ Ù†Ø³Ø® ÙˆØ§Ù„Ø±Ù‚Ø¹ Ø²ÙŠ Ù…Ø§ Ø¨ÙŠØ¹Ù„Ù…ÙˆÙ†Ø§ ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\n",
            "[01:10.040 --> 01:12.380]  ÙˆØ­ØªÙ‰ Ø§Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ø±Ù‚Ø¹ Ø¨ØªÙ‚ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø´ÙŠØ¡\n",
            "[01:12.380 --> 01:18.820]  ÙˆØ®Ø· Ø§Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ø±Ù‚Ø§Ø¹ ÙÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø´ÙŠØ¡ ØªØ§Ù†ÙŠ\n",
            "[01:18.820 --> 01:25.680]  Ù‡Ù… Ø§Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ø±ÙŠØ­Ø§Ù† ÙˆØ§Ù„Ù…Ø­Ù‚Ù‚ ÙˆØ§Ù„Ø«Ù„Ø« ÙˆØ§Ù„Ø±Ù‚Ø§Ø¹ ÙˆØ§Ù„ØªÙˆÙ‚ÙŠØ¹\n",
            "[01:26.300 --> 01:30.420]  Ø¯ÙˆÙ„ Ø§Ù„Ø³ØªØ© Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù„ÙŠ Ù…Ù†Ù‡Ù… Ø·Ù„Ø¹ Ø­Ø§Ø¬Ø§Øª ÙƒØªÙŠØ± Ø¬Ø¯Ø§\n",
            "[01:30.420 --> 01:36.400]  ÙˆÙ„Ø£Ù† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„ÙŠØ³ Ù…Ø¬Ø±Ø¯ Ø­Ø±ÙˆÙ\n",
            "[01:36.400 --> 01:38.880]  Ø¨Ù„ Ù‡Ùˆ ÙÙ† Ø¨Ø­Ø¯ Ø°Ø§ØªÙ‡\n",
            "[01:38.880 --> 01:43.260]  ÙØ¥Ù†Ù‡ Ù…Ø­ÙƒÙˆÙ… Ø¨Ù‚ÙˆØ§Ø¹Ø¯ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ØªÙ†Ø§ØºÙ…Ø©\n",
            "[01:43.260 --> 01:47.640]  ØªØ¬Ø¹Ù„ Ù…Ù† ØªÙ„Ùƒ Ø§Ù„Ø®Ø·ÙˆØ· Ù„ÙˆØ­Ø© ÙÙ†ÙŠØ© Ø¨Ø°Ø§ØªÙ‡Ø§\n",
            "[01:47.640 --> 01:50.620]  Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ÙŠ Ø¨ØªØ­ÙƒÙ… Ø£ÙŠ Ø®Ø· Ù…Ù† Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
            "[01:50.620 --> 01:52.700]  ÙÙŠÙ‡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ØªÙ†Ø·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡Ù… ÙƒÙ„Ù‡Ù…\n",
            "[01:52.700 --> 01:55.940]  ÙˆØ£Ù„ÙˆÙ‡ Ù‡ÙŠ Ø£Ù†Ù‡ Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ø®Ø· Ù…Ù†Ø³ÙˆØ¨\n",
            "[01:56.300 --> 01:58.080]  Ù…Ù†Ø³ÙˆØ¨ Ø¨Ù…Ø¹Ù†Ù‰ proportion\n",
            "[01:58.080 --> 02:04.680]  ÙÙ‡Ùˆ Ø§Ù„Ù…Ù‡Ù… Ø£Ù†Ù‡ Ø§Ù„Ø­Ø±Ù Ù„Ù„Ø­Ø±Ù Ø¨ÙŠØ¨Ù‚Ù‰ ÙÙŠÙ‡ Ù†Ø³Ø¨Ø© Ù…Ø¹ÙŠÙ†Ø© Ù„Ø§Ø²Ù… ØªØªØ¨Ø¹\n",
            "[02:04.680 --> 02:06.680]  ÙˆØ§Ù„ÙƒÙ„Ù…Ø© Ù„Ù„ÙƒÙ„Ù…Ø© ÙˆØ§Ù„Ø³Ø·Ø± Ù„Ù„Ø³Ø·Ø±\n",
            "[02:06.680 --> 02:12.480]  Ù„Ø£Ù†Ù‡ Ø¨Ø¹Ø¯ Ø£Ø¨Ø­Ø§Ø« ÙƒØªÙŠØ± Ù‚ÙˆÙŠ Ù„Ù‚ÙˆØ§ Ø£Ù†Ù‡ ÙÙŠÙ‡ golden ratio Ù…Ø«Ù„Ø§\n",
            "[02:12.480 --> 02:15.520]  golden ratio Ø£Ùˆ Ø§Ù„Ù„ÙŠ Ù‡ÙŠ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©\n",
            "[02:15.520 --> 02:19.240]  Ø¯ÙŠ Ø­Ø§Ø¬Ø© ancient Ø¯ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ù† Ø£ÙŠØ§Ù… Ø§Ù„Ø¨Ù†ÙŠ Ø¢Ø¯Ù… Ù†Ø²Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¶\n",
            "[02:19.240 --> 02:22.520]  Ù‡ÙŠ Ø§Ù„ ratio Ø§Ù„Ù„ÙŠ Ø±Ø¨Ù†Ø§ Ø®Ù„Ù‚ Ø¨Ù‡Ø§ Ø§Ù„ÙƒÙˆÙ†\n",
            "[02:22.520 --> 02:25.460]  ÙÙ„Ù…Ø§ Ø¨ØªØ·Ø¨Ù‚ÙŠÙ‡Ø§ ÙÙŠ Ø§Ù„ÙÙ†ÙˆÙ† Ø¹Ù…ÙˆÙ…Ø§Ù‹\n",
            "[02:25.460 --> 02:26.800]  Ø¹ÙŠÙ† Ø§Ù„Ø¨Ù†ÙŠ Ø¢Ø¯Ù… Ø¨ØªØ³ØªØ±ÙŠØ­\n",
            "[02:26.800 --> 02:29.560]  ÙØ§Ù„Ø®Ø·ÙˆØ· Ø¯ÙŠ ÙƒÙ„Ù‡Ø§ Ù„Ù…Ø§ Ø¨ÙŠØ·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡Ø§\n",
            "[02:29.560 --> 02:32.240]  a ratio Ø£Ùˆ a proportion Ù…Ù†Ø³ÙˆØ¨\n",
            "[02:32.240 --> 02:35.340]  Ø¨ØªÙ„Ø§Ù‚ÙŠÙ‡Ø§ Ù…Ø³ØªØµØ§ØºØ©\n",
            "[02:35.340 --> 02:47.940]  ÙˆØ¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…ØªÙ†Ø§Ù‡ÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø§Ù„ØªÙŠ ÙŠØªØ·Ù„Ø¨Ù‡Ø§ Ù†Ø­Øª Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[02:47.940 --> 02:51.260]  Ù„Ù…Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„ÙŠÙ‡ Ù…Ù† ØªÙØ§ØµÙŠÙ„ Ø¯Ù‚ÙŠÙ‚Ø©\n",
            "[02:51.260 --> 02:55.260]  ÙØ¥Ù† Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ø´Ø±Ø§Ø¡ Ø§Ù„Ù„ÙˆØ­Ø§Øª Ø§Ù„Ø±Ø®Ù…ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙÙ†\n",
            "[02:55.460 --> 02:57.760]  Ø´Ù‡Ø¯ ØªØ±Ø§Ø¬Ø¹Ø§Ù‹ ÙƒØ¨ÙŠØ±Ø§Ù‹\n",
            "[02:57.760 --> 03:02.260]  ÙˆØ£ØµØ¨Ø­ Ù…Ù‚ØªØµØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø´ÙˆØ§Ù‡Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø¨Ø±\n",
            "[03:02.260 --> 03:04.560]  Ø§Ù„Ù†Ø­Øª Ø¨ÙŠØ­ØªØ§Ø¬ ÙˆÙ‚Øª Ø·ÙˆÙŠÙ„\n",
            "[03:04.560 --> 03:06.440]  Ø¹Ø§ÙŠØ² ØµØ¨Ø±Ù‡ ÙƒÙ„ Ø§Ù„Ø¯Ø§Ù†ÙŠ\n",
            "[03:06.440 --> 03:09.840]  ÙˆØ¨Ø¹Ø¯ÙŠÙ† Ø¨Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø© Ø¨ÙŠØ¨Ù‚Ù‰ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø³Ø±ÙŠØ¹ Ø´ÙˆÙŠØ©\n",
            "[03:09.840 --> 03:11.440]  Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù†Ø­Øª Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø®Ø§Ù…\n",
            "[03:11.440 --> 03:13.440]  Ø¨ØªØ¨ØªØ¯ÙŠ Ø§Ù„Ø£ÙˆÙ„ Ø¨Ø§Ù„Ø®Ø·Ø§Øª\n",
            "[03:13.440 --> 03:17.040]  ÙŠØ­Ø¯Ø¯ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆÙŠÙƒØªØ¨ Ø¨Ø§Ù„Ù‚Ù„Ù… Ø§Ù„Ø¨Ø³\n",
            "[03:17.040 --> 03:19.040]  Ø®Ø· Ø¨Ø§Ù„Ø­Ø¨Ø± Ø§Ù„Ø´ÙŠÙ†ÙŠ Ø¹Ø§Ø¯ÙŠ Ø®Ø§Ù„Øµ\n",
            "[03:19.040 --> 03:21.060]  ÙˆØ¨Ø¹Ø¯ÙŠÙ† Ø£Ù†Ø§ Ø¨ØªØ¨ØªØ¯ÙŠ Ø¨Ø§Ù„Ø£Ø²Ù…ÙŠÙ† ÙˆØ§Ù„Ù…Ø·Ø±Ø§Ø¡\n",
            "[03:21.060 --> 03:23.460]  Ø£Ù†Ø§ Ø­ØªÙ„Ù‚ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ù‡Ùˆ ÙƒØªØ¨Ù‡ Ø¨Ø§Ù„Ø®Ø·\n",
            "[03:23.460 --> 03:25.460]  ÙˆØ¨Ø¹Ø¯ÙŠÙ† Ø¨ØªØ¯ÙŠÙ‡Ù… Ø¨Ø¹Ø¯ ÙƒØ¯Ù‡\n",
            "[03:25.460 --> 03:28.460]  Ø¨Ø§Ù„Ù„ÙˆÙ† Ù…Ø«Ù„Ø§Ù‹ Ø§Ù„Ù„ÙŠ Ø¥Ø­Ù†Ø§ Ø¨Ù†ÙØ¶Ù„Ù‡\n",
            "[03:28.460 --> 03:32.200]  ÙˆØ¨Ø¹Ø¯ÙŠÙ† Ø¨ØªØªÙ†Ø¶Ù ÙˆÙŠØ¨Ù‚Ù‰ Ø§Ù„Ø·Ø¨ÙŠØ¹Ø© ØªØ±Ø¬Ø¹ Ø²ÙŠ Ø®Ø· Ø§Ù„Ø®Ø·Ø§Øª ØªØ§Ù†ÙŠ\n",
            "[03:32.200 --> 03:34.200]  Ø¨Ø³ Ù…Ø§ Ø¯ÙˆÙ† Ø£Ø¨ÙˆÙŠØ§ ÙˆÙ…Ø§ Ù†Ø­ÙˆØ·\n",
            "[03:34.200 --> 03:37.200]  ÙÙŠÙ‡ ØªØ±Ø§Ø¬Ø¹ ÙÙŠ Ø§Ù„Ù„ÙˆØ­Ø§Øª Ø§Ù„Ø±Ø®Ø§Ù…\n",
            "[03:37.200 --> 03:40.200]  Ù„Ø£Ù† Ø§Ù„Ù†Ø¸Ø± Ø£Ù† Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¯Ù„ÙˆÙ‚ØªÙŠ Ù…Ø§ Ø¨Ù‚Ø§Ø´ ÙŠØ¹Ù†ÙŠ\n",
            "[03:40.200 --> 03:43.440]  Ø§Ù„Ù„Ø®Ø§Ù… ØºØ§Ù„ÙŠ ÙˆØ§Ù„Ù†Ø­Øª ØºØ§Ù„ÙŠ ÙˆØ§Ù„Ø³Ø¹ÙˆØ¬Ø± Ø§Ù„Ø³Ù…Ø§Ø¹ÙŠ ØºØ§Ù„ÙŠ\n",
            "[03:43.440 --> 03:45.240]  ÙƒÙ„ Ø´ÙŠØ¡ Ø¨ÙŠØºÙ„Ø· Ø£Ù…Ø§Ù…Ù†Ø§\n",
            "[03:45.240 --> 03:46.740]  ÙØ§Ù„Ù†Ø§Ø³ Ø®Ù„Ø§Øµ ÙÙŠÙ‡ ØªØ±Ø§Ø¬Ø¹\n",
            "[03:46.740 --> 03:48.240]  ÙˆØ¨Ø¹Ø¯ÙŠÙ† ÙÙŠÙ† Ø§Ù„Ù„Ø§ÙØªØ§Øª Ø¨ØªØ²Ø¨Ø·\n",
            "[03:48.240 --> 03:51.880]  ÙƒÙ„Ù‡Ù… Ø¨ÙŠØ¹Ù…Ù„ÙˆØ§ Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø¥Ù†Ø¬Ù„ÙŠØ²Ø© Ø¹Ù„Ù‰ Ù„ÙˆØ­Ø§Øª Ø§Ù„Ù…Ø­Ù„Ø§Øª ÙˆØ¥Ø¶Ø§Ø¡Ø© ÙˆØ£Ù†ÙˆØ§Ø±\n",
            "[03:51.880 --> 03:54.880]  Ø¥Ù†Ù…Ø§ Ù…Ù† Ø§Ù„Ø²Ù…Ø§Ù† ÙƒÙ†Ø§ Ù†Ø¹Ù…Ù„ Ù…Ù† Ù„ÙˆØ­Ø§Øª Ø§Ù„Ù…Ø­Ù„Ø§Øª\n",
            "[03:54.880 --> 03:58.880]  ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù…Ø­Ù„Ø§Øª ÙƒÙ†Ø§ Ø¨Ù†Ø¹Ù…Ù„Ù‡Ø§ Ø¨Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆÙ†ÙƒØªØ¨Ù‡Ø§ Ù„Ø¥Ø³Ù… Ø§Ù„Ù…Ø­Ù„\n",
            "[03:58.880 --> 04:00.880]  Ø§Ù„Ø¢Ù† ÙÙŠØ´ ÙƒÙ„Ø§Ù… Ø¯Ù‡\n",
            "[04:00.880 --> 04:02.880]  Ù…Ø§ ØªØ·ÙˆØ± Ø§Ù„Ø²Ù…Ù† Ø¨Ù‚Ù‰\n",
            "[04:08.880 --> 04:10.880]  ÙˆØ¨Ø§Ù„Ø¯ÙˆØ§ÙŠØ© ÙˆØ§Ù„Ù‚Ù„Ù…\n",
            "[04:10.880 --> 04:13.880]  Ù…Ø§ Ø²Ø§Ù„ Ø·Ù„Ø§Ø¨ Ù…Ø¯Ø±Ø³Ø© Ø®Ù„ÙŠÙ„ Ø£ØºØ§ÙŠØ©\n",
            "[04:13.880 --> 04:15.880]  ØªØ¹Ù„Ù…ÙˆÙ† ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¹Ù„Ù‰ Ø£Ø³Ù†Ùˆ\n",
            "[04:15.880 --> 04:18.880]  ÙÙ‡ÙŠ Ø£ÙˆÙ„ Ù…Ø¯Ø±Ø³Ø© Ø£Ù†Ø´Ø¦Øª Ù„ØªØ¹Ù„ÙŠÙ… Ù‡Ø°Ø§ Ø§Ù„ÙÙ†\n",
            "[04:18.880 --> 04:20.880]  ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·\n",
            "[04:20.880 --> 04:21.880]  ÙÙŠ Ø¹Ù‡Ø¯Ù‡\n",
            "[04:21.880 --> 04:23.880]  ÙˆØ±ØºÙ… ØªØ¯Ù‡ÙˆØ± Ø£ÙˆØ¯Ø§Ø¹ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\n",
            "[04:23.880 --> 04:26.880]  Ø¥Ù„Ø§ Ø¥Ù†Ù‡Ø§ ØªØ´Ù‡Ø¯ Ø¥Ù‚Ø¨Ø§Ù„ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø±Ø§ØºØ¨ÙŠÙ† Ù„ØªØ¹Ù„Ù… Ù‡Ø°Ø§ Ø§Ù„ÙÙ† Ø§Ù„Ø¹Ø±ÙŠÙ‚\n",
            "[04:26.880 --> 04:28.880]  Ù†Ø¹Ù„Ù… Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[04:28.880 --> 04:30.880]  Ø§Ù„Ø®Ù…Ø³Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
            "[04:30.880 --> 04:34.880]  Ø§Ù„Ù„ÙŠ Ù‡Ùˆ Ø®Ø· Ø§Ù„Ø³Ù„Ø³ ÙˆØ§Ù„Ù†Ø³Ø® ÙˆØ§Ù„ÙØ±Ø³ÙŠ ÙˆØ§Ù„Ø±ÙƒØ¹Ø© ÙˆØ§Ù„Ø¬ÙˆØ§Ù†Ø¨\n",
            "[04:34.880 --> 04:37.880]  Ø«Ù… Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ ÙÙŠÙ‡ Ø§Ù„ØªØ®ØµØµ\n",
            "[04:37.880 --> 04:39.880]  Ø¯Ø¨Ù„ÙˆÙ… ØªØ®ØµØµ ÙˆØªØ²Ù‡ÙŠØ¨\n",
            "[04:39.880 --> 04:41.880]  ÙŠÙˆØ¬Ø¯ Ø¥Ø¬Ù…Ø§Ù„ ÙƒØ¨ÙŠØ±\n",
            "[04:41.880 --> 04:43.880]  ÙŠÙˆØ¬Ø¯ Ø®Ø· Ø£Ø³Ø§Ø³ÙŠ\n",
            "[04:43.880 --> 04:45.880]  ÙŠÙˆØ¬Ø¯ Ø®Ø· Ø£Ø³Ø§Ø³ÙŠ\n",
            "[04:45.880 --> 04:47.880]  ÙŠÙˆØ¬Ø¯ Ø®Ø· Ø£Ø³Ø§Ø³ÙŠ\n",
            "[04:47.880 --> 04:49.880]  ÙŠÙˆØ¬Ø¯ Ø®Ø· Ø£Ø³Ø§Ø³ÙŠ\n",
            "[04:49.880 --> 04:50.880]  ÙŠÙˆØ¬Ø¯ Ø®Ø· Ø£Ø³Ø§Ø³ÙŠ\n",
            "[04:50.880 --> 04:55.880]  ÙŠÙˆØ¬Ø¯ Ø¥Ø¬Ù…Ø§Ù„ ÙƒØ¨ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ø·Ù„Ø§Ø¨ Ù„Ù„Ø­Ø¶ÙˆØ± Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ù‡Ù†Ø§\n",
            "[04:55.880 --> 04:59.880]  ÙˆØ®Ø§ØµØ© Ø§Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ù…Ø¨Ø¹ÙˆØ«ÙŠÙ† Ø¥Ù„Ù‰ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø£Ø²Ø± Ø§Ù„Ø´Ø±ÙŠÙ\n",
            "[04:59.880 --> 05:04.880]  Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø£Ù† ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ ÙƒÙ…Ø§ Ø¹Ù…Ù„ ÙÙŠ Ù„ÙŠØ¨ÙŠØ§\n",
            "[05:04.880 --> 05:12.880]  Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø®ØªØµØ© Ù„Ù„Ø°ÙŠÙ† ÙŠØ±ÙŠØ¯ÙˆÙ† Ø£Ù† ÙŠØªØ¹Ù„Ù…ÙˆØ§ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆÙŠØ¹Ù…Ù„ÙˆØ§ Ø¨Ù‡\n",
            "[05:12.880 --> 05:16.880]  Ø£Ù…Ø§ ØºØ§Ù„Ø¨ÙŠØ© Ø§Ù„Ø¹Ø¸Ù…Ù‰ Ù„Ù„Ø­Ø¶ÙˆØ± Ù‡Ù†Ø§\n",
            "[05:16.880 --> 05:19.880]  ÙÙ‡Ù… Ù‡Ø§ÙˆÙˆÙ† ÙˆÙ„ÙŠØ³ÙˆØ§ Ù…Ø­ØªØ±Ù‚ÙˆÙ†\n",
            "[05:19.880 --> 05:25.880]  Ø³Ù†Ø±ÙŠØ¯ Ø£Ù† ÙŠÙƒÙˆÙ† ÙƒØ³Ù… Ù„Ù„Ù…Ø­ØªØ±ÙÙŠÙ† Ø§Ù„Ø°ÙŠÙ† Ø³ÙŠØ³ØªØºÙ„ÙˆÙ† Ø¨Ù‡Ø°Ø§ Ø§Ù„ÙÙ† ÙˆÙƒØ³Ù… Ø£Ø®Ø± Ù„Ù„Ù‡ÙˆØ§Ø¡\n",
            "[05:25.880 --> 05:33.880]  ÙˆØ±ØºÙ… ØªØ¯Ù†ÙŠ Ø£Ø¬ÙˆØ± Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² Ø®Ù…Ø³Ø© Ø¬Ù†ÙŠÙ‡Ø§Øª ÙÙŠ Ø§Ù„Ø­ØµØ©\n",
            "[05:33.880 --> 05:35.880]  ÙÙŠ Ø£Ù†Ù‡Ù… Ù…Ø§ Ø²Ø§Ù„ÙˆØ§ Ù…ØªÙ…Ø³ÙƒÙŠÙ† Ø¨ØªØ¯Ø±ÙŠØ³ ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[05:35.880 --> 05:41.880]  ÙˆØ°Ù„Ùƒ Ù„Ø¥ÙŠÙ…Ø§Ù†Ù‡Ù… Ø¨Ø£Ù‡Ù…ÙŠØ© ØªØ¹Ù„Ù…Ù‡ ÙƒÙˆØ³ÙŠÙ„Ø© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
            "[05:43.880 --> 05:45.880]  ÙÙ‡ÙŠ ØªØ¯Ø±ÙŠØ³ Ø§Ù„ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[05:45.880 --> 05:47.880]  ÙÙ‡ÙŠ ØªØ¯Ø±ÙŠØ³ Ø§Ù„ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[05:47.880 --> 05:55.200]  ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¨ÙŠØª Ø§Ù„Ø¹ØªÙŠÙ‚ ÙŠØ¨Ø¯Ø£ Ù…Ø­Ù…Ø¯ Ø´ÙØ¹ÙŠ ÙÙŠ Ù†Ø­Øª Ù„ÙˆØ­Ø§Øª Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[05:55.200 --> 05:59.600]  Ù…Ø³ØªÙ„Ù‡Ù…Ø§ Ù…Ù† Ø¥Ø±Ø« Ø¹Ø§Ø¦Ù„Ø© Ø§Ù„Ø¹Ø±ÙŠÙ‚ Ø£ØµÙˆÙ„ ÙÙ†ÙˆÙ†Ù‡Ø§\n",
            "[05:59.600 --> 06:06.020]  ØªØ®Ø±Ø¬ Ù…Ù† Ù…Ø¯Ø±Ø³Ø© Ø®Ù„ÙŠÙ„ Ø£ØºÙ‰ ÙˆÙ†Ø´Ø£ ÙÙŠ Ø¨ÙŠØ¦Ø© ØªØ¹Ø´Ù‚ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[06:06.020 --> 06:12.200]  Ù…Ù†Ø²Ù„ ØªØ¹Ù‚Ø¨Øª Ø¹Ù„ÙŠÙ‡ Ø®Ù…Ø³Ø© Ø£Ø¬ÙŠØ§Ù„ Ù…Ù† Ø£Ø´Ù‡Ø± Ø®Ø·Ø·ÙŠÙ† Ø§Ù„Ù…ØµØ±ÙŠÙŠÙ†\n",
            "[06:12.200 --> 06:17.180]  Ø¨ØµÙ…Ø§ØªÙ‡Ù… ÙˆØ§Ø¶Ø­Ø© ÙˆØ´Ø§Ù‡Ø¯Ø© Ø¹Ù„ÙŠÙ‡Ù… ÙÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[06:17.180 --> 06:24.020]  ÙˆÙ…Ø§ Ø²Ø§Ù„Øª Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙÙ† Ø¯ÙŠ Ø¥ÙŠÙ…Ø§Ù†Ù‡Ù… Ø¨Ø£Ù‡Ù…ÙŠØªÙ‡\n",
            "[06:24.020 --> 06:29.580]  Ø£Ù†Ø§ Ù…Ø­Ù…Ø¯ Ø´ÙØ¹ÙŠ Ø¨Ø§Ø­Ø« ÙˆÙÙŠÙ‡ ÙÙ†Ø§Ù† Ù…ØªØ®ØµØµ ÙÙŠ ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[06:29.580 --> 06:34.000]  Ù…Ù‡ØªÙ… Ø¨ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ ÙˆØ§Ù„ÙÙ†ÙˆÙ† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…\n",
            "[06:34.000 --> 06:39.360]  Ø¯Ù‡ Ø±Ø§Ø¬Ø¹ Ù„Ø³Ø¨Ø¨ Ù†Ø´Ø£ØªÙŠ ÙÙŠ Ø¹Ø§Ø¦Ù„Ø© Ø®Ø§Ù„Ø¯ Ø³ÙˆÙÙŠ Ø²Ø§Ø¯Ø©\n",
            "[06:39.360 --> 06:43.620]  Ù‡ÙŠ Ø¹Ø§Ø¦Ù„Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„ÙÙ†ÙˆÙ† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ\n",
            "[06:43.620 --> 06:46.260]  Ø¹Ù„Ù‰ Ù…Ø¯Ø§Ø± Ø£ÙƒØªØ± Ù…Ù† 200 Ø³Ù†Ø©\n",
            "[06:46.260 --> 06:47.000]  Ø¯Ù‡ Ø§Ù„ØªØ§Ø±ÙŠØ®\n",
            "[06:47.180 --> 06:50.380]  Ø§Ù„Ù…ÙˆØ«Ù‚ Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø­Ø¯ÙŠØ«\n",
            "[06:50.380 --> 06:53.580]  ÙˆØ£Ù†Ø§ Ø¨Ø£Ø¹ØªØ¨Ø± Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø§Ù…Ø³ Ù…Ù† Ø§Ù„Ø¹Ø§Ø¦Ù„Ø©\n",
            "[06:53.580 --> 07:00.420]  Ø§Ù„Ù„ÙŠ Ø¨Ø´ØªØºÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¢Ù† ÙˆØ¨Ù…Ø§Ø±Ø³ ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø´ÙƒÙ„ Ø¹Ù…Ù„ÙŠ\n",
            "[07:06.420 --> 07:10.180]  Ø®Ù„ÙŠÙ†ÙŠ Ø£Ù‚ÙˆÙ„Ùƒ Ø£Ù† ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø«Ù„ Ø¨Ø§Ù‚ÙŠ Ø§Ù„ÙÙ†ÙˆÙ†\n",
            "[07:10.180 --> 07:14.060]  ÙˆØ§Ù„Ù…Ø¸Ø§Ù‡Ø± Ø§Ù„Ø¹Ù…Ø§Ø±Ø© ÙˆØ§Ù„Ø­Ø¶Ø§Ø±Ø© ÙÙŠ Ø§Ù„Ø£Ø³ÙˆØ± Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
            "[07:14.060 --> 07:16.980]  Ø¨ÙŠØ¬ÙŠ Ø¹Ù„ÙŠÙ‡Ø§ Ø¹ØµÙˆØ± Ø§Ø²Ø¯Ù‡Ø§Ø± ÙˆØ¹ØµÙˆØ±\n",
            "[07:17.180 --> 07:22.380]  Ø®ÙÙˆØ· ÙˆØ¹Ø¯Ù… Ø§Ø²Ø¯Ù‡Ø§Ø± ÙÙ‡Ù†Ù„Ø§Ù‚ÙŠ Ø£Ù†Ù‡ ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨ÙŠØ¬ÙŠ\n",
            "[07:22.380 --> 07:27.100]  Ø¹Ù„ÙŠÙ‡ Ø£Ùˆ Ø£ÙƒØªÙŠØ± ÙƒØ§Ù† ÙÙŠÙ‡ Ø§Ø²Ø¯Ù‡Ø§Ø± Ø­Ø§Ù„ÙŠØ§ ÙÙŠ Ù…ØµØ± Ø¨ÙŠØ´Ù‡Ø¯ Ø´ÙŠØ¡ Ù…Ù† Ù…Ù†\n",
            "[07:27.100 --> 07:33.800]  Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ø§Ø²Ø¯Ù‡Ø§Ø± ÙÙŠ ØªØ¹Ù„Ù… ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆØ³Ø§Ø· Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
            "[07:33.800 --> 07:40.100]  Ø§Ù„ÙÙ†ÙŠØ© ÙˆØºÙŠØ± Ø§Ù„ÙÙ†ÙŠØ© ÙŠØ¹Ù†ÙŠ ÙÙŠ Ø¨Ø¹Ø¶Ù‡Ù… Ø¨ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ù…Ø§Ù„ÙŠØ§Øª Ø¯Ø§Ø®Ù„\n",
            "[07:40.100 --> 07:43.060]  ÙÙ† Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆØ§Ù„Ø¨Ø¹Ø¶ Ø¨ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ±Ø§Ø« ÙˆØ§Ù„Ù‚Ø¯Ù… ÙˆØ¥Ø²Ø§ÙŠ Ù†Ø³ØªÙ‚ÙŠ Ø£Ø¹Ù…Ø§Ù„Ù†Ø§.\n",
            "[07:43.060 --> 07:44.020]  Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ø§Ù„ØªÙŠ ØªØ´ØªØºÙ„Ù‡Ø§ ÙÙŠ Ø¹Ø§Ù… Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ù…ØµØ± Ø£Ùˆ ÙÙŠ Ù…ØµØ± ÙˆÙ…Ù† Ø£Ø¬Ù„ Ù‡Ø°Ø§ Ù„Ø§ ÙŠØ³ØªØ·ÙŠØ¹ Ø£Ù† Ù†ØªØ­Ø¯Ø« Ø¹Ù†Ù‡Ø§.\n",
            "[07:44.020 --> 07:51.240]  ÙˆØ¥Ø²Ø§ÙŠ Ù†Ø³ØªÙ‚ÙŠ Ø£Ø¹Ù…Ø§Ù„ ØªÙ†ØªØ³Ø¨ Ø§Ù„Ø­Ø¯Ø§Ø³Ø© Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø§Ù„ØªÙŠ ØªØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ø£ØµØ§Ù„Ø©\n",
            "[07:51.240 --> 07:56.120]  ÙÙ‡Ù†Ù„Ø§Ù‚ÙŠ ÙÙŠÙ‡ Ø´ÙŠØ¡ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ùƒ Ø­Ø§Ù„ÙŠØ§Ù‹ ÙˆÙ„ÙƒÙ† ÙÙŠÙ‡ Ø£ÙˆØ³Ø§Ø· Ù…Ø®ØªÙ„ÙØ©\n",
            "[07:56.120 --> 07:58.460]  ÙˆÙ†ØªÙ…Ù†Ù‰ Ø·Ø¨Ø¹Ø§Ù‹ Ø£Ù† ÙŠØ¨Ù‚Ù‰ ÙÙŠÙ‡ Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ¨Ø±\n",
            "[07:58.460 --> 08:12.400]  ÙˆØ¥ÙŠÙ…Ø§Ù†Ø§Ù‹ Ø¨Ø£Ù‡Ù…ÙŠØ© Ø¯ÙˆØ±Ù‡ Ù‚Ø±Ø±Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø«Ù‚Ø§ÙØ© ØªÙ†Ø¸ÙŠÙ… Ù…Ù„ØªÙ‚Ù‰ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[08:12.400 --> 08:17.180]  Ù„ØªØ´Ø¬ÙŠØ¹ Ø§Ù„Ø´Ø¨Ø§Ø¨ Ø§Ù„Ù…ÙˆÙ‡Ø¨ÙŠÙ† Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ù„ÙˆØ­Ø§Øª Ø¨Ù‡Ø°Ø§ Ø§Ù„ÙÙ†\n",
            "[08:17.180 --> 08:24.940]  ÙˆÙ‚Ø¯ Ø´Ø§Ø±Ùƒ Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„ØªÙ‚Ù‰ Ø¨Ù„ÙˆØ­Ø§Øª Ø£Ø¨Ø±Ø²Øª ØªØ·ÙˆØ± Ø£Ø³Ø§Ù„ÙŠØ¨ ÙˆØ£Ø´ÙƒØ§Ù„ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[08:24.940 --> 08:29.560]  ÙƒÙ…Ø§ Ø´Ø§Ø±Ùƒ ÙÙŠ Ø§Ù„Ù…Ù„ØªÙ‚Ù‰ ÙÙ†Ø§Ù†ÙˆÙ† ØºÙŠØ± Ù†Ø§Ø·Ù‚ÙŠÙ† Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
            "[08:29.560 --> 08:33.220]  Ù…Ù…Ø§ ÙŠØ¹ÙƒØ³ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ø¨Ù‡Ø°Ø§ Ø§Ù„ÙÙ†\n",
            "[08:33.220 --> 08:39.080]  ÙŠØ£ØªÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø¬Ù‡ÙˆØ¯ Ø§Ù„ÙˆØ²Ø§Ø±Ø© ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "[08:39.080 --> 08:42.180]  Ø¶Ù…Ù† Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØµÙˆÙ† Ø§Ù„Ø¹Ø§Ø¬Ù„ Ù„Ù„ØªØ±Ø§Ø« Ø§Ù„Ø«Ù‚Ø§ÙÙŠ\n",
            "[08:42.400 --> 08:45.340]  ØºÙŠØ± Ø§Ù„Ù…Ø§Ø¯ÙŠ Ø¨Ø§Ù„ÙŠÙˆÙ†Ø³ÙƒÙˆ\n",
            "[08:45.340 --> 08:58.400]  Ø³ÙŠØ¸Ù„ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¬Ø²Ø¡Ø§Ù‹ Ø­ÙŠØ§Ù‹ Ù…Ù† Ù‡ÙˆÙŠØªÙ†Ø§ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
            "[08:58.920 --> 09:01.380]  Ù…Ø­ÙÙˆØ± Ø¹Ù„Ù‰ Ø¬Ø¯Ø±Ø§Ù† Ø§Ù„ØªØ§Ø±ÙŠØ®\n",
            "[09:01.380 --> 09:06.160]  ÙˆØ³ØªØ¸Ù„ Ù…Ø³Ø¦ÙˆÙ„ÙŠØ© Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„ÙŠÙ‡ Ø¢Ù…Ù†Ø© ÙÙŠ Ø£Ø¹Ù†Ø§Ù‚Ù†Ø§\n",
            "[09:06.160 --> 09:09.880]  ÙˆÙ„Ø¹Ù„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„Ù‚Ø§Ø¦Ù…ÙŠÙ† Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙÙ†\n",
            "[09:09.880 --> 09:11.880]  ÙˆØªØ¶Ù…ÙŠÙ†Ù‡ ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©\n",
            "[09:12.400 --> 09:18.860]  Ù‡Ùˆ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªÙ…Ø±Ø§Ø±Ù‡ ÙˆØªÙˆØ§Ø±Ø«Ù‡ Ø¹Ø¨Ø± Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©\n",
            "âœ… Saved Arabic transcript to: /content/drive/MyDrive/ArabicVideoSummariser/transcripts/Calligraphy_ar.txt\n",
            "âœ… Saved full Whisper output (AR) to: /content/drive/MyDrive/ArabicVideoSummariser/transcripts/Calligraphy.json\n",
            "[00:00.000 --> 00:02.000]  The Arab Line\n",
            "[00:04.000 --> 00:06.000]  In the heart of the historical Cairo,\n",
            "[00:06.000 --> 00:10.000]  where Islamic civilization is embodied in every angle,\n",
            "[00:10.000 --> 00:16.000]  the Arab Line tells an eternal story about beauty and creativity.\n",
            "[00:16.000 --> 00:19.000]  It is more than just a line.\n",
            "[00:19.000 --> 00:21.000]  It is art.\n",
            "[00:21.000 --> 00:23.000]  It is an Arab identity.\n",
            "[00:24.000 --> 00:27.000]  The Arab Line existed before Islam.\n",
            "[00:27.000 --> 00:30.000]  It was in the Arab era.\n",
            "[00:30.000 --> 00:32.000]  But it evolved and changed after Islam.\n",
            "[00:32.000 --> 00:34.000]  The Arab Line exists,\n",
            "[00:34.000 --> 00:40.000]  but it is simply a family of plant lines.\n",
            "[00:40.000 --> 00:47.000]  But the examples we have before and after Islam are different.\n",
            "[00:47.000 --> 00:50.000]  Something happened with the revelation of the message.\n",
            "[00:53.000 --> 00:56.000]  Arab lines are very diverse.\n",
            "[00:56.000 --> 00:57.000]  They are divided into three categories.\n",
            "[00:57.000 --> 01:00.000]  Each line tells about an era in which it was issued\n",
            "[01:00.000 --> 01:03.000]  and its unique uses.\n",
            "[01:04.000 --> 01:07.000]  There are many types of the Arab Line.\n",
            "[01:07.000 --> 01:09.000]  It is not just about copying and printing,\n",
            "[01:09.000 --> 01:10.000]  as they teach us in school.\n",
            "[01:10.000 --> 01:12.000]  Even copying and printing is a different thing.\n",
            "[01:12.000 --> 01:14.000]  And the line of copying and printing\n",
            "[01:14.000 --> 01:19.000]  in the history of the Arab Line is something else.\n",
            "[01:19.000 --> 01:22.000]  They are copying, printing,\n",
            "[01:22.000 --> 01:24.000]  verification, and printing,\n",
            "[01:24.000 --> 01:26.000]  printing, and signing.\n",
            "[01:26.000 --> 01:28.000]  These are the six main categories.\n",
            "[01:28.000 --> 01:30.000]  Many of them are very different.\n",
            "[01:33.000 --> 01:36.000]  The Arab Line is not just letters,\n",
            "[01:36.000 --> 01:39.000]  but it is an art in and of itself.\n",
            "[01:39.000 --> 01:43.000]  It is ruled by precise and harmonious lines.\n",
            "[01:43.000 --> 01:47.000]  These lines make an artistic painting.\n",
            "[01:48.000 --> 01:50.000]  The lines that rule any line in the Arab Line\n",
            "[01:50.000 --> 01:52.000]  have a rule that applies to all of them.\n",
            "[01:52.000 --> 01:55.000]  And that rule is that it must be a line that is applied.\n",
            "[01:56.000 --> 01:58.000]  It is applied in the meaning of proportion.\n",
            "[01:58.000 --> 02:02.000]  The important thing is that the letter to the letter\n",
            "[02:02.000 --> 02:04.000]  has a certain ratio that must be followed,\n",
            "[02:04.000 --> 02:07.000]  and the word to the word, and the line to the line.\n",
            "[02:07.000 --> 02:10.000]  After a lot of research,\n",
            "[02:10.000 --> 02:12.000]  they found that there is a golden ratio.\n",
            "[02:12.000 --> 02:16.000]  The golden ratio is something ancient.\n",
            "[02:16.000 --> 02:18.000]  It existed since the time of the human beings\n",
            "[02:18.000 --> 02:19.000]  when they descended on earth.\n",
            "[02:19.000 --> 02:22.000]  It is the ratio that God created the universe with.\n",
            "[02:22.000 --> 02:25.000]  When it is applied in art in general,\n",
            "[02:25.000 --> 02:27.000]  the eyes of the human being relax.\n",
            "[02:27.000 --> 02:29.000]  When all these lines are applied,\n",
            "[02:29.000 --> 02:32.000]  a ratio or a proportion is applied,\n",
            "[02:32.000 --> 02:35.000]  you find it consistent.\n",
            "[02:42.000 --> 02:45.000]  Despite the endless and complex accuracy\n",
            "[02:45.000 --> 02:48.000]  required by the Arab Line,\n",
            "[02:48.000 --> 02:51.000]  what it contains of precise details,\n",
            "[02:51.000 --> 02:54.000]  is that the demand to buy the digital paintings\n",
            "[02:54.000 --> 02:58.000]  for this art has witnessed a great decline.\n",
            "[02:58.000 --> 03:02.000]  And it has become limited to the details of the graves.\n",
            "[03:03.000 --> 03:05.000]  The sculpting takes a long time.\n",
            "[03:05.000 --> 03:07.000]  It takes patience and patience.\n",
            "[03:07.000 --> 03:08.000]  And then, with practice,\n",
            "[03:08.000 --> 03:10.000]  the human being becomes a little faster.\n",
            "[03:10.000 --> 03:12.000]  The steps of sculpting on the marble\n",
            "[03:12.000 --> 03:14.000]  start with the lines,\n",
            "[03:14.000 --> 03:16.000]  and define the distances,\n",
            "[03:16.000 --> 03:17.000]  and write with the pencil,\n",
            "[03:17.000 --> 03:19.000]  and the line with the pencil is normal.\n",
            "[03:19.000 --> 03:21.000]  And then I start with the lines and the lines,\n",
            "[03:21.000 --> 03:23.000]  and I will find what he wrote with the line.\n",
            "[03:24.000 --> 03:26.000]  And then I give them after that,\n",
            "[03:26.000 --> 03:28.000]  in the color that we prefer.\n",
            "[03:28.000 --> 03:30.000]  And then I clean it up,\n",
            "[03:30.000 --> 03:32.000]  and it will be normal to return to the same lines,\n",
            "[03:32.000 --> 03:40.560]  but without a\n",
            "[03:40.560 --> 03:42.000]  There is a decline in the paintings of the graves,\n",
            "[03:42.000 --> 03:43.000]  because the work is not expensive now.\n",
            "[03:43.000 --> 03:44.000]  The marble is expensive,\n",
            "[03:44.000 --> 03:45.000]  and the sky is expensive.\n",
            "[03:45.000 --> 03:46.000]  Everything is different.\n",
            "[03:46.000 --> 03:47.000]  People are in a decline.\n",
            "[03:47.000 --> 03:48.000]  And then where are the details?\n",
            "[03:48.000 --> 03:50.000]  They all make them now English\n",
            "[03:50.000 --> 03:51.000]  on the paintings of the shops,\n",
            "[03:51.000 --> 03:52.000]  and lighting, and lights.\n",
            "[03:52.000 --> 03:53.000]  When we used to start with the paintings of the shops,\n",
            "[03:53.000 --> 03:55.000]  we used to make them with the Arabic line,\n",
            "[03:55.000 --> 03:57.000]  and write down the name of the shop.\n",
            "[03:57.000 --> 03:59.000]  Now there is no such thing.\n",
            "[03:59.000 --> 04:01.000]  The time is evolving.\n",
            "[04:09.000 --> 04:11.000]  And with the tools and the pen,\n",
            "[04:11.000 --> 04:13.000]  students of Khalil Agha School\n",
            "[04:13.000 --> 04:15.000]  are still learning the art of the Arabic line\n",
            "[04:15.000 --> 04:16.000]  on their heads.\n",
            "[04:16.000 --> 04:17.000]  It is the first school\n",
            "[04:17.000 --> 04:19.000]  established to teach this art\n",
            "[04:19.000 --> 04:21.000]  in the Middle East region.\n",
            "[04:21.000 --> 04:23.000]  In the reign of King Fuad I,\n",
            "[04:23.000 --> 04:25.000]  the school was established in 1922\n",
            "[04:25.000 --> 04:27.000]  with the aim of preserving the Arabic identity.\n",
            "[04:27.000 --> 04:30.000]  Despite the decline of the school's facilities,\n",
            "[04:30.000 --> 04:32.000]  it still receives the acceptance of the students\n",
            "[04:32.000 --> 04:35.000]  who want to learn this ancient art.\n",
            "[04:35.000 --> 04:38.000]  We teach the students all kinds of the Arabic line,\n",
            "[04:38.000 --> 04:40.000]  the five main ones,\n",
            "[04:40.000 --> 04:42.000]  which are the line of the Trinity,\n",
            "[04:42.000 --> 04:43.000]  the Naseb,\n",
            "[04:43.000 --> 04:44.000]  the Persian,\n",
            "[04:44.000 --> 04:45.000]  the Ruk'ah,\n",
            "[04:45.000 --> 04:46.000]  and the Juhani.\n",
            "[04:46.000 --> 04:48.000]  Then there is the specialization,\n",
            "[04:48.000 --> 04:50.000]  the diploma of specialization and graduation.\n",
            "[04:50.000 --> 04:52.000]  There is a great majority of students\n",
            "[04:52.000 --> 04:55.000]  who want to attend the school here,\n",
            "[04:55.000 --> 04:57.000]  especially the students\n",
            "[04:57.000 --> 05:00.000]  who are sent to the Al-Azhar Al-Sharif University.\n",
            "[05:00.000 --> 05:03.000]  The challenges are that there must be,\n",
            "[05:03.000 --> 05:05.000]  as was done in Libya,\n",
            "[05:05.000 --> 05:07.000]  a school of the Arabic line\n",
            "[05:07.000 --> 05:11.000]  dedicated to those who want to learn the Arabic line\n",
            "[05:11.000 --> 05:13.000]  and work with it.\n",
            "[05:13.000 --> 05:16.000]  As for the great majority of those who come here,\n",
            "[05:16.000 --> 05:19.000]  they are nomads and not experts.\n",
            "[05:19.000 --> 05:21.000]  We want to be a part of the experts\n",
            "[05:21.000 --> 05:24.000]  who will work with this art\n",
            "[05:24.000 --> 05:26.000]  and another part of the hobby.\n",
            "[05:26.000 --> 05:29.000]  Despite the cost of teachers' salaries at the school,\n",
            "[05:29.000 --> 05:32.000]  which is not exceeding 5 pounds per class,\n",
            "[05:32.000 --> 05:36.000]  they are still attached to teaching the Arabic line art,\n",
            "[05:36.000 --> 05:39.000]  because they believe that learning is important\n",
            "[05:39.000 --> 05:42.000]  as a means of preserving the Arabic identity.\n",
            "[05:49.000 --> 05:51.000]  In this ancient house,\n",
            "[05:51.000 --> 05:55.000]  Mohamed Shafi'i is creating the Arabic line\n",
            "[05:55.000 --> 05:57.000]  with the help of his family.\n",
            "[05:57.000 --> 05:59.000]  He is inspired by the family's heritage\n",
            "[05:59.000 --> 06:01.000]  and the ancient principles of their art.\n",
            "[06:01.000 --> 06:03.000]  He graduated from Khalil Agha School\n",
            "[06:03.000 --> 06:06.000]  and was born in an environment that admires the Arabic line.\n",
            "[06:06.000 --> 06:09.000]  It is a house that has been preserved\n",
            "[06:09.000 --> 06:12.000]  by five generations of the most famous Egyptian writers.\n",
            "[06:12.000 --> 06:15.000]  Their drawings are clear and they are witnessed\n",
            "[06:15.000 --> 06:17.000]  in the history of the Arabic line.\n",
            "[06:17.000 --> 06:19.000]  This family still preserves this tradition.\n",
            "[06:19.000 --> 06:24.000]  This is their belief in the importance of this art.\n",
            "[06:24.000 --> 06:26.000]  I am Mohamed Shafi'i,\n",
            "[06:26.000 --> 06:29.000]  a researcher and an artist specializing in the Arabic line art.\n",
            "[06:29.000 --> 06:32.000]  I am interested in the Arabic line art in particular\n",
            "[06:32.000 --> 06:34.000]  and Islamic arts in general.\n",
            "[06:34.000 --> 06:39.000]  This is because I was born in the family of Khaled Soufi Zada.\n",
            "[06:39.000 --> 06:42.000]  It is a family specialized in Islamic arts\n",
            "[06:42.000 --> 06:44.000]  and the Arabic line art in particular\n",
            "[06:44.000 --> 06:46.000]  for more than 200 years.\n",
            "[06:46.000 --> 06:47.000]  This is the history of the family.\n",
            "[06:47.000 --> 06:50.000]  I am a member of the family's fifth generation\n",
            "[06:50.000 --> 06:53.000]  and I have been working and practicing the Arabic line art\n",
            "[06:53.000 --> 06:55.000]  in a practical way.\n",
            "[06:55.000 --> 06:58.000]  The Arabic line art is like other arts,\n",
            "[06:58.000 --> 07:01.000]  the architectural and cultural manifestations\n",
            "[07:01.000 --> 07:03.000]  in different eras.\n",
            "[07:03.000 --> 07:06.000]  It has the era of prosperity and the era of art.\n",
            "[07:06.000 --> 07:08.000]  The Arabic line art is a work of art\n",
            "[07:08.000 --> 07:10.000]  that is not only a work of art\n",
            "[07:10.000 --> 07:12.000]  but also a work of art that is a work of art.\n",
            "[07:12.000 --> 07:14.000]  It is a work of art that is a work of art\n",
            "[07:14.000 --> 07:16.000]  and a work of art that is a work of art.\n",
            "[07:17.000 --> 07:19.000]  It is a work of art that is a work of art\n",
            "[07:19.000 --> 07:21.000]  and it has the era of prosperity and the era of art.\n",
            "[07:21.000 --> 07:23.000]  We will find that the Arabic line art\n",
            "[07:23.000 --> 07:24.000]  has a lot of prosperity\n",
            "[07:24.000 --> 07:26.000]  in Egypt right now.\n",
            "[07:26.000 --> 07:30.000]  It bears the form of prosperity.\n",
            "[07:30.000 --> 07:32.000]  There is a learning of the Arabic line art\n",
            "[07:32.000 --> 07:34.000]  between the different media,\n",
            "[07:34.000 --> 07:37.000]  the artistic and the non-artistic.\n",
            "[07:37.000 --> 07:40.000]  Some people are looking for the beauties\n",
            "[07:40.000 --> 07:42.000]  within the Arabic line art art\n",
            "[07:42.000 --> 07:44.000]  and some are looking for the heritage,\n",
            "[07:44.000 --> 07:45.000]  the ancient,\n",
            "[07:45.000 --> 07:47.000]  and how to use it\n",
            "[07:47.000 --> 07:49.000]  to create works that are modernized\n",
            "[07:49.000 --> 07:50.000]  from the ancient works\n",
            "[07:50.000 --> 07:52.000]  that are distinguished by the originality.\n",
            "[07:52.000 --> 07:54.000]  We will find some of the prosperity\n",
            "[07:54.000 --> 07:56.000]  but there are different mediums\n",
            "[07:56.000 --> 07:57.000]  and we hope that there will be\n",
            "[07:57.000 --> 07:59.000]  more interest.\n",
            "[08:06.000 --> 08:08.000]  Believing in the importance of its role,\n",
            "[08:08.000 --> 08:11.000]  the Ministry of Culture decided to organize\n",
            "[08:11.000 --> 08:13.000]  the Arabic Line Art Collective\n",
            "[08:13.000 --> 08:15.000]  to encourage talented youth\n",
            "[08:15.000 --> 08:17.000]  to produce paintings with this art\n",
            "[08:17.000 --> 08:19.000]  and a large number of people\n",
            "[08:19.000 --> 08:21.000]  participated in this project\n",
            "[08:21.000 --> 08:23.000]  with paintings that showed the development\n",
            "[08:23.000 --> 08:25.000]  of Arabic line patterns and shapes.\n",
            "[08:25.000 --> 08:27.000]  As artists participating in the project,\n",
            "[08:27.000 --> 08:29.000]  non-Arabic speakers\n",
            "[08:29.000 --> 08:31.000]  reflect the global interest\n",
            "[08:31.000 --> 08:33.000]  in this art.\n",
            "[08:33.000 --> 08:36.000]  This comes to the side of the Ministry's efforts\n",
            "[08:36.000 --> 08:39.000]  in recording the Arabic line\n",
            "[08:39.000 --> 08:42.000]  within the urgent design of the non-material\n",
            "[08:42.000 --> 08:45.000]  cultural heritage in UNESCO.\n",
            "[08:47.000 --> 08:49.000]  The Arabian Line Art Collective\n",
            "[08:49.000 --> 08:51.000]  is a part of our Arab identity\n",
            "[08:51.000 --> 08:53.000]  and it is a part of our history.\n",
            "[08:53.000 --> 08:55.000]  Our responsibility is to preserve it\n",
            "[08:55.000 --> 08:57.000]  and to keep it safe in our hearts.\n",
            "[08:57.000 --> 08:59.000]  Perhaps the interest of those who are\n",
            "[08:59.000 --> 09:00.000]  interested in this art\n",
            "[09:00.000 --> 09:04.100]  and its\n",
            "[09:04.100 --> 09:06.000]  guarantee in educational methods\n",
            "[09:06.000 --> 09:09.000]  is the best way to guarantee its continuity\n",
            "[09:09.000 --> 09:11.000]  and heritage.\n",
            "[09:11.000 --> 09:13.000]  We hope that the Arab Line Art Collective\n",
            "[09:13.000 --> 09:15.000]  will continue to provide\n",
            "[09:15.000 --> 09:16.400]  a\n",
            "[09:16.400 --> 09:19.440]  lasting andã„ã„\n",
            "[09:19.440 --> 09:21.820]  inspiration forberry\n",
            "[09:21.820 --> 09:28.400]  to these generations.\n",
            "âœ… Saved English translation to: /content/drive/MyDrive/ArabicVideoSummariser/transcripts/Calligraphy_en.txt\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import json\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"large\", device=\"cuda\")\n",
        "\n",
        "# transcribe (Arabic)\n",
        "result = model.transcribe(video_path, language=\"ar\", task=\"transcribe\", verbose=True)\n",
        "\n",
        "with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result['text'])\n",
        "print(f\"âœ… Saved Arabic transcript to: {transcript_path}\")\n",
        "\n",
        "with open(transcript_path.replace(\".txt\", \"_with_timecodes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for segment in result[\"segments\"]:\n",
        "        start = segment[\"start\"]\n",
        "        end = segment[\"end\"]\n",
        "        text = segment[\"text\"]\n",
        "        f.write(f\"[{start:.2f} - {end:.2f}] {text}\\n\")\n",
        "\n",
        "# âœ… Save full result as JSON (NEW)\n",
        "with open(trascription_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… Saved full Whisper output (AR) to: {trascription_json_path}\")\n",
        "\n",
        "# Translate (Arabic â†’ English)\n",
        "result_en = model.transcribe(video_path, language=\"ar\", task=\"translate\", verbose=True)\n",
        "with open(translation_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result_en[\"text\"])\n",
        "print(f\"âœ… Saved English translation to: {translation_path}\")\n",
        "\n",
        "# Save timecoded translation\n",
        "with open(translation_path.replace(\".txt\", \"_with_timecodes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for segment in result_en[\"segments\"]:\n",
        "        start = segment[\"start\"]\n",
        "        end = segment[\"end\"]\n",
        "        text = segment[\"text\"]\n",
        "        f.write(f\"[{start:.2f} - {end:.2f}] {text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ–¼ï¸ 5. KeyFrame Detection & Captioning"
      ],
      "metadata": {
        "id": "o1HpAGrQQ7ro"
      },
      "id": "o1HpAGrQQ7ro"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import json\n",
        "from PIL import Image\n",
        "from scenedetect import open_video, SceneManager\n",
        "#from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "\n",
        "# ============ SETUP ============\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# BLIP-2 model\n",
        "caption_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\",use_fast=False)\n",
        "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Translation model (EN â†’ AR)\n",
        "translator_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "translator_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\").to(device)\n",
        "\n",
        "\n",
        "\n",
        "captions = {}\n",
        "\n",
        "scene_manager = SceneManager()\n",
        "scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
        "video = open_video(video_path)\n",
        "scene_manager.detect_scenes(video)\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "# --- Extract frames ---\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "for i, (start, _) in enumerate(scene_list):\n",
        "  frame_num = int(start.get_seconds() * fps)\n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    continue\n",
        "\n",
        "  frame_name = f\"scene_{i:03}.jpg\"\n",
        "  frame_path = os.path.join(keyframe_dir, frame_name)\n",
        "  cv2.imwrite(frame_path, frame)\n",
        "\n",
        "  # Convert to PIL\n",
        "  image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  # --- Captioning with BLIP-2 ---\n",
        "  inputs = caption_processor(images=image, return_tensors=\"pt\").to(device, torch.float16 if device == \"cuda\" else torch.float32)\n",
        "  generated_ids = caption_model.generate(**inputs, max_new_tokens=50)\n",
        "  english_caption = caption_processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Translate to Arabic ---\n",
        "  translation_inputs = translator_tokenizer(english_caption, return_tensors=\"pt\", padding=True).to(device)\n",
        "  translated = translator_model.generate(**translation_inputs)\n",
        "  arabic_caption = translator_tokenizer.decode(translated[0], skip_special_tokens=True).strip()\n",
        "\n",
        "  # --- Save result with scene start time ---\n",
        "  captions[frame_name] = {\n",
        "    \"scene_time\": round(start.get_seconds(), 2),  # Time in seconds, rounded for readability\n",
        "    \"english\": english_caption,\n",
        "    \"arabic\": arabic_caption\n",
        "    }\n",
        "\n",
        "  print(f\"âœ“ {frame_name} @ {start.get_seconds():.2f}s | EN: {english_caption} | AR: {arabic_caption}\")\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save JSON\n",
        "with open(captions_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… Captions saved to: {json_path}\")"
      ],
      "metadata": {
        "id": "hBfFY4xeKa5n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hBfFY4xeKa5n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§¹ 6. Clean the Script"
      ],
      "metadata": {
        "id": "ThznxocQv7uh"
      },
      "id": "ThznxocQv7uh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Clean the Transcript"
      ],
      "metadata": {
        "id": "4EykBV4P8EBl"
      },
      "id": "4EykBV4P8EBl"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.utils.charmap import CharMapper\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "\n",
        "# ==== Setup ====\n",
        "normalizer = CharMapper.builtin_mapper('arclean')\n",
        "\n",
        "disambig = MLEDisambiguator.pretrained()\n",
        "\n",
        "# ==== Function: Clean, Segment, POS tag ====\n",
        "def arabic_segment_and_analyze(text):\n",
        "    # Normalize Arabic text (removes Tatweel, unifies alef/ya, strips diacritics)\n",
        "    normalized_text = normalizer.map_string(text)\n",
        "\n",
        "    # Insert sentence breaks based on conjunctions and commas\n",
        "    segmented = re.sub(r'(?<=\\S)\\s+(?=(Ùˆ|Ø«Ù…|Ù„ÙƒÙ†|Ù|Ø¨Ø¹Ø¯|Ø¥Ù„Ø§ Ø£Ù†|ØºÙŠØ± Ø£Ù†)\\s)', r'. ', normalized_text)\n",
        "    segmented = re.sub(r'ØŒ|\\.\\s*', '.\\n', segmented)\n",
        "    sentences = [s.strip() for s in segmented.split('\\n') if s.strip()]\n",
        "\n",
        "    # POS tagging\n",
        "    results = []\n",
        "    for sent in sentences:\n",
        "        tokens = simple_word_tokenize(sent)\n",
        "        disambig_results = disambig.disambiguate(tokens)\n",
        "        tagged = [(tok, d.analyses[0].analysis['pos']) for tok, d in zip(tokens, disambig_results)]\n",
        "        results.append({\n",
        "            \"sentence\": sent,\n",
        "            \"tokens_with_pos\": tagged\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# ==== File Paths ====\n",
        "output_txt = os.path.join(transcripts_path, f\"{video_name}_segmented_ar.txt\")\n",
        "\n",
        "# ==== Run segmentation ====\n",
        "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "segmented = arabic_segment_and_analyze(raw_text)\n",
        "\n",
        "# ==== Save output ====\n",
        "with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in segmented:\n",
        "        f.write(item['sentence'] + '\\n')\n",
        "\n",
        "print(f\"âœ… Segmented transcript saved to: {output_txt}\")\n"
      ],
      "metadata": {
        "id": "m-lXmHon_n5G"
      },
      "id": "m-lXmHon_n5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Clean the captions"
      ],
      "metadata": {
        "id": "AGZ5Ydcc8MQa"
      },
      "id": "AGZ5Ydcc8MQa"
    },
    {
      "cell_type": "markdown",
      "id": "eef4a65b",
      "metadata": {
        "id": "eef4a65b"
      },
      "source": [
        "## ğŸ§  7. Process Transcript into Overlapping Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa7a3d4",
      "metadata": {
        "id": "1aa7a3d4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Load tokenizer ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# === Load transcript text ===\n",
        "with open(transcript_txt, encoding=\"utf-8\") as f:\n",
        "    full_transcript = f.read()\n",
        "\n",
        "# === Tokenize in small overlapping windows ===\n",
        "tokens = tokenizer.tokenize(full_transcript)\n",
        "\n",
        "# Define chunking parameters\n",
        "chunk_size = 128\n",
        "step = 64\n",
        "\n",
        "# Generate safe overlapping chunks (as token strings)\n",
        "token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens) - chunk_size + 1, step)]\n",
        "\n",
        "# Convert token chunks back to readable strings\n",
        "text_chunks = [tokenizer.convert_tokens_to_string(chunk) for chunk in token_chunks]\n",
        "\n",
        "# Prepare model-ready input (â‰¤512 tokens, padded)\n",
        "tokenized_chunks = [\n",
        "    tokenizer(chunk_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n",
        "    for chunk_text in text_chunks\n",
        "]\n",
        "\n",
        "# Preview\n",
        "for i, chunk in enumerate(text_chunks[:3]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd49d45",
      "metadata": {
        "id": "8fd49d45"
      },
      "source": [
        "## ğŸ–¼ï¸ 8. Load Scene Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32077c3a",
      "metadata": {
        "id": "32077c3a"
      },
      "outputs": [],
      "source": [
        "# Load captions from JSON\n",
        "import json\n",
        "captions_json = os.path.join(captions_path, f\"{os.path.splitext(video_filename)[0]}.json\")\n",
        "with open(captions_json, encoding='utf-8') as f:\n",
        "    scenes = json.load(f)\n",
        "scene_captions = [(scene, data[\"arabic\"]) for scene, data in scenes.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fea4337",
      "metadata": {
        "id": "9fea4337"
      },
      "source": [
        "## ğŸ”¡ 9. Embed Captions and Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20958ab",
      "metadata": {
        "id": "d20958ab"
      },
      "outputs": [],
      "source": [
        "# Encode using multilingual Sentence-BERT\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "caption_texts = [text for _, text in scene_captions]\n",
        "caption_embeddings = model.encode(caption_texts, convert_to_tensor=True)\n",
        "transcript_embeddings = model.encode(transcript_chunks, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501f8e8c",
      "metadata": {
        "id": "501f8e8c"
      },
      "source": [
        "## ğŸ”— 10. Match Captions to Transcript Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80225615",
      "metadata": {
        "id": "80225615"
      },
      "outputs": [],
      "source": [
        "# Find best transcript match for each caption\n",
        "results = []\n",
        "similarities = util.cos_sim(caption_embeddings, transcript_embeddings)\n",
        "for i, (scene_id, caption_text) in enumerate(scene_captions):\n",
        "    sim_scores = similarities[i]\n",
        "    top_idx = sim_scores.argmax().item()\n",
        "    results.append({\n",
        "        \"scene_id\": scene_id,\n",
        "        \"caption\": caption_text,\n",
        "        \"best_transcript_chunk\": transcript_chunks[top_idx],\n",
        "        \"similarity_score\": float(sim_scores[top_idx])\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c0e9ff",
      "metadata": {
        "id": "00c0e9ff"
      },
      "source": [
        "## ğŸ“¥ 11. Output Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458b6474",
      "metadata": {
        "id": "458b6474"
      },
      "outputs": [],
      "source": [
        "# Display a few matches\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df[['scene_id', 'caption', 'best_transcript_chunk', 'similarity_score']].head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_z2ibBr631mn",
        "e_VWwOEolSNk",
        "kJfBHPBq3nqm",
        "ThznxocQv7uh",
        "4EykBV4P8EBl",
        "eef4a65b",
        "8fd49d45",
        "9fea4337",
        "501f8e8c",
        "00c0e9ff"
      ],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}