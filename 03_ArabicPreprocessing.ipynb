{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de79610c-eaf6-41a5-c0e7-99024cf0261f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Requirement already satisfied: camel-tools in /usr/local/lib/python3.12/dist-packages (1.5.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.17.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from camel-tools) (5.5.2)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.16.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.3.8)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers<4.44.0,>=4.0 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (4.43.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.32.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.14.1)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.20.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from camel-tools) (4.67.1)\n",
            "Requirement already satisfied: muddler in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.1.3)\n",
            "Requirement already satisfied: camel-kenlm>=2025.4.8 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2025.4.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.6.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.19.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->camel-tools) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->camel-tools) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->camel-tools) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->camel-tools) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->camel-tools) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers<4.44.0,>=4.0->camel-tools) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n",
            "Usage:\n",
            "    camel_data (-i | --install) [-f | --force] <PACKAGE>\n",
            "    camel_data (-p | --post-install) <PACKAGE> <ARGS>...\n",
            "    camel_data (-l | --list)\n",
            "    camel_data (-u | --update)\n",
            "    camel_data (-v | --version)\n",
            "    camel_data (-h | --help)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# âœ… Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the morphology DB for MSA\n",
        "!camel_data -i morphology-db-msa-r13\n",
        "\n",
        "# Download the MLE disambiguator for MSA\n",
        "!camel_data -i disambig-mle-calima-msa-r13"
      ],
      "metadata": {
        "id": "q-NIP7_Bw6cD",
        "outputId": "c82d7fcc-0fb7-4db0-aa30-ef38f53fd4a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q-NIP7_Bw6cD",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following packages will be installed: 'morphology-db-msa-r13'\n",
            "Downloading package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:02<00:00, 13.8MB/s]\n",
            "Extracting package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:00<00:00, 553MB/s]\n",
            "The following packages will be installed: 'disambig-mle-calima-msa-r13'\n",
            "Downloading package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:23<00:00, 3.84MB/s]\n",
            "Extracting package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:00<00:00, 563MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uwk3G4EZ-Wq",
        "outputId": "624c606c-2257-4bfb-e63d-d0ba7bec08f2"
      },
      "id": "-uwk3G4EZ-Wq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define base paths\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar_with_timecodes.txt\")\n",
        "captions_json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "preprocessed_captions_path= os.path.join(preprocessed_path, f\"captions_{video_name}_ar.json\")\n",
        "preprocessed_transcript_path= os.path.join(preprocessed_path, f\"transcript_{video_name}_ar.json\")\n",
        "clean_captions_path= os.path.join(preprocessed_path, f\"clean_captions_{video_name}_ar.json\")\n",
        "clean_transcript_path= os.path.join(preprocessed_path, f\"clean_transcript_{video_name}_ar.json\")"
      ],
      "metadata": {
        "id": "A5xIEuotYZzf"
      },
      "id": "A5xIEuotYZzf",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "60040256",
      "metadata": {
        "id": "60040256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b075255-c127-4138-b550-18613f8ae329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 69 transcript segments from PaperMaking_ar_with_timecodes.txt.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ðŸ“„ Parse transcript file with timecodes\n",
        "import re\n",
        "\n",
        "def load_transcript(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    segments = []\n",
        "    pattern = re.compile(r\"\\[(\\d+\\.\\d+) - (\\d+\\.\\d+)\\]\\s+(.*)\")\n",
        "    for line in lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            start, end, text = match.groups()\n",
        "            segments.append({\n",
        "                \"start\": float(start),\n",
        "                \"end\": float(end),\n",
        "                \"text\": text.strip()\n",
        "            })\n",
        "    return segments\n",
        "\n",
        "assert os.path.exists(transcript_path), f\"Transcript file not found: {transcript_path}\"\n",
        "\n",
        "segments = load_transcript(transcript_path)\n",
        "print(f\"Loaded {len(segments)} transcript segments from {os.path.basename(transcript_path)}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "74bf09b8",
      "metadata": {
        "id": "74bf09b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade22d61-df07-439e-d28d-3e8dc4cfa586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 54 scene captions from PaperMaking.json.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ðŸ“„ Load caption JSON\n",
        "import json\n",
        "\n",
        "def load_captions(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    captions = []\n",
        "    for scene_id, meta in data.items():\n",
        "        captions.append({\n",
        "            \"scene_id\": scene_id,\n",
        "            \"scene_time\": meta[\"scene_time\"],\n",
        "            \"caption\": meta[\"arabic\"]\n",
        "        })\n",
        "    return captions\n",
        "assert os.path.exists(captions_json_path), f\"Transcript file not found: {captions_json_path}\"\n",
        "captions = load_captions(captions_json_path)\n",
        "print(f\"Loaded {len(captions)} scene captions from {os.path.basename(captions_json_path)}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "47197d02",
      "metadata": {
        "id": "47197d02"
      },
      "outputs": [],
      "source": [
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "disambig = MLEDisambiguator.pretrained()\n",
        "\n",
        "def preprocess(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    tokens = [t for t in simple_word_tokenize(text) if t.strip()]\n",
        "\n",
        "    results = disambig.disambiguate(tokens)\n",
        "\n",
        "    lemmas, pos_tags, diacs, glosses = [], [], [], []\n",
        "\n",
        "    for tok, res in zip(tokens, results):\n",
        "        if res.analyses:\n",
        "            top = res.analyses[0]               # ScoredAnalysis\n",
        "            a = top.analysis                    # dict of features\n",
        "            # Prefer lexeme; fall back gracefully\n",
        "            lemma = a.get('lex') or a.get('lemma') or a.get('stem') or a.get('diac') or tok\n",
        "            lemmas.append(lemma)\n",
        "            pos_tags.append(a.get('pos'))\n",
        "            diacs.append(a.get('diac'))\n",
        "            glosses.append(a.get('gloss'))\n",
        "        else:\n",
        "            lemmas.append(tok)\n",
        "            pos_tags.append(None)\n",
        "            diacs.append(None)\n",
        "            glosses.append(None)\n",
        "\n",
        "    return {\n",
        "        \"original\": text,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": lemmas,\n",
        "        \"lemmas_dediac\": [dediac_ar(l) if isinstance(l, str) else l for l in lemmas],\n",
        "        \"pos\": pos_tags,\n",
        "        \"diac\": diacs,\n",
        "        \"gloss\": glosses,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "575758c2",
      "metadata": {
        "id": "575758c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76e86ef-41eb-451a-87e5-9093aa9ee4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved: /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/transcript_PaperMaking_ar.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ðŸ” Process all transcript segments\n",
        "processed_segments = []\n",
        "for seg in segments:\n",
        "    proc = preprocess(seg[\"text\"])\n",
        "    processed_segments.append({\n",
        "        \"start\": seg[\"start\"],\n",
        "        \"end\": seg[\"end\"],\n",
        "        \"original\": seg[\"text\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ… Saved: \"+ preprocessed_transcript_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "56c6d282",
      "metadata": {
        "id": "56c6d282",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44cd9c2-d45e-4817-a631-682e9800e4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved: /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/captions_PaperMaking_ar.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ðŸ” Process all captions\n",
        "processed_captions = []\n",
        "for cap in captions:\n",
        "    proc = preprocess(cap[\"caption\"])\n",
        "    processed_captions.append({\n",
        "        \"scene_id\": cap[\"scene_id\"],\n",
        "        \"scene_time\": cap[\"scene_time\"],\n",
        "        \"original\": cap[\"caption\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ… Saved: \"+ preprocessed_captions_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Clean preprocessed captions & transcript (alignment-ready) ----\n",
        "# Uses your already-defined variables:\n",
        "# base_path, videos_path, transcripts_path, captions_path, preprocessed_path,\n",
        "# video_filename, video_path, video_name,\n",
        "# transcript_path, captions_json_path,\n",
        "# preprocessed_captions_path, preprocessed_transcript_path,\n",
        "# clean_captions_path, clean_transcript_path\n",
        "\n",
        "import os, json, re, string, csv\n",
        "\n",
        "# Where to save the summary CSV\n",
        "summary_csv_path = os.path.join(preprocessed_path, f\"cleaning_summary_{video_name}.csv\")\n",
        "\n",
        "# ---------- Config ----------\n",
        "# (tweak if needed)\n",
        "NEAR_DUP_JACCARD = 0.90  # consecutive near-duplicate threshold (captions-only)\n",
        "\n",
        "# Arabic punctuation + ASCII\n",
        "AR_PUNCT = set('''Ù€ØŒØ›ØŸâ€¦Â«Â»â€œâ€''') | set('\"\\'()[]{}:;,.!?-/\\\\|@#$%^&*_+=~`')\n",
        "\n",
        "# Light Arabic stopwords (lemmas)\n",
        "AR_STOP = set(\"\"\"\n",
        "ÙÙŠ Ù…Ù† Ø¹Ù„Ù‰ Ø¹Ù† Ø¥Ù„Ù‰ Ø­ØªÙ‰ Ù…Ù†Ø° Ø®Ù„Ø§Ù„ Ù„Ø¯Ù‰ Ù„ÙƒÙ„ Ù„Ù‡Ø§ Ù„Ù‡ Ù„Ù‡Ù… Ù‡Ùˆ Ù‡ÙŠ Ù‡Ù… Ù‡Ù† Ù†Ø­Ù† Ø§Ù†Ø§ Ø£Ù†Øª Ø§Ù†ØªÙŠ Ø§Ù†ØªÙ… Ù‡Ø°Ø§ Ù‡Ø°Ù‡ Ø°Ù„Ùƒ ØªÙ„Ùƒ Ù‡Ù†Ø§Ùƒ Ù‡Ù†Ø§ Ø­ÙŠØ« Ù„Ù…Ø§ Ù„Ø§ Ù…Ø§ ÙŠØ§ Ø£Ùˆ Ø£Ù… Ø«Ù… Ø¨Ù„ Ø¥Ù† Ø£Ù† ÙƒØ§Ù† ØªÙƒÙˆÙ† ÙŠÙƒÙˆÙ† ÙƒØ§Ù†Øª ÙƒØ§Ù†ÙˆØ§ ÙƒÙ† ÙƒÙ†Øª ÙƒÙ†Ø§ Ø¥Ø° Ø¥Ø°Ø§ Ø¥Ø°Ù† Ù‚Ø¯ Ù„Ù† Ù„Ù… Ø£Ù„Ø§ Ø§Ù„Ø§ Ø¥Ù„Ø§ ØºÙŠØ± Ø³ÙˆÙ‰ ÙƒÙ„Ø§ ÙƒÙ…Ø§ ÙƒÙŠÙ Ù…ØªÙ‰ Ø£ÙŠ Ø§Ù„Ø°ÙŠ Ø§Ù„ØªÙŠ Ø§Ù„Ø°ÙŠÙ† Ø§Ù„Ù„ÙˆØ§ØªÙŠ Ù‡Ù†Ø§Ù„Ùƒ Ù‡ÙƒØ°Ø§ ÙƒÙ„ Ø¬Ø¯Ø§ ÙÙ‚Ø· Ù…Ø«Ù„ Ø¨Ø¹Ø¶ Ø±Ø¨Ù…Ø§ Ø§ÙŠØ¶Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø£ÙŠØ¶Ø§Ù‹ Ø§ÙŠØ¶Ø§Ù‹ Ù„ÙŠØ³ Ù„ÙŠØ³ÙˆØ§ Ø¯ÙˆÙ† Ø¨Ø¯ÙˆÙ† Ù…Ø¹ Ø¨ÙŠÙ†\n",
        "\"\"\".split())\n",
        "\n",
        "# Minimal dialectâ†’MSA normalization for lemmas (extend as needed)\n",
        "DIALECT_MAP = {\n",
        "    \"ÙƒÙˆÙŠØ³\": \"Ø¬ÙŠØ¯\", \"ÙƒÙÙˆÙŽÙŠÙ‘ÙØ³\": \"Ø¬ÙŽÙŠÙ‘ÙØ¯\",\n",
        "    \"Ø§Ø²Ø§ÙŠ\": \"ÙƒÙŠÙ\", \"Ø§Ø²Ø§Ù‰\": \"ÙƒÙŠÙ\", \"Ø¥Ø²Ø§ÙŠ\": \"ÙƒÙŠÙ\",\n",
        "    \"Ø§Ø­Ù†Ø§\": \"Ù†Ø­Ù†\", \"Ø£Ø­Ù‘\": \"Ù†Ø­Ù†\",\n",
        "    \"Ø¹Ø§ÙŠØ²\": \"ÙŠØ±ÙŠØ¯\", \"Ø¹Ø§ÙŠØ²ÙŠÙ†\": \"Ù†Ø±ÙŠØ¯\",\n",
        "    \"Ø¨Ù†Ø¹Ù…Ù„\": \"Ù†ÙŽØ¹Ù’Ù…ÙŽÙ„\",\n",
        "    \"Ø¨Ù†Ø­Ø·Ù‡\": \"Ù†ÙŽØ¶ÙŽØ¹\",\n",
        "    \"Ø¨Ù†Ù†Ù‚Ø¹ÙˆÙ‡\": \"Ù†ÙŽÙ†Ù’Ù‚ÙŽØ¹ÙÙ‡Ù\",\n",
        "    \"ÙƒØ¯Ù‡\": \"Ù‡ÙƒØ°Ø§\",\n",
        "    \"ÙÙŠØ¹Ù†ÙŠ\": \"ÙŠØ¹Ù†ÙŠ\", \"ÙŠØ¹Ù†ÙŠ\": \"ÙŠØ¹Ù†ÙŠ\",\n",
        "    \"ÙƒØªÙŠØ±\": \"ÙƒØ«ÙŠØ±\",\n",
        "    \"Ø§Ù„Ù„ÙŠ\": \"Ø§Ù„Ø°ÙŠ\",\n",
        "    \"Ø¨ÙŠØ¨Ù‚Ù‰\": \"ÙŠÙƒÙˆÙ†\",\n",
        "    \"Ø¨ÙŠØ¬ÙŠ\": \"ÙŠØ£ØªÙŠ\",\n",
        "    \"Ø¨ÙŠØªØ±Ø³Ù…\": \"ÙŠÙØ±Ù’Ø³ÙŽÙ…\",\n",
        "    \"Ù…ÙÙŠØ´\": \"Ù„Ø§ ÙŠÙˆØ¬Ø¯\",\n",
        "    \"Ù…Ø´\": \"Ù„ÙŠØ³\",\n",
        "    \"Ø·Ø¨Ø¹Ø§\": \"Ø·Ø¨Ø¹Ø§Ù‹\",\n",
        "    \"Ø¨Ù†Ø¬ÙŠØ¨\": \"Ù†ÙŽØ¬Ù’Ù„ÙØ¨\",\n",
        "}\n",
        "\n",
        "def is_arabic_char(ch: str) -> bool:\n",
        "    code = ord(ch)\n",
        "    return ((0x0600 <= code <= 0x06FF) or\n",
        "            (0x0750 <= code <= 0x077F) or\n",
        "            (0x08A0 <= code <= 0x08FF))\n",
        "\n",
        "def is_mostly_arabic(token: str) -> bool:\n",
        "    if not token:\n",
        "        return False\n",
        "    letters = [c for c in token if c.isalpha()]\n",
        "    if not letters:\n",
        "        return False\n",
        "    ar = sum(1 for c in letters if is_arabic_char(c))\n",
        "    return ar / len(letters) >= 0.6\n",
        "\n",
        "def normalize_punct(s: str) -> str:\n",
        "    s = (s or \"\").replace('Ù€', '')\n",
        "    s = s.replace('â€œ','\"').replace('â€','\"').replace('â€™',\"'\").replace('â€˜',\"'\")\n",
        "    s = s.replace('Â«','\"').replace('Â»','\"')\n",
        "    # collapse repeated punctuation\n",
        "    s = re.sub(r'([:;,\\.\\-\\â€”\\â€“\\!ØŸ\\?\"])\\\\1{1,}', r'\\1', s)\n",
        "    return s\n",
        "\n",
        "def clean_tokens(tokens, lemmas):\n",
        "    \"\"\"Return (tokens_clean, lemmas_clean) with:\n",
        "       - punctuation normalization\n",
        "       - drop pure punct / single Latin\n",
        "       - prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "       - stopword trimming on lemmas\n",
        "       - small dialect normalization on lemmas\n",
        "       - collapse consecutive duplicates\n",
        "    \"\"\"\n",
        "    tokens = tokens or []\n",
        "    lemmas = lemmas or []\n",
        "    out_toks, out_lems = [], []\n",
        "\n",
        "    for t, l in zip(tokens, lemmas):\n",
        "        t = normalize_punct((t or \"\").strip())\n",
        "        l = normalize_punct((l or \"\").strip())\n",
        "\n",
        "        # empty?\n",
        "        if not t and not l:\n",
        "            continue\n",
        "\n",
        "        # drop pure punctuation or single Latin char\n",
        "        if t and (all((ch in AR_PUNCT or ch.isspace()) for ch in t) or re.fullmatch(r\"[A-Za-z]\", t)):\n",
        "            continue\n",
        "        if l and (all((ch in AR_PUNCT or ch.isspace()) for ch in l) or re.fullmatch(r\"[A-Za-z]\", l)):\n",
        "            l = \"\"  # force fallback\n",
        "\n",
        "        # prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "        if not l or not is_mostly_arabic(l):\n",
        "            l = t if is_mostly_arabic(t) else l\n",
        "\n",
        "        # stopword trimming (lemmas)\n",
        "        if l in AR_STOP:\n",
        "            # keep token if it looks meaningful Arabic\n",
        "            if t and t not in AR_STOP and is_mostly_arabic(t):\n",
        "                out_toks.append(t)\n",
        "                out_lems.append(t)\n",
        "            continue\n",
        "\n",
        "        # dialect normalization\n",
        "        l = DIALECT_MAP.get(l, l)\n",
        "\n",
        "        out_toks.append(t)\n",
        "        out_lems.append(l)\n",
        "\n",
        "    # collapse consecutive duplicate (token,lemma) pairs\n",
        "    dedup_toks, dedup_lems = [], []\n",
        "    prev = None\n",
        "    for t, l in zip(out_toks, out_lems):\n",
        "        pair = (t, l)\n",
        "        if pair != prev:\n",
        "            dedup_toks.append(t)\n",
        "            dedup_lems.append(l)\n",
        "        prev = pair\n",
        "    return dedup_toks, dedup_lems\n",
        "\n",
        "def jaccard(a, b):\n",
        "    sa, sb = set(a), set(b)\n",
        "    if not sa and not sb:\n",
        "        return 1.0\n",
        "    if not sa or not sb:\n",
        "        return 0.0\n",
        "    return len(sa & sb) / max(1, len(sa | sb))\n",
        "\n",
        "# ---------- Load inputs ----------\n",
        "assert os.path.exists(preprocessed_captions_path), f\"Missing: {preprocessed_captions_path}\"\n",
        "assert os.path.exists(preprocessed_transcript_path), f\"Missing: {preprocessed_transcript_path}\"\n",
        "\n",
        "with open(preprocessed_captions_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    captions = json.load(f)\n",
        "with open(preprocessed_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    transcript = json.load(f)\n",
        "\n",
        "# ---------- Clean captions (+ remove consecutive near-duplicates) ----------\n",
        "cleaned_caps = []\n",
        "removed_indices = []\n",
        "prev_lems = None\n",
        "\n",
        "for i, item in enumerate(captions):\n",
        "    toks = item.get(\"tokens\", [])\n",
        "    lems = item.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "\n",
        "    # skip if empty after cleaning\n",
        "    if not clems and not ctoks:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    # remove consecutive near-duplicates at lemma level\n",
        "    if prev_lems is not None and jaccard(prev_lems, clems) >= NEAR_DUP_JACCARD:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    cleaned_caps.append({\n",
        "        \"scene_id\": item.get(\"scene_id\"),\n",
        "        \"scene_time\": item.get(\"scene_time\"),\n",
        "        \"original\": item.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "    prev_lems = clems\n",
        "\n",
        "# ---------- Clean transcript ----------\n",
        "cleaned_tr = []\n",
        "for utt in transcript:\n",
        "    toks = utt.get(\"tokens\", [])\n",
        "    lems = utt.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "    cleaned_tr.append({\n",
        "        \"start\": utt.get(\"start\"),\n",
        "        \"end\": utt.get(\"end\"),\n",
        "        \"original\": utt.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "\n",
        "# ---------- Save outputs ----------\n",
        "os.makedirs(preprocessed_path, exist_ok=True)\n",
        "with open(clean_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_caps, f, ensure_ascii=False, indent=2)\n",
        "with open(clean_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_tr, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------- Summary CSV ----------\n",
        "def count(items, key):\n",
        "    return sum(len(x.get(key, [])) for x in items)\n",
        "\n",
        "summary_rows = [\n",
        "    {\n",
        "        \"dataset\": \"captions\",\n",
        "        \"items_before\": len(captions),\n",
        "        \"items_after\": len(cleaned_caps),\n",
        "        \"tokens_before\": count(captions, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_caps, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(captions, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_caps, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": len(removed_indices),\n",
        "    },\n",
        "    {\n",
        "        \"dataset\": \"transcript\",\n",
        "        \"items_before\": len(transcript),\n",
        "        \"items_after\": len(cleaned_tr),\n",
        "        \"tokens_before\": count(transcript, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_tr, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(transcript, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_tr, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": 0,\n",
        "    },\n",
        "]\n",
        "\n",
        "with open(summary_csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(summary_rows)\n",
        "\n",
        "print(\"âœ… Cleaning done.\")\n",
        "print(f\"â€¢ Clean captions  â†’ {clean_captions_path}\")\n",
        "print(f\"â€¢ Clean transcript â†’ {clean_transcript_path}\")\n",
        "print(f\"â€¢ Summary CSV      â†’ {summary_csv_path}\")\n"
      ],
      "metadata": {
        "id": "F2JznbEq2JP4",
        "outputId": "bfc26e04-2cd5-46c3-818a-9d7060bea960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "F2JznbEq2JP4",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cleaning done.\n",
            "â€¢ Clean captions  â†’ /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/clean_captions_PaperMaking_ar.json\n",
            "â€¢ Clean transcript â†’ /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/clean_transcript_PaperMaking_ar.json\n",
            "â€¢ Summary CSV      â†’ /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/cleaning_summary_PaperMaking.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}