{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ‚úÖ Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the morphology DB for MSA\n",
        "!camel_data -i morphology-db-msa-r13\n",
        "\n",
        "# Download the MLE disambiguator for MSA\n",
        "!camel_data -i disambig-mle-calima-msa-r13"
      ],
      "metadata": {
        "id": "q-NIP7_Bw6cD"
      },
      "id": "q-NIP7_Bw6cD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-uwk3G4EZ-Wq"
      },
      "id": "-uwk3G4EZ-Wq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define base paths\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar_with_timecodes.txt\")\n",
        "captions_json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "preprocessed_captions_path= os.path.join(preprocessed_path, f\"captions_{video_name}_ar.json\")\n",
        "preprocessed_transcript_path= os.path.join(preprocessed_path, f\"transcript_{video_name}_ar.json\")\n",
        "clean_captions_path= os.path.join(preprocessed_path, f\"clean_captions_{video_name}_ar.json\")\n",
        "clean_transcript_path= os.path.join(preprocessed_path, f\"clean_transcript_{video_name}_ar.json\")"
      ],
      "metadata": {
        "id": "A5xIEuotYZzf"
      },
      "id": "A5xIEuotYZzf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60040256",
      "metadata": {
        "id": "60040256"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üìÑ Parse transcript file with timecodes\n",
        "import re\n",
        "\n",
        "def load_transcript(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    segments = []\n",
        "    pattern = re.compile(r\"\\[(\\d+\\.\\d+) - (\\d+\\.\\d+)\\]\\s+(.*)\")\n",
        "    for line in lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            start, end, text = match.groups()\n",
        "            segments.append({\n",
        "                \"start\": float(start),\n",
        "                \"end\": float(end),\n",
        "                \"text\": text.strip()\n",
        "            })\n",
        "    return segments\n",
        "\n",
        "assert os.path.exists(transcript_path), f\"Transcript file not found: {transcript_path}\"\n",
        "\n",
        "segments = load_transcript(transcript_path)\n",
        "print(f\"Loaded {len(segments)} transcript segments from {os.path.basename(transcript_path)}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74bf09b8",
      "metadata": {
        "id": "74bf09b8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üìÑ Load caption JSON\n",
        "import json\n",
        "\n",
        "def load_captions(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    captions = []\n",
        "    for scene_id, meta in data.items():\n",
        "        captions.append({\n",
        "            \"scene_id\": scene_id,\n",
        "            \"scene_time\": meta[\"scene_time\"],\n",
        "            \"caption\": meta[\"arabic\"]\n",
        "        })\n",
        "    return captions\n",
        "assert os.path.exists(captions_json_path), f\"Transcript file not found: {captions_json_path}\"\n",
        "captions = load_captions(captions_json_path)\n",
        "print(f\"Loaded {len(captions)} scene captions from {os.path.basename(captions_json_path)}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47197d02",
      "metadata": {
        "id": "47197d02"
      },
      "outputs": [],
      "source": [
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "disambig = MLEDisambiguator.pretrained()\n",
        "\n",
        "def preprocess(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    tokens = [t for t in simple_word_tokenize(text) if t.strip()]\n",
        "\n",
        "    results = disambig.disambiguate(tokens)\n",
        "\n",
        "    lemmas, pos_tags, diacs, glosses = [], [], [], []\n",
        "\n",
        "    for tok, res in zip(tokens, results):\n",
        "        if res.analyses:\n",
        "            top = res.analyses[0]               # ScoredAnalysis\n",
        "            a = top.analysis                    # dict of features\n",
        "            # Prefer lexeme; fall back gracefully\n",
        "            lemma = a.get('lex') or a.get('lemma') or a.get('stem') or a.get('diac') or tok\n",
        "            lemmas.append(lemma)\n",
        "            pos_tags.append(a.get('pos'))\n",
        "            diacs.append(a.get('diac'))\n",
        "            glosses.append(a.get('gloss'))\n",
        "        else:\n",
        "            lemmas.append(tok)\n",
        "            pos_tags.append(None)\n",
        "            diacs.append(None)\n",
        "            glosses.append(None)\n",
        "\n",
        "    return {\n",
        "        \"original\": text,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": lemmas,\n",
        "        \"lemmas_dediac\": [dediac_ar(l) if isinstance(l, str) else l for l in lemmas],\n",
        "        \"pos\": pos_tags,\n",
        "        \"diac\": diacs,\n",
        "        \"gloss\": glosses,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575758c2",
      "metadata": {
        "id": "575758c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üîÅ Process all transcript segments\n",
        "processed_segments = []\n",
        "for seg in segments:\n",
        "    proc = preprocess(seg[\"text\"])\n",
        "    processed_segments.append({\n",
        "        \"start\": seg[\"start\"],\n",
        "        \"end\": seg[\"end\"],\n",
        "        \"original\": seg[\"text\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved: \"+ preprocessed_transcript_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c6d282",
      "metadata": {
        "id": "56c6d282"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üîÅ Process all captions\n",
        "processed_captions = []\n",
        "for cap in captions:\n",
        "    proc = preprocess(cap[\"caption\"])\n",
        "    processed_captions.append({\n",
        "        \"scene_id\": cap[\"scene_id\"],\n",
        "        \"scene_time\": cap[\"scene_time\"],\n",
        "        \"original\": cap[\"caption\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved: \"+ preprocessed_captions_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Clean preprocessed captions & transcript (alignment-ready) ----\n",
        "# Uses your already-defined variables:\n",
        "# base_path, videos_path, transcripts_path, captions_path, preprocessed_path,\n",
        "# video_filename, video_path, video_name,\n",
        "# transcript_path, captions_json_path,\n",
        "# preprocessed_captions_path, preprocessed_transcript_path,\n",
        "# clean_captions_path, clean_transcript_path\n",
        "\n",
        "import os, json, re, string, csv\n",
        "\n",
        "# Where to save the summary CSV\n",
        "summary_csv_path = os.path.join(preprocessed_path, f\"cleaning_summary_{video_name}.csv\")\n",
        "\n",
        "# ---------- Config ----------\n",
        "# (tweak if needed)\n",
        "NEAR_DUP_JACCARD = 0.90  # consecutive near-duplicate threshold (captions-only)\n",
        "\n",
        "# Arabic punctuation + ASCII\n",
        "AR_PUNCT = set('''ŸÄÿåÿõÿü‚Ä¶¬´¬ª‚Äú‚Äù''') | set('\"\\'()[]{}:;,.!?-/\\\\|@#$%^&*_+=~`')\n",
        "\n",
        "# Light Arabic stopwords (lemmas)\n",
        "AR_STOP = set(\"\"\"\n",
        "ŸÅŸä ŸÖŸÜ ÿπŸÑŸâ ÿπŸÜ ÿ•ŸÑŸâ ÿ≠ÿ™Ÿâ ŸÖŸÜÿ∞ ÿÆŸÑÿßŸÑ ŸÑÿØŸâ ŸÑŸÉŸÑ ŸÑŸáÿß ŸÑŸá ŸÑŸáŸÖ ŸáŸà ŸáŸä ŸáŸÖ ŸáŸÜ ŸÜÿ≠ŸÜ ÿßŸÜÿß ÿ£ŸÜÿ™ ÿßŸÜÿ™Ÿä ÿßŸÜÿ™ŸÖ Ÿáÿ∞ÿß Ÿáÿ∞Ÿá ÿ∞ŸÑŸÉ ÿ™ŸÑŸÉ ŸáŸÜÿßŸÉ ŸáŸÜÿß ÿ≠Ÿäÿ´ ŸÑŸÖÿß ŸÑÿß ŸÖÿß Ÿäÿß ÿ£Ÿà ÿ£ŸÖ ÿ´ŸÖ ÿ®ŸÑ ÿ•ŸÜ ÿ£ŸÜ ŸÉÿßŸÜ ÿ™ŸÉŸàŸÜ ŸäŸÉŸàŸÜ ŸÉÿßŸÜÿ™ ŸÉÿßŸÜŸàÿß ŸÉŸÜ ŸÉŸÜÿ™ ŸÉŸÜÿß ÿ•ÿ∞ ÿ•ÿ∞ÿß ÿ•ÿ∞ŸÜ ŸÇÿØ ŸÑŸÜ ŸÑŸÖ ÿ£ŸÑÿß ÿßŸÑÿß ÿ•ŸÑÿß ÿ∫Ÿäÿ± ÿ≥ŸàŸâ ŸÉŸÑÿß ŸÉŸÖÿß ŸÉŸäŸÅ ŸÖÿ™Ÿâ ÿ£Ÿä ÿßŸÑÿ∞Ÿä ÿßŸÑÿ™Ÿä ÿßŸÑÿ∞ŸäŸÜ ÿßŸÑŸÑŸàÿßÿ™Ÿä ŸáŸÜÿßŸÑŸÉ ŸáŸÉÿ∞ÿß ŸÉŸÑ ÿ¨ÿØÿß ŸÅŸÇÿ∑ ŸÖÿ´ŸÑ ÿ®ÿπÿ∂ ÿ±ÿ®ŸÖÿß ÿßŸäÿ∂ÿß ÿ£Ÿäÿ∂Ÿãÿß ÿ£Ÿäÿ∂ÿßŸã ÿßŸäÿ∂ÿßŸã ŸÑŸäÿ≥ ŸÑŸäÿ≥Ÿàÿß ÿØŸàŸÜ ÿ®ÿØŸàŸÜ ŸÖÿπ ÿ®ŸäŸÜ\n",
        "\"\"\".split())\n",
        "\n",
        "# Minimal dialect‚ÜíMSA normalization for lemmas (extend as needed)\n",
        "DIALECT_MAP = {\n",
        "    \"ŸÉŸàŸäÿ≥\": \"ÿ¨ŸäÿØ\", \"ŸÉŸèŸàŸéŸäŸëŸêÿ≥\": \"ÿ¨ŸéŸäŸëŸêÿØ\",\n",
        "    \"ÿßÿ≤ÿßŸä\": \"ŸÉŸäŸÅ\", \"ÿßÿ≤ÿßŸâ\": \"ŸÉŸäŸÅ\", \"ÿ•ÿ≤ÿßŸä\": \"ŸÉŸäŸÅ\",\n",
        "    \"ÿßÿ≠ŸÜÿß\": \"ŸÜÿ≠ŸÜ\", \"ÿ£ÿ≠Ÿë\": \"ŸÜÿ≠ŸÜ\",\n",
        "    \"ÿπÿßŸäÿ≤\": \"Ÿäÿ±ŸäÿØ\", \"ÿπÿßŸäÿ≤ŸäŸÜ\": \"ŸÜÿ±ŸäÿØ\",\n",
        "    \"ÿ®ŸÜÿπŸÖŸÑ\": \"ŸÜŸéÿπŸíŸÖŸéŸÑ\",\n",
        "    \"ÿ®ŸÜÿ≠ÿ∑Ÿá\": \"ŸÜŸéÿ∂Ÿéÿπ\",\n",
        "    \"ÿ®ŸÜŸÜŸÇÿπŸàŸá\": \"ŸÜŸéŸÜŸíŸÇŸéÿπŸèŸáŸè\",\n",
        "    \"ŸÉÿØŸá\": \"ŸáŸÉÿ∞ÿß\",\n",
        "    \"ŸÅŸäÿπŸÜŸä\": \"ŸäÿπŸÜŸä\", \"ŸäÿπŸÜŸä\": \"ŸäÿπŸÜŸä\",\n",
        "    \"ŸÉÿ™Ÿäÿ±\": \"ŸÉÿ´Ÿäÿ±\",\n",
        "    \"ÿßŸÑŸÑŸä\": \"ÿßŸÑÿ∞Ÿä\",\n",
        "    \"ÿ®Ÿäÿ®ŸÇŸâ\": \"ŸäŸÉŸàŸÜ\",\n",
        "    \"ÿ®Ÿäÿ¨Ÿä\": \"Ÿäÿ£ÿ™Ÿä\",\n",
        "    \"ÿ®Ÿäÿ™ÿ±ÿ≥ŸÖ\": \"ŸäŸèÿ±Ÿíÿ≥ŸéŸÖ\",\n",
        "    \"ŸÖŸÅŸäÿ¥\": \"ŸÑÿß ŸäŸàÿ¨ÿØ\",\n",
        "    \"ŸÖÿ¥\": \"ŸÑŸäÿ≥\",\n",
        "    \"ÿ∑ÿ®ÿπÿß\": \"ÿ∑ÿ®ÿπÿßŸã\",\n",
        "    \"ÿ®ŸÜÿ¨Ÿäÿ®\": \"ŸÜŸéÿ¨ŸíŸÑŸêÿ®\",\n",
        "}\n",
        "\n",
        "def is_arabic_char(ch: str) -> bool:\n",
        "    code = ord(ch)\n",
        "    return ((0x0600 <= code <= 0x06FF) or\n",
        "            (0x0750 <= code <= 0x077F) or\n",
        "            (0x08A0 <= code <= 0x08FF))\n",
        "\n",
        "def is_mostly_arabic(token: str) -> bool:\n",
        "    if not token:\n",
        "        return False\n",
        "    letters = [c for c in token if c.isalpha()]\n",
        "    if not letters:\n",
        "        return False\n",
        "    ar = sum(1 for c in letters if is_arabic_char(c))\n",
        "    return ar / len(letters) >= 0.6\n",
        "\n",
        "def normalize_punct(s: str) -> str:\n",
        "    s = (s or \"\").replace('ŸÄ', '')\n",
        "    s = s.replace('‚Äú','\"').replace('‚Äù','\"').replace('‚Äô',\"'\").replace('‚Äò',\"'\")\n",
        "    s = s.replace('¬´','\"').replace('¬ª','\"')\n",
        "    # collapse repeated punctuation\n",
        "    s = re.sub(r'([:;,\\.\\-\\‚Äî\\‚Äì\\!ÿü\\?\"])\\\\1{1,}', r'\\1', s)\n",
        "    return s\n",
        "\n",
        "def clean_tokens(tokens, lemmas):\n",
        "    \"\"\"Return (tokens_clean, lemmas_clean) with:\n",
        "       - punctuation normalization\n",
        "       - drop pure punct / single Latin\n",
        "       - prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "       - stopword trimming on lemmas\n",
        "       - small dialect normalization on lemmas\n",
        "       - collapse consecutive duplicates\n",
        "    \"\"\"\n",
        "    tokens = tokens or []\n",
        "    lemmas = lemmas or []\n",
        "    out_toks, out_lems = [], []\n",
        "\n",
        "    for t, l in zip(tokens, lemmas):\n",
        "        t = normalize_punct((t or \"\").strip())\n",
        "        l = normalize_punct((l or \"\").strip())\n",
        "\n",
        "        # empty?\n",
        "        if not t and not l:\n",
        "            continue\n",
        "\n",
        "        # drop pure punctuation or single Latin char\n",
        "        if t and (all((ch in AR_PUNCT or ch.isspace()) for ch in t) or re.fullmatch(r\"[A-Za-z]\", t)):\n",
        "            continue\n",
        "        if l and (all((ch in AR_PUNCT or ch.isspace()) for ch in l) or re.fullmatch(r\"[A-Za-z]\", l)):\n",
        "            l = \"\"  # force fallback\n",
        "\n",
        "        # prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "        if not l or not is_mostly_arabic(l):\n",
        "            l = t if is_mostly_arabic(t) else l\n",
        "\n",
        "        # stopword trimming (lemmas)\n",
        "        if l in AR_STOP:\n",
        "            # keep token if it looks meaningful Arabic\n",
        "            if t and t not in AR_STOP and is_mostly_arabic(t):\n",
        "                out_toks.append(t)\n",
        "                out_lems.append(t)\n",
        "            continue\n",
        "\n",
        "        # dialect normalization\n",
        "        l = DIALECT_MAP.get(l, l)\n",
        "\n",
        "        out_toks.append(t)\n",
        "        out_lems.append(l)\n",
        "\n",
        "    # collapse consecutive duplicate (token,lemma) pairs\n",
        "    dedup_toks, dedup_lems = [], []\n",
        "    prev = None\n",
        "    for t, l in zip(out_toks, out_lems):\n",
        "        pair = (t, l)\n",
        "        if pair != prev:\n",
        "            dedup_toks.append(t)\n",
        "            dedup_lems.append(l)\n",
        "        prev = pair\n",
        "    return dedup_toks, dedup_lems\n",
        "\n",
        "def jaccard(a, b):\n",
        "    sa, sb = set(a), set(b)\n",
        "    if not sa and not sb:\n",
        "        return 1.0\n",
        "    if not sa or not sb:\n",
        "        return 0.0\n",
        "    return len(sa & sb) / max(1, len(sa | sb))\n",
        "\n",
        "# ---------- Load inputs ----------\n",
        "assert os.path.exists(preprocessed_captions_path), f\"Missing: {preprocessed_captions_path}\"\n",
        "assert os.path.exists(preprocessed_transcript_path), f\"Missing: {preprocessed_transcript_path}\"\n",
        "\n",
        "with open(preprocessed_captions_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    captions = json.load(f)\n",
        "with open(preprocessed_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    transcript = json.load(f)\n",
        "\n",
        "# ---------- Clean captions (+ remove consecutive near-duplicates) ----------\n",
        "cleaned_caps = []\n",
        "removed_indices = []\n",
        "prev_lems = None\n",
        "\n",
        "for i, item in enumerate(captions):\n",
        "    toks = item.get(\"tokens\", [])\n",
        "    lems = item.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "\n",
        "    # skip if empty after cleaning\n",
        "    if not clems and not ctoks:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    # remove consecutive near-duplicates at lemma level\n",
        "    if prev_lems is not None and jaccard(prev_lems, clems) >= NEAR_DUP_JACCARD:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    cleaned_caps.append({\n",
        "        \"scene_id\": item.get(\"scene_id\"),\n",
        "        \"scene_time\": item.get(\"scene_time\"),\n",
        "        \"original\": item.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "    prev_lems = clems\n",
        "\n",
        "# ---------- Clean transcript ----------\n",
        "cleaned_tr = []\n",
        "for utt in transcript:\n",
        "    toks = utt.get(\"tokens\", [])\n",
        "    lems = utt.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "    cleaned_tr.append({\n",
        "        \"start\": utt.get(\"start\"),\n",
        "        \"end\": utt.get(\"end\"),\n",
        "        \"original\": utt.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "\n",
        "# ---------- Save outputs ----------\n",
        "os.makedirs(preprocessed_path, exist_ok=True)\n",
        "with open(clean_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_caps, f, ensure_ascii=False, indent=2)\n",
        "with open(clean_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_tr, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------- Summary CSV ----------\n",
        "def count(items, key):\n",
        "    return sum(len(x.get(key, [])) for x in items)\n",
        "\n",
        "summary_rows = [\n",
        "    {\n",
        "        \"dataset\": \"captions\",\n",
        "        \"items_before\": len(captions),\n",
        "        \"items_after\": len(cleaned_caps),\n",
        "        \"tokens_before\": count(captions, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_caps, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(captions, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_caps, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": len(removed_indices),\n",
        "    },\n",
        "    {\n",
        "        \"dataset\": \"transcript\",\n",
        "        \"items_before\": len(transcript),\n",
        "        \"items_after\": len(cleaned_tr),\n",
        "        \"tokens_before\": count(transcript, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_tr, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(transcript, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_tr, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": 0,\n",
        "    },\n",
        "]\n",
        "\n",
        "with open(summary_csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(summary_rows)\n",
        "\n",
        "print(\"‚úÖ Cleaning done.\")\n",
        "print(f\"‚Ä¢ Clean captions  ‚Üí {clean_captions_path}\")\n",
        "print(f\"‚Ä¢ Clean transcript ‚Üí {clean_transcript_path}\")\n",
        "print(f\"‚Ä¢ Summary CSV      ‚Üí {summary_csv_path}\")\n"
      ],
      "metadata": {
        "id": "F2JznbEq2JP4"
      },
      "id": "F2JznbEq2JP4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}