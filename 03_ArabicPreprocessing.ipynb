{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# âœ… Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the morphology DB for MSA\n",
        "!camel_data -i morphology-db-msa-r13\n",
        "\n",
        "# Download the MLE disambiguator for MSA\n",
        "!camel_data -i disambig-mle-calima-msa-r13"
      ],
      "metadata": {
        "id": "q-NIP7_Bw6cD"
      },
      "id": "q-NIP7_Bw6cD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-uwk3G4EZ-Wq"
      },
      "id": "-uwk3G4EZ-Wq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define base paths\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar_with_timecodes.txt\")\n",
        "captions_json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "preprocessed_captions_path= os.path.join(preprocessed_path, f\"captions_{video_name}_ar.json\")\n",
        "preprocessed_transcript_path= os.path.join(preprocessed_path, f\"transcript_{video_name}_ar.json\")\n",
        "clean_captions_path= os.path.join(preprocessed_path, f\"clean_captions_{video_name}_ar.json\")\n",
        "clean_transcript_path= os.path.join(preprocessed_path, f\"clean_transcript_{video_name}_ar.json\")"
      ],
      "metadata": {
        "id": "A5xIEuotYZzf"
      },
      "id": "A5xIEuotYZzf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60040256",
      "metadata": {
        "id": "60040256"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ðŸ“„ Parse transcript file with timecodes\n",
        "import re\n",
        "\n",
        "def load_transcript(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    segments = []\n",
        "    pattern = re.compile(r\"\\[(\\d+\\.\\d+) - (\\d+\\.\\d+)\\]\\s+(.*)\")\n",
        "    for line in lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            start, end, text = match.groups()\n",
        "            segments.append({\n",
        "                \"start\": float(start),\n",
        "                \"end\": float(end),\n",
        "                \"text\": text.strip()\n",
        "            })\n",
        "    return segments\n",
        "\n",
        "assert os.path.exists(transcript_path), f\"Transcript file not found: {transcript_path}\"\n",
        "\n",
        "segments = load_transcript(transcript_path)\n",
        "print(f\"Loaded {len(segments)} transcript segments from {os.path.basename(transcript_path)}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74bf09b8",
      "metadata": {
        "id": "74bf09b8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ðŸ“„ Load caption JSON\n",
        "import json\n",
        "\n",
        "def load_captions(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    captions = []\n",
        "    for scene_id, meta in data.items():\n",
        "        captions.append({\n",
        "            \"scene_id\": scene_id,\n",
        "            \"scene_time\": meta[\"scene_time\"],\n",
        "            \"caption\": meta[\"arabic\"]\n",
        "        })\n",
        "    return captions\n",
        "assert os.path.exists(captions_json_path), f\"Transcript file not found: {captions_json_path}\"\n",
        "captions = load_captions(captions_json_path)\n",
        "print(f\"Loaded {len(captions)} scene captions from {os.path.basename(captions_json_path)}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47197d02",
      "metadata": {
        "id": "47197d02"
      },
      "outputs": [],
      "source": [
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "disambig = MLEDisambiguator.pretrained()\n",
        "\n",
        "def preprocess(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    tokens = [t for t in simple_word_tokenize(text) if t.strip()]\n",
        "\n",
        "    results = disambig.disambiguate(tokens)\n",
        "\n",
        "    lemmas, pos_tags, diacs, glosses = [], [], [], []\n",
        "\n",
        "    for tok, res in zip(tokens, results):\n",
        "        if res.analyses:\n",
        "            top = res.analyses[0]               # ScoredAnalysis\n",
        "            a = top.analysis                    # dict of features\n",
        "            # Prefer lexeme; fall back gracefully\n",
        "            lemma = a.get('lex') or a.get('lemma') or a.get('stem') or a.get('diac') or tok\n",
        "            lemmas.append(lemma)\n",
        "            pos_tags.append(a.get('pos'))\n",
        "            diacs.append(a.get('diac'))\n",
        "            glosses.append(a.get('gloss'))\n",
        "        else:\n",
        "            lemmas.append(tok)\n",
        "            pos_tags.append(None)\n",
        "            diacs.append(None)\n",
        "            glosses.append(None)\n",
        "\n",
        "    return {\n",
        "        \"original\": text,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": lemmas,\n",
        "        \"lemmas_dediac\": [dediac_ar(l) if isinstance(l, str) else l for l in lemmas],\n",
        "        \"pos\": pos_tags,\n",
        "        \"diac\": diacs,\n",
        "        \"gloss\": glosses,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575758c2",
      "metadata": {
        "id": "575758c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ðŸ” Process all transcript segments\n",
        "processed_segments = []\n",
        "for seg in segments:\n",
        "    proc = preprocess(seg[\"text\"])\n",
        "    processed_segments.append({\n",
        "        \"start\": seg[\"start\"],\n",
        "        \"end\": seg[\"end\"],\n",
        "        \"original\": seg[\"text\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ… Saved: \"+ preprocessed_transcript_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c6d282",
      "metadata": {
        "id": "56c6d282"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ðŸ” Process all captions\n",
        "processed_captions = []\n",
        "for cap in captions:\n",
        "    proc = preprocess(cap[\"caption\"])\n",
        "    processed_captions.append({\n",
        "        \"scene_id\": cap[\"scene_id\"],\n",
        "        \"scene_time\": cap[\"scene_time\"],\n",
        "        \"original\": cap[\"caption\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ… Saved: \"+ preprocessed_captions_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Clean preprocessed captions & transcript (alignment-ready) ----\n",
        "# Uses your already-defined variables:\n",
        "# base_path, videos_path, transcripts_path, captions_path, preprocessed_path,\n",
        "# video_filename, video_path, video_name,\n",
        "# transcript_path, captions_json_path,\n",
        "# preprocessed_captions_path, preprocessed_transcript_path,\n",
        "# clean_captions_path, clean_transcript_path\n",
        "\n",
        "import os, json, re, string, csv\n",
        "\n",
        "# Where to save the summary CSV\n",
        "summary_csv_path = os.path.join(preprocessed_path, f\"cleaning_summary_{video_name}.csv\")\n",
        "\n",
        "# ---------- Config ----------\n",
        "# (tweak if needed)\n",
        "NEAR_DUP_JACCARD = 0.90  # consecutive near-duplicate threshold (captions-only)\n",
        "\n",
        "# Arabic punctuation + ASCII\n",
        "AR_PUNCT = set('''Ù€ØŒØ›ØŸâ€¦Â«Â»â€œâ€''') | set('\"\\'()[]{}:;,.!?-/\\\\|@#$%^&*_+=~`')\n",
        "\n",
        "# Light Arabic stopwords (lemmas)\n",
        "AR_STOP = set(\"\"\"\n",
        "ÙÙŠ Ù…Ù† Ø¹Ù„Ù‰ Ø¹Ù† Ø¥Ù„Ù‰ Ø­ØªÙ‰ Ù…Ù†Ø° Ø®Ù„Ø§Ù„ Ù„Ø¯Ù‰ Ù„ÙƒÙ„ Ù„Ù‡Ø§ Ù„Ù‡ Ù„Ù‡Ù… Ù‡Ùˆ Ù‡ÙŠ Ù‡Ù… Ù‡Ù† Ù†Ø­Ù† Ø§Ù†Ø§ Ø£Ù†Øª Ø§Ù†ØªÙŠ Ø§Ù†ØªÙ… Ù‡Ø°Ø§ Ù‡Ø°Ù‡ Ø°Ù„Ùƒ ØªÙ„Ùƒ Ù‡Ù†Ø§Ùƒ Ù‡Ù†Ø§ Ø­ÙŠØ« Ù„Ù…Ø§ Ù„Ø§ Ù…Ø§ ÙŠØ§ Ø£Ùˆ Ø£Ù… Ø«Ù… Ø¨Ù„ Ø¥Ù† Ø£Ù† ÙƒØ§Ù† ØªÙƒÙˆÙ† ÙŠÙƒÙˆÙ† ÙƒØ§Ù†Øª ÙƒØ§Ù†ÙˆØ§ ÙƒÙ† ÙƒÙ†Øª ÙƒÙ†Ø§ Ø¥Ø° Ø¥Ø°Ø§ Ø¥Ø°Ù† Ù‚Ø¯ Ù„Ù† Ù„Ù… Ø£Ù„Ø§ Ø§Ù„Ø§ Ø¥Ù„Ø§ ØºÙŠØ± Ø³ÙˆÙ‰ ÙƒÙ„Ø§ ÙƒÙ…Ø§ ÙƒÙŠÙ Ù…ØªÙ‰ Ø£ÙŠ Ø§Ù„Ø°ÙŠ Ø§Ù„ØªÙŠ Ø§Ù„Ø°ÙŠÙ† Ø§Ù„Ù„ÙˆØ§ØªÙŠ Ù‡Ù†Ø§Ù„Ùƒ Ù‡ÙƒØ°Ø§ ÙƒÙ„ Ø¬Ø¯Ø§ ÙÙ‚Ø· Ù…Ø«Ù„ Ø¨Ø¹Ø¶ Ø±Ø¨Ù…Ø§ Ø§ÙŠØ¶Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø£ÙŠØ¶Ø§Ù‹ Ø§ÙŠØ¶Ø§Ù‹ Ù„ÙŠØ³ Ù„ÙŠØ³ÙˆØ§ Ø¯ÙˆÙ† Ø¨Ø¯ÙˆÙ† Ù…Ø¹ Ø¨ÙŠÙ†\n",
        "\"\"\".split())\n",
        "\n",
        "# Minimal dialectâ†’MSA normalization for lemmas (extend as needed)\n",
        "DIALECT_MAP = {\n",
        "    \"ÙƒÙˆÙŠØ³\": \"Ø¬ÙŠØ¯\", \"ÙƒÙÙˆÙŽÙŠÙ‘ÙØ³\": \"Ø¬ÙŽÙŠÙ‘ÙØ¯\",\n",
        "    \"Ø§Ø²Ø§ÙŠ\": \"ÙƒÙŠÙ\", \"Ø§Ø²Ø§Ù‰\": \"ÙƒÙŠÙ\", \"Ø¥Ø²Ø§ÙŠ\": \"ÙƒÙŠÙ\",\n",
        "    \"Ø§Ø­Ù†Ø§\": \"Ù†Ø­Ù†\", \"Ø£Ø­Ù‘\": \"Ù†Ø­Ù†\",\n",
        "    \"Ø¹Ø§ÙŠØ²\": \"ÙŠØ±ÙŠØ¯\", \"Ø¹Ø§ÙŠØ²ÙŠÙ†\": \"Ù†Ø±ÙŠØ¯\",\n",
        "    \"Ø¨Ù†Ø¹Ù…Ù„\": \"Ù†ÙŽØ¹Ù’Ù…ÙŽÙ„\",\n",
        "    \"Ø¨Ù†Ø­Ø·Ù‡\": \"Ù†ÙŽØ¶ÙŽØ¹\",\n",
        "    \"Ø¨Ù†Ù†Ù‚Ø¹ÙˆÙ‡\": \"Ù†ÙŽÙ†Ù’Ù‚ÙŽØ¹ÙÙ‡Ù\",\n",
        "    \"ÙƒØ¯Ù‡\": \"Ù‡ÙƒØ°Ø§\",\n",
        "    \"ÙÙŠØ¹Ù†ÙŠ\": \"ÙŠØ¹Ù†ÙŠ\", \"ÙŠØ¹Ù†ÙŠ\": \"ÙŠØ¹Ù†ÙŠ\",\n",
        "    \"ÙƒØªÙŠØ±\": \"ÙƒØ«ÙŠØ±\",\n",
        "    \"Ø§Ù„Ù„ÙŠ\": \"Ø§Ù„Ø°ÙŠ\",\n",
        "    \"Ø¨ÙŠØ¨Ù‚Ù‰\": \"ÙŠÙƒÙˆÙ†\",\n",
        "    \"Ø¨ÙŠØ¬ÙŠ\": \"ÙŠØ£ØªÙŠ\",\n",
        "    \"Ø¨ÙŠØªØ±Ø³Ù…\": \"ÙŠÙØ±Ù’Ø³ÙŽÙ…\",\n",
        "    \"Ù…ÙÙŠØ´\": \"Ù„Ø§ ÙŠÙˆØ¬Ø¯\",\n",
        "    \"Ù…Ø´\": \"Ù„ÙŠØ³\",\n",
        "    \"Ø·Ø¨Ø¹Ø§\": \"Ø·Ø¨Ø¹Ø§Ù‹\",\n",
        "    \"Ø¨Ù†Ø¬ÙŠØ¨\": \"Ù†ÙŽØ¬Ù’Ù„ÙØ¨\",\n",
        "}\n",
        "\n",
        "def is_arabic_char(ch: str) -> bool:\n",
        "    code = ord(ch)\n",
        "    return ((0x0600 <= code <= 0x06FF) or\n",
        "            (0x0750 <= code <= 0x077F) or\n",
        "            (0x08A0 <= code <= 0x08FF))\n",
        "\n",
        "def is_mostly_arabic(token: str) -> bool:\n",
        "    if not token:\n",
        "        return False\n",
        "    letters = [c for c in token if c.isalpha()]\n",
        "    if not letters:\n",
        "        return False\n",
        "    ar = sum(1 for c in letters if is_arabic_char(c))\n",
        "    return ar / len(letters) >= 0.6\n",
        "\n",
        "def normalize_punct(s: str) -> str:\n",
        "    s = (s or \"\").replace('Ù€', '')\n",
        "    s = s.replace('â€œ','\"').replace('â€','\"').replace('â€™',\"'\").replace('â€˜',\"'\")\n",
        "    s = s.replace('Â«','\"').replace('Â»','\"')\n",
        "    # collapse repeated punctuation\n",
        "    s = re.sub(r'([:;,\\.\\-\\â€”\\â€“\\!ØŸ\\?\"])\\\\1{1,}', r'\\1', s)\n",
        "    return s\n",
        "\n",
        "def clean_tokens(tokens, lemmas):\n",
        "    \"\"\"Return (tokens_clean, lemmas_clean) with:\n",
        "       - punctuation normalization\n",
        "       - drop pure punct / single Latin\n",
        "       - prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "       - stopword trimming on lemmas\n",
        "       - small dialect normalization on lemmas\n",
        "       - collapse consecutive duplicates\n",
        "    \"\"\"\n",
        "    tokens = tokens or []\n",
        "    lemmas = lemmas or []\n",
        "    out_toks, out_lems = [], []\n",
        "\n",
        "    for t, l in zip(tokens, lemmas):\n",
        "        t = normalize_punct((t or \"\").strip())\n",
        "        l = normalize_punct((l or \"\").strip())\n",
        "\n",
        "        # empty?\n",
        "        if not t and not l:\n",
        "            continue\n",
        "\n",
        "        # drop pure punctuation or single Latin char\n",
        "        if t and (all((ch in AR_PUNCT or ch.isspace()) for ch in t) or re.fullmatch(r\"[A-Za-z]\", t)):\n",
        "            continue\n",
        "        if l and (all((ch in AR_PUNCT or ch.isspace()) for ch in l) or re.fullmatch(r\"[A-Za-z]\", l)):\n",
        "            l = \"\"  # force fallback\n",
        "\n",
        "        # prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "        if not l or not is_mostly_arabic(l):\n",
        "            l = t if is_mostly_arabic(t) else l\n",
        "\n",
        "        # stopword trimming (lemmas)\n",
        "        if l in AR_STOP:\n",
        "            # keep token if it looks meaningful Arabic\n",
        "            if t and t not in AR_STOP and is_mostly_arabic(t):\n",
        "                out_toks.append(t)\n",
        "                out_lems.append(t)\n",
        "            continue\n",
        "\n",
        "        # dialect normalization\n",
        "        l = DIALECT_MAP.get(l, l)\n",
        "\n",
        "        out_toks.append(t)\n",
        "        out_lems.append(l)\n",
        "\n",
        "    # collapse consecutive duplicate (token,lemma) pairs\n",
        "    dedup_toks, dedup_lems = [], []\n",
        "    prev = None\n",
        "    for t, l in zip(out_toks, out_lems):\n",
        "        pair = (t, l)\n",
        "        if pair != prev:\n",
        "            dedup_toks.append(t)\n",
        "            dedup_lems.append(l)\n",
        "        prev = pair\n",
        "    return dedup_toks, dedup_lems\n",
        "\n",
        "def jaccard(a, b):\n",
        "    sa, sb = set(a), set(b)\n",
        "    if not sa and not sb:\n",
        "        return 1.0\n",
        "    if not sa or not sb:\n",
        "        return 0.0\n",
        "    return len(sa & sb) / max(1, len(sa | sb))\n",
        "\n",
        "# ---------- Load inputs ----------\n",
        "assert os.path.exists(preprocessed_captions_path), f\"Missing: {preprocessed_captions_path}\"\n",
        "assert os.path.exists(preprocessed_transcript_path), f\"Missing: {preprocessed_transcript_path}\"\n",
        "\n",
        "with open(preprocessed_captions_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    captions = json.load(f)\n",
        "with open(preprocessed_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    transcript = json.load(f)\n",
        "\n",
        "# ---------- Clean captions (+ remove consecutive near-duplicates) ----------\n",
        "cleaned_caps = []\n",
        "removed_indices = []\n",
        "prev_lems = None\n",
        "\n",
        "for i, item in enumerate(captions):\n",
        "    toks = item.get(\"tokens\", [])\n",
        "    lems = item.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "\n",
        "    # skip if empty after cleaning\n",
        "    if not clems and not ctoks:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    # remove consecutive near-duplicates at lemma level\n",
        "    if prev_lems is not None and jaccard(prev_lems, clems) >= NEAR_DUP_JACCARD:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    cleaned_caps.append({\n",
        "        \"scene_id\": item.get(\"scene_id\"),\n",
        "        \"scene_time\": item.get(\"scene_time\"),\n",
        "        \"original\": item.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "    prev_lems = clems\n",
        "\n",
        "# ---------- Clean transcript ----------\n",
        "cleaned_tr = []\n",
        "for utt in transcript:\n",
        "    toks = utt.get(\"tokens\", [])\n",
        "    lems = utt.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "    cleaned_tr.append({\n",
        "        \"start\": utt.get(\"start\"),\n",
        "        \"end\": utt.get(\"end\"),\n",
        "        \"original\": utt.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "\n",
        "# ---------- Save outputs ----------\n",
        "os.makedirs(preprocessed_path, exist_ok=True)\n",
        "with open(clean_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_caps, f, ensure_ascii=False, indent=2)\n",
        "with open(clean_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_tr, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------- Summary CSV ----------\n",
        "def count(items, key):\n",
        "    return sum(len(x.get(key, [])) for x in items)\n",
        "\n",
        "summary_rows = [\n",
        "    {\n",
        "        \"dataset\": \"captions\",\n",
        "        \"items_before\": len(captions),\n",
        "        \"items_after\": len(cleaned_caps),\n",
        "        \"tokens_before\": count(captions, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_caps, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(captions, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_caps, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": len(removed_indices),\n",
        "    },\n",
        "    {\n",
        "        \"dataset\": \"transcript\",\n",
        "        \"items_before\": len(transcript),\n",
        "        \"items_after\": len(cleaned_tr),\n",
        "        \"tokens_before\": count(transcript, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_tr, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(transcript, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_tr, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": 0,\n",
        "    },\n",
        "]\n",
        "\n",
        "with open(summary_csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(summary_rows)\n",
        "\n",
        "print(\"âœ… Cleaning done.\")\n",
        "print(f\"â€¢ Clean captions  â†’ {clean_captions_path}\")\n",
        "print(f\"â€¢ Clean transcript â†’ {clean_transcript_path}\")\n",
        "print(f\"â€¢ Summary CSV      â†’ {summary_csv_path}\")\n"
      ],
      "metadata": {
        "id": "F2JznbEq2JP4"
      },
      "id": "F2JznbEq2JP4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}