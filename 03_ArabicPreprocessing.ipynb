{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/03_ArabicPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376a981b",
      "metadata": {
        "id": "376a981b"
      },
      "source": [
        "# Arabic Preprocessing with CAMeL Tools\n",
        "This notebook performs Arabic text preprocessing using CAMeL Tools, including normalization, lemmatization, and optional dialect detection. Designed for use before alignment or semantic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8441fd0b",
      "metadata": {
        "id": "8441fd0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de79610c-eaf6-41a5-c0e7-99024cf0261f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Requirement already satisfied: camel-tools in /usr/local/lib/python3.12/dist-packages (1.5.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.17.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from camel-tools) (5.5.2)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.16.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from camel-tools) (1.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.3.8)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers<4.44.0,>=4.0 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (4.43.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.32.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2.14.1)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.20.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from camel-tools) (4.67.1)\n",
            "Requirement already satisfied: muddler in /usr/local/lib/python3.12/dist-packages (from camel-tools) (0.1.3)\n",
            "Requirement already satisfied: camel-kenlm>=2025.4.8 in /usr/local/lib/python3.12/dist-packages (from camel-tools) (2025.4.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->camel-tools) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.6.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.19.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->camel-tools) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->camel-tools) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->camel-tools) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->camel-tools) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->camel-tools) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->camel-tools) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers<4.44.0,>=4.0->camel-tools) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n",
            "Usage:\n",
            "    camel_data (-i | --install) [-f | --force] <PACKAGE>\n",
            "    camel_data (-p | --post-install) <PACKAGE> <ARGS>...\n",
            "    camel_data (-l | --list)\n",
            "    camel_data (-u | --update)\n",
            "    camel_data (-v | --version)\n",
            "    camel_data (-h | --help)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ✅ Install compatible versions of NumPy and CAMeL Tools\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install camel-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the morphology DB for MSA\n",
        "!camel_data -i morphology-db-msa-r13\n",
        "\n",
        "# Download the MLE disambiguator for MSA\n",
        "!camel_data -i disambig-mle-calima-msa-r13"
      ],
      "metadata": {
        "id": "q-NIP7_Bw6cD",
        "outputId": "c82d7fcc-0fb7-4db0-aa30-ef38f53fd4a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q-NIP7_Bw6cD",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following packages will be installed: 'morphology-db-msa-r13'\n",
            "Downloading package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:02<00:00, 13.8MB/s]\n",
            "Extracting package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:00<00:00, 553MB/s]\n",
            "The following packages will be installed: 'disambig-mle-calima-msa-r13'\n",
            "Downloading package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:23<00:00, 3.84MB/s]\n",
            "Extracting package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:00<00:00, 563MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uwk3G4EZ-Wq",
        "outputId": "624c606c-2257-4bfb-e63d-d0ba7bec08f2"
      },
      "id": "-uwk3G4EZ-Wq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define base paths\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "preprocessed_path= os.path.join(base_path, \"Preprocessed\")\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video file not found: {video_path}\"\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "transcript_path = os.path.join(transcripts_path, f\"{video_name}_ar_with_timecodes.txt\")\n",
        "captions_json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "preprocessed_captions_path= os.path.join(preprocessed_path, f\"captions_{video_name}_ar.json\")\n",
        "preprocessed_transcript_path= os.path.join(preprocessed_path, f\"transcript_{video_name}_ar.json\")\n",
        "clean_captions_path= os.path.join(preprocessed_path, f\"clean_captions_{video_name}_ar.json\")\n",
        "clean_transcript_path= os.path.join(preprocessed_path, f\"clean_transcript_{video_name}_ar.json\")"
      ],
      "metadata": {
        "id": "A5xIEuotYZzf"
      },
      "id": "A5xIEuotYZzf",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "60040256",
      "metadata": {
        "id": "60040256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b075255-c127-4138-b550-18613f8ae329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 69 transcript segments from PaperMaking_ar_with_timecodes.txt.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 📄 Parse transcript file with timecodes\n",
        "import re\n",
        "\n",
        "def load_transcript(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    segments = []\n",
        "    pattern = re.compile(r\"\\[(\\d+\\.\\d+) - (\\d+\\.\\d+)\\]\\s+(.*)\")\n",
        "    for line in lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            start, end, text = match.groups()\n",
        "            segments.append({\n",
        "                \"start\": float(start),\n",
        "                \"end\": float(end),\n",
        "                \"text\": text.strip()\n",
        "            })\n",
        "    return segments\n",
        "\n",
        "assert os.path.exists(transcript_path), f\"Transcript file not found: {transcript_path}\"\n",
        "\n",
        "segments = load_transcript(transcript_path)\n",
        "print(f\"Loaded {len(segments)} transcript segments from {os.path.basename(transcript_path)}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "74bf09b8",
      "metadata": {
        "id": "74bf09b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade22d61-df07-439e-d28d-3e8dc4cfa586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 54 scene captions from PaperMaking.json.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 📄 Load caption JSON\n",
        "import json\n",
        "\n",
        "def load_captions(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    captions = []\n",
        "    for scene_id, meta in data.items():\n",
        "        captions.append({\n",
        "            \"scene_id\": scene_id,\n",
        "            \"scene_time\": meta[\"scene_time\"],\n",
        "            \"caption\": meta[\"arabic\"]\n",
        "        })\n",
        "    return captions\n",
        "assert os.path.exists(captions_json_path), f\"Transcript file not found: {captions_json_path}\"\n",
        "captions = load_captions(captions_json_path)\n",
        "print(f\"Loaded {len(captions)} scene captions from {os.path.basename(captions_json_path)}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "47197d02",
      "metadata": {
        "id": "47197d02"
      },
      "outputs": [],
      "source": [
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "disambig = MLEDisambiguator.pretrained()\n",
        "\n",
        "def preprocess(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    tokens = [t for t in simple_word_tokenize(text) if t.strip()]\n",
        "\n",
        "    results = disambig.disambiguate(tokens)\n",
        "\n",
        "    lemmas, pos_tags, diacs, glosses = [], [], [], []\n",
        "\n",
        "    for tok, res in zip(tokens, results):\n",
        "        if res.analyses:\n",
        "            top = res.analyses[0]               # ScoredAnalysis\n",
        "            a = top.analysis                    # dict of features\n",
        "            # Prefer lexeme; fall back gracefully\n",
        "            lemma = a.get('lex') or a.get('lemma') or a.get('stem') or a.get('diac') or tok\n",
        "            lemmas.append(lemma)\n",
        "            pos_tags.append(a.get('pos'))\n",
        "            diacs.append(a.get('diac'))\n",
        "            glosses.append(a.get('gloss'))\n",
        "        else:\n",
        "            lemmas.append(tok)\n",
        "            pos_tags.append(None)\n",
        "            diacs.append(None)\n",
        "            glosses.append(None)\n",
        "\n",
        "    return {\n",
        "        \"original\": text,\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmas\": lemmas,\n",
        "        \"lemmas_dediac\": [dediac_ar(l) if isinstance(l, str) else l for l in lemmas],\n",
        "        \"pos\": pos_tags,\n",
        "        \"diac\": diacs,\n",
        "        \"gloss\": glosses,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "575758c2",
      "metadata": {
        "id": "575758c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76e86ef-41eb-451a-87e5-9093aa9ee4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/transcript_PaperMaking_ar.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 🔁 Process all transcript segments\n",
        "processed_segments = []\n",
        "for seg in segments:\n",
        "    proc = preprocess(seg[\"text\"])\n",
        "    processed_segments.append({\n",
        "        \"start\": seg[\"start\"],\n",
        "        \"end\": seg[\"end\"],\n",
        "        \"original\": seg[\"text\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Saved: \"+ preprocessed_transcript_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "56c6d282",
      "metadata": {
        "id": "56c6d282",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44cd9c2-d45e-4817-a631-682e9800e4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/captions_PaperMaking_ar.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 🔁 Process all captions\n",
        "processed_captions = []\n",
        "for cap in captions:\n",
        "    proc = preprocess(cap[\"caption\"])\n",
        "    processed_captions.append({\n",
        "        \"scene_id\": cap[\"scene_id\"],\n",
        "        \"scene_time\": cap[\"scene_time\"],\n",
        "        \"original\": cap[\"caption\"],\n",
        "        \"tokens\": proc[\"tokens\"],\n",
        "        \"lemmas\": proc[\"lemmas\"]\n",
        "    })\n",
        "\n",
        "with open(preprocessed_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Saved: \"+ preprocessed_captions_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Clean preprocessed captions & transcript (alignment-ready) ----\n",
        "# Uses your already-defined variables:\n",
        "# base_path, videos_path, transcripts_path, captions_path, preprocessed_path,\n",
        "# video_filename, video_path, video_name,\n",
        "# transcript_path, captions_json_path,\n",
        "# preprocessed_captions_path, preprocessed_transcript_path,\n",
        "# clean_captions_path, clean_transcript_path\n",
        "\n",
        "import os, json, re, string, csv\n",
        "\n",
        "# Where to save the summary CSV\n",
        "summary_csv_path = os.path.join(preprocessed_path, f\"cleaning_summary_{video_name}.csv\")\n",
        "\n",
        "# ---------- Config ----------\n",
        "# (tweak if needed)\n",
        "NEAR_DUP_JACCARD = 0.90  # consecutive near-duplicate threshold (captions-only)\n",
        "\n",
        "# Arabic punctuation + ASCII\n",
        "AR_PUNCT = set('''ـ،؛؟…«»“”''') | set('\"\\'()[]{}:;,.!?-/\\\\|@#$%^&*_+=~`')\n",
        "\n",
        "# Light Arabic stopwords (lemmas)\n",
        "AR_STOP = set(\"\"\"\n",
        "في من على عن إلى حتى منذ خلال لدى لكل لها له لهم هو هي هم هن نحن انا أنت انتي انتم هذا هذه ذلك تلك هناك هنا حيث لما لا ما يا أو أم ثم بل إن أن كان تكون يكون كانت كانوا كن كنت كنا إذ إذا إذن قد لن لم ألا الا إلا غير سوى كلا كما كيف متى أي الذي التي الذين اللواتي هنالك هكذا كل جدا فقط مثل بعض ربما ايضا أيضًا أيضاً ايضاً ليس ليسوا دون بدون مع بين\n",
        "\"\"\".split())\n",
        "\n",
        "# Minimal dialect→MSA normalization for lemmas (extend as needed)\n",
        "DIALECT_MAP = {\n",
        "    \"كويس\": \"جيد\", \"كُوَيِّس\": \"جَيِّد\",\n",
        "    \"ازاي\": \"كيف\", \"ازاى\": \"كيف\", \"إزاي\": \"كيف\",\n",
        "    \"احنا\": \"نحن\", \"أحّ\": \"نحن\",\n",
        "    \"عايز\": \"يريد\", \"عايزين\": \"نريد\",\n",
        "    \"بنعمل\": \"نَعْمَل\",\n",
        "    \"بنحطه\": \"نَضَع\",\n",
        "    \"بننقعوه\": \"نَنْقَعُهُ\",\n",
        "    \"كده\": \"هكذا\",\n",
        "    \"فيعني\": \"يعني\", \"يعني\": \"يعني\",\n",
        "    \"كتير\": \"كثير\",\n",
        "    \"اللي\": \"الذي\",\n",
        "    \"بيبقى\": \"يكون\",\n",
        "    \"بيجي\": \"يأتي\",\n",
        "    \"بيترسم\": \"يُرْسَم\",\n",
        "    \"مفيش\": \"لا يوجد\",\n",
        "    \"مش\": \"ليس\",\n",
        "    \"طبعا\": \"طبعاً\",\n",
        "    \"بنجيب\": \"نَجْلِب\",\n",
        "}\n",
        "\n",
        "def is_arabic_char(ch: str) -> bool:\n",
        "    code = ord(ch)\n",
        "    return ((0x0600 <= code <= 0x06FF) or\n",
        "            (0x0750 <= code <= 0x077F) or\n",
        "            (0x08A0 <= code <= 0x08FF))\n",
        "\n",
        "def is_mostly_arabic(token: str) -> bool:\n",
        "    if not token:\n",
        "        return False\n",
        "    letters = [c for c in token if c.isalpha()]\n",
        "    if not letters:\n",
        "        return False\n",
        "    ar = sum(1 for c in letters if is_arabic_char(c))\n",
        "    return ar / len(letters) >= 0.6\n",
        "\n",
        "def normalize_punct(s: str) -> str:\n",
        "    s = (s or \"\").replace('ـ', '')\n",
        "    s = s.replace('“','\"').replace('”','\"').replace('’',\"'\").replace('‘',\"'\")\n",
        "    s = s.replace('«','\"').replace('»','\"')\n",
        "    # collapse repeated punctuation\n",
        "    s = re.sub(r'([:;,\\.\\-\\—\\–\\!؟\\?\"])\\\\1{1,}', r'\\1', s)\n",
        "    return s\n",
        "\n",
        "def clean_tokens(tokens, lemmas):\n",
        "    \"\"\"Return (tokens_clean, lemmas_clean) with:\n",
        "       - punctuation normalization\n",
        "       - drop pure punct / single Latin\n",
        "       - prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "       - stopword trimming on lemmas\n",
        "       - small dialect normalization on lemmas\n",
        "       - collapse consecutive duplicates\n",
        "    \"\"\"\n",
        "    tokens = tokens or []\n",
        "    lemmas = lemmas or []\n",
        "    out_toks, out_lems = [], []\n",
        "\n",
        "    for t, l in zip(tokens, lemmas):\n",
        "        t = normalize_punct((t or \"\").strip())\n",
        "        l = normalize_punct((l or \"\").strip())\n",
        "\n",
        "        # empty?\n",
        "        if not t and not l:\n",
        "            continue\n",
        "\n",
        "        # drop pure punctuation or single Latin char\n",
        "        if t and (all((ch in AR_PUNCT or ch.isspace()) for ch in t) or re.fullmatch(r\"[A-Za-z]\", t)):\n",
        "            continue\n",
        "        if l and (all((ch in AR_PUNCT or ch.isspace()) for ch in l) or re.fullmatch(r\"[A-Za-z]\", l)):\n",
        "            l = \"\"  # force fallback\n",
        "\n",
        "        # prefer lemma; fallback to token if lemma missing/non-Arabic\n",
        "        if not l or not is_mostly_arabic(l):\n",
        "            l = t if is_mostly_arabic(t) else l\n",
        "\n",
        "        # stopword trimming (lemmas)\n",
        "        if l in AR_STOP:\n",
        "            # keep token if it looks meaningful Arabic\n",
        "            if t and t not in AR_STOP and is_mostly_arabic(t):\n",
        "                out_toks.append(t)\n",
        "                out_lems.append(t)\n",
        "            continue\n",
        "\n",
        "        # dialect normalization\n",
        "        l = DIALECT_MAP.get(l, l)\n",
        "\n",
        "        out_toks.append(t)\n",
        "        out_lems.append(l)\n",
        "\n",
        "    # collapse consecutive duplicate (token,lemma) pairs\n",
        "    dedup_toks, dedup_lems = [], []\n",
        "    prev = None\n",
        "    for t, l in zip(out_toks, out_lems):\n",
        "        pair = (t, l)\n",
        "        if pair != prev:\n",
        "            dedup_toks.append(t)\n",
        "            dedup_lems.append(l)\n",
        "        prev = pair\n",
        "    return dedup_toks, dedup_lems\n",
        "\n",
        "def jaccard(a, b):\n",
        "    sa, sb = set(a), set(b)\n",
        "    if not sa and not sb:\n",
        "        return 1.0\n",
        "    if not sa or not sb:\n",
        "        return 0.0\n",
        "    return len(sa & sb) / max(1, len(sa | sb))\n",
        "\n",
        "# ---------- Load inputs ----------\n",
        "assert os.path.exists(preprocessed_captions_path), f\"Missing: {preprocessed_captions_path}\"\n",
        "assert os.path.exists(preprocessed_transcript_path), f\"Missing: {preprocessed_transcript_path}\"\n",
        "\n",
        "with open(preprocessed_captions_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    captions = json.load(f)\n",
        "with open(preprocessed_transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    transcript = json.load(f)\n",
        "\n",
        "# ---------- Clean captions (+ remove consecutive near-duplicates) ----------\n",
        "cleaned_caps = []\n",
        "removed_indices = []\n",
        "prev_lems = None\n",
        "\n",
        "for i, item in enumerate(captions):\n",
        "    toks = item.get(\"tokens\", [])\n",
        "    lems = item.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "\n",
        "    # skip if empty after cleaning\n",
        "    if not clems and not ctoks:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    # remove consecutive near-duplicates at lemma level\n",
        "    if prev_lems is not None and jaccard(prev_lems, clems) >= NEAR_DUP_JACCARD:\n",
        "        removed_indices.append(i)\n",
        "        continue\n",
        "\n",
        "    cleaned_caps.append({\n",
        "        \"scene_id\": item.get(\"scene_id\"),\n",
        "        \"scene_time\": item.get(\"scene_time\"),\n",
        "        \"original\": item.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "    prev_lems = clems\n",
        "\n",
        "# ---------- Clean transcript ----------\n",
        "cleaned_tr = []\n",
        "for utt in transcript:\n",
        "    toks = utt.get(\"tokens\", [])\n",
        "    lems = utt.get(\"lemmas\", [])\n",
        "    ctoks, clems = clean_tokens(toks, lems)\n",
        "    cleaned_tr.append({\n",
        "        \"start\": utt.get(\"start\"),\n",
        "        \"end\": utt.get(\"end\"),\n",
        "        \"original\": utt.get(\"original\"),\n",
        "        \"tokens_clean\": ctoks,\n",
        "        \"lemmas_clean\": clems,\n",
        "    })\n",
        "\n",
        "# ---------- Save outputs ----------\n",
        "os.makedirs(preprocessed_path, exist_ok=True)\n",
        "with open(clean_captions_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_caps, f, ensure_ascii=False, indent=2)\n",
        "with open(clean_transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_tr, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------- Summary CSV ----------\n",
        "def count(items, key):\n",
        "    return sum(len(x.get(key, [])) for x in items)\n",
        "\n",
        "summary_rows = [\n",
        "    {\n",
        "        \"dataset\": \"captions\",\n",
        "        \"items_before\": len(captions),\n",
        "        \"items_after\": len(cleaned_caps),\n",
        "        \"tokens_before\": count(captions, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_caps, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(captions, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_caps, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": len(removed_indices),\n",
        "    },\n",
        "    {\n",
        "        \"dataset\": \"transcript\",\n",
        "        \"items_before\": len(transcript),\n",
        "        \"items_after\": len(cleaned_tr),\n",
        "        \"tokens_before\": count(transcript, \"tokens\"),\n",
        "        \"tokens_after\": count(cleaned_tr, \"tokens_clean\"),\n",
        "        \"lemmas_before\": count(transcript, \"lemmas\"),\n",
        "        \"lemmas_after\": count(cleaned_tr, \"lemmas_clean\"),\n",
        "        \"duplicates_removed\": 0,\n",
        "    },\n",
        "]\n",
        "\n",
        "with open(summary_csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(summary_rows)\n",
        "\n",
        "print(\"✅ Cleaning done.\")\n",
        "print(f\"• Clean captions  → {clean_captions_path}\")\n",
        "print(f\"• Clean transcript → {clean_transcript_path}\")\n",
        "print(f\"• Summary CSV      → {summary_csv_path}\")\n"
      ],
      "metadata": {
        "id": "F2JznbEq2JP4",
        "outputId": "bfc26e04-2cd5-46c3-818a-9d7060bea960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "F2JznbEq2JP4",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaning done.\n",
            "• Clean captions  → /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/clean_captions_PaperMaking_ar.json\n",
            "• Clean transcript → /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/clean_transcript_PaperMaking_ar.json\n",
            "• Summary CSV      → /content/drive/MyDrive/ArabicVideoSummariser/Preprocessed/cleaning_summary_PaperMaking.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}