{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/02_scenedetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import scenedetect\n",
        "except Exception:\n",
        "    !pip -q install \"scenedetect>=0.6,<0.7\"\n",
        "\n",
        "try:\n",
        "    import sentence_transformers\n",
        "except Exception:\n",
        "    !pip -q install \"sentence-transformers>=2.2,<2.7\"\n"
      ],
      "metadata": {
        "id": "vQ_MsxKQqegW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxWDlYDHOhNg"
      },
      "outputs": [],
      "source": [
        "# --- Imports\n",
        "import os, json, random, re\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# --- Reproducibility (deterministic decoding across runs/devices)\n",
        "random.seed(0); np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "acfN6m07pO12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Paths & params\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "keyframes_path = os.path.join(base_path, \"keyframes\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "\n",
        "os.makedirs(captions_path, exist_ok=True)\n",
        "os.makedirs(keyframes_path, exist_ok=True)\n",
        "\n",
        "# Read params.json (must contain: {\"video_file\": \"...\"} )\n",
        "param_path = os.path.join(base_path, \"params.json\")\n",
        "with open(param_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "#video_filename = params.get(\"video_file\")\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video not found: {video_path}\"\n",
        "\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "keyframe_dir = os.path.join(keyframes_path, video_name)\n",
        "os.makedirs(keyframe_dir, exist_ok=True)\n",
        "\n",
        "captions_json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "\n",
        "print(f\"üé• Processing video file: {video_filename}\")"
      ],
      "metadata": {
        "id": "VtP1iRGEnA1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"üñ•Ô∏è Vision/Text device:\", device)\n",
        "\n",
        "# BLIP-2 (EN captioning) ‚Äî stays on GPU via device_map\n",
        "caption_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", use_fast=False)\n",
        "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n",
        "\n",
        "# --- Robust NLLB lang-id resolver (works across tokenizer variants)\n",
        "def resolve_lang_id(tokenizer, lang_code: str) -> int:\n",
        "    if hasattr(tokenizer, \"lang_code_to_id\"):\n",
        "        return tokenizer.lang_code_to_id[lang_code]\n",
        "    if hasattr(tokenizer, \"get_lang_id\"):\n",
        "        return tokenizer.get_lang_id(lang_code)\n",
        "    if hasattr(tokenizer, \"lang_code_to_token\"):\n",
        "        tok = tokenizer.lang_code_to_token[lang_code]\n",
        "        tid = tokenizer.convert_tokens_to_ids(tok)\n",
        "        if tid != tokenizer.unk_token_id:\n",
        "            return tid\n",
        "    for cand in (f\"__{lang_code}__\", f\"<<{lang_code}>>\", lang_code):\n",
        "        tid = tokenizer.convert_tokens_to_ids(cand)\n",
        "        if tid != tokenizer.unk_token_id:\n",
        "            return tid\n",
        "    raise RuntimeError(f\"Could not resolve language id for: {lang_code}\")\n",
        "\n",
        "# NLLB-200 (EN -> AR) ‚Äî keep on CPU to avoid GPU OOM\n",
        "TRANS_DEVICE = \"cpu\"\n",
        "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "translator_tokenizer = AutoTokenizer.from_pretrained(nllb_model_name, src_lang=\"eng_Latn\")\n",
        "translator_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name).to(TRANS_DEVICE)\n",
        "forced_bos_token_id = resolve_lang_id(translator_tokenizer, \"arb_Arab\")\n",
        "print(\"‚úÖ NLLB-200 loaded on CPU. forced_bos_token_id:\", forced_bos_token_id)\n",
        "\n",
        "# CLIP for image<->EN text grounding ‚Äî keep on GPU if available\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "print(\"‚úÖ Models ready.\")\n"
      ],
      "metadata": {
        "id": "KL9honqono7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLFyG2Y7ULGF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "timecode_re = re.compile(r\"\\[(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)\\]\\s+(.*)\")\n",
        "\n",
        "def get_transcript_context_ar(scene_time: float, window_s: float = 10.0) -> str:\n",
        "    \"\"\"Return Arabic transcript text around scene_time ¬± window_s (concatenated).\"\"\"\n",
        "    if not os.path.exists(transcript_tc_path):\n",
        "        return \"\"\n",
        "    ctx = []\n",
        "    start_win, end_win = scene_time - window_s, scene_time + window_s\n",
        "    with open(transcript_tc_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            m = timecode_re.match(line.strip())\n",
        "            if not m:\n",
        "                continue\n",
        "            s, e, text = float(m.group(1)), float(m.group(2)), m.group(3)\n",
        "            if not (e < start_win or s > end_win):\n",
        "                ctx.append(text)\n",
        "    return \" \".join(ctx)[:1500]  # keep short for embeddings\n",
        "\n",
        "def grab_frames_around(cap: cv2.VideoCapture, fps: float, base_time_s: float, offsets=(0.0, 0.2, 0.4)):\n",
        "    \"\"\"Return list of PIL images at base_time_s + offsets (seconds).\"\"\"\n",
        "    images = []\n",
        "    for off in offsets:\n",
        "        t = max(0.0, base_time_s + off)\n",
        "        frame_idx = int(t * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ok, fr = cap.read()\n",
        "        if ok:\n",
        "            images.append(Image.fromarray(cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)))\n",
        "    return images\n",
        "\n",
        "@torch.no_grad()\n",
        "def blip2_nbest_en(images, beams=5, returns=3):\n",
        "    \"\"\"Generate N-best English captions across multiple images (deterministic).\"\"\"\n",
        "    cands = []\n",
        "    for img in images:\n",
        "        inputs = caption_processor(images=[img], return_tensors=\"pt\", padding=True)\\\n",
        "                 .to(device, torch.float16 if device==\"cuda\" else torch.float32)\n",
        "        ids = caption_model.generate(\n",
        "            **inputs,\n",
        "            do_sample=False,\n",
        "            num_beams=beams,\n",
        "            num_return_sequences=returns,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=1.05,\n",
        "            no_repeat_ngram_size=3,\n",
        "            max_new_tokens=50,\n",
        "        )\n",
        "        for seq in ids:\n",
        "            txt = caption_processor.decode(seq, skip_special_tokens=True).strip()\n",
        "            cands.append(txt)\n",
        "    seen, uniq = set(), []\n",
        "    for t in cands:\n",
        "        if t not in seen:\n",
        "            seen.add(t); uniq.append(t)\n",
        "    return uniq\n",
        "\n",
        "@torch.no_grad()\n",
        "def mt_nbest_ar(en_list, beams=4, returns=2):\n",
        "    \"\"\"Translate EN candidates to Arabic using NLLB on CPU; return unique (en, ar) pairs.\"\"\"\n",
        "    pairs, seen = [], set()\n",
        "    for en in en_list:\n",
        "        ti = translator_tokenizer([en], return_tensors=\"pt\", padding=True).to(TRANS_DEVICE)\n",
        "        out = translator_model.generate(\n",
        "            **ti,\n",
        "            do_sample=False,                 # deterministic\n",
        "            num_beams=beams,\n",
        "            num_return_sequences=returns,    # smaller to keep CPU fast\n",
        "            max_new_tokens=64,\n",
        "            no_repeat_ngram_size=3,\n",
        "            repetition_penalty=1.15,\n",
        "            forced_bos_token_id=forced_bos_token_id,  # Arabic output\n",
        "        )\n",
        "        for seq in out:\n",
        "            ar = translator_tokenizer.decode(seq, skip_special_tokens=True).strip()\n",
        "            key = (en, ar)\n",
        "            if key not in seen:\n",
        "                seen.add(key); pairs.append(key)\n",
        "    return pairs\n",
        "\n",
        "@torch.no_grad()\n",
        "def clip_image_text_score(image_pil: Image.Image, text_en: str) -> float:\n",
        "    \"\"\"Cosine similarity between image and EN caption using CLIP.\"\"\"\n",
        "    inputs = clip_processor(text=[text_en], images=image_pil, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outs = clip_model(**inputs)\n",
        "    img = outs.image_embeds / outs.image_embeds.norm(dim=-1, keepdim=True)\n",
        "    txt = outs.text_embeds / outs.text_embeds.norm(dim=-1, keepdim=True)\n",
        "    return float((img @ txt.T).squeeze().detach().cpu())\n",
        "\n",
        "@torch.no_grad()\n",
        "def sbert_sim_ar(ar_caption: str, ar_context: str) -> float:\n",
        "    \"\"\"Cosine similarity between AR caption and AR transcript context.\"\"\"\n",
        "    if not ar_context:\n",
        "        return 0.0\n",
        "    embs = sbert.encode([ar_caption, ar_context], convert_to_tensor=True, normalize_embeddings=True)\n",
        "    return float(util.cos_sim(embs[0], embs[1]).item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene detection (PySceneDetect)\n",
        "from scenedetect import open_video, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "\n",
        "# Detect scenes\n",
        "scene_manager = SceneManager()\n",
        "scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
        "video = open_video(video_path)\n",
        "scene_manager.detect_scenes(video)\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "# VideoCapture for frame access\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "\n",
        "print(f\"üé¨ Detected {len(scene_list)} scenes, FPS={fps:.2f}\")\n"
      ],
      "metadata": {
        "id": "V3BSgkx7orCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== captioning & Visual-grounding re-ranking =====\n",
        "import os, json, cv2, numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Container for results\n",
        "captions = {}\n",
        "\n",
        "# Hyperparameters (keep consistent with main run for fair comparison)\n",
        "frame_offsets = (0.0,  0.2,  0.4)   # multi-frame sampling around scene start\n",
        "beams_caption, returns_caption = 5, 3\n",
        "beams_mt, returns_mt = 4, 2       # translate best EN for parity (not used for scoring)\n",
        "save_keyframes = True\n",
        "\n",
        "for i, (start, _) in enumerate(scene_list):\n",
        "    scene_t = start.get_seconds()\n",
        "\n",
        "    # 1) Multi-frame sampling around the same scene boundry\n",
        "    images = grab_frames_around(cap, fps, scene_t, offsets=frame_offsets)\n",
        "    if not images:\n",
        "        print(f\"‚ö†Ô∏è Scene {i:03} has no decodable frames; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Save representative keyframe (first sampled image)\n",
        "    frame_name = f\"scene_{i:03}.jpg\"\n",
        "    if save_keyframes:\n",
        "        frame_path = os.path.join(keyframe_dir, frame_name)\n",
        "        cv2.imwrite(frame_path, cv2.cvtColor(np.array(images[0]), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # 2) English N-best captions across frames with deterministic decoding\n",
        "    en_cands = blip2_nbest_en(images, beams=beams_caption, returns=returns_caption)\n",
        "    if not en_cands:\n",
        "        print(f\"‚ö†Ô∏è No EN candidates for scene {i:03}; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 3) Visual grounding: average CLIP score across sampled frames\n",
        "    en_vis = {en: float(np.mean([clip_image_text_score(img, en) for img in images])) for en in en_cands}\n",
        "\n",
        "    # 4) Pick best English by visual score\n",
        "    best_en = max(en_vis.keys(), key=lambda t: en_vis[t])\n",
        "    best_vscore = en_vis[best_en]\n",
        "\n",
        "    # 5) Translate best EN (N-best) for Arabic output\n",
        "    pair_cands = mt_nbest_ar([best_en], beams=beams_mt, returns=returns_mt)\n",
        "    if pair_cands:\n",
        "        # Prefer the first Arabic for the chosen EN\n",
        "        cand_for_best_en = [ar for (en, ar) in pair_cands if en == best_en]\n",
        "        best_ar = cand_for_best_en[0] if cand_for_best_en else pair_cands[0][1]\n",
        "    else:\n",
        "        best_ar = \"\"\n",
        "\n",
        "    # 6) Save visually validated result\n",
        "    captions[frame_name] = {\n",
        "        \"scene_time\": round(scene_t, 2),\n",
        "        \"english\": best_en,\n",
        "        \"arabic\": best_ar,\n",
        "        \"scores\": {\"visual\": round(best_vscore, 4)},\n",
        "    }\n",
        "\n",
        "    print(f\"‚úì {frame_name} @ {scene_t:.2f}s | EN*: {best_en} | AR: {best_ar} | V={best_vscore:.3f}\")\n",
        "\n",
        "cap.release()\n",
        "\n",
        "with open(captions_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Captions saved to: {captions_json_path}\")\n",
        "print(f\"üñºÔ∏è Keyframes dir: {keyframe_dir}\")"
      ],
      "metadata": {
        "id": "Uitm6R4NLqpo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (env_scene)",
      "language": "python",
      "name": "env_scene"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}