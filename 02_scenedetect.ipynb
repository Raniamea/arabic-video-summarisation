{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raniamea/arabic-video-summarisation/blob/main/02_scenedetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KxWDlYDHOhNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "3fa8390c-89e1-482a-b795-ec495cfc5cef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scenedetect'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-41569775.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscenedetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSceneManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscenedetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContentDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scenedetect'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# --- Imports\n",
        "import os, json, random, re\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "from scenedetect import open_video, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "\n",
        "# --- Reproducibility (deterministic decoding across runs/devices)\n",
        "random.seed(0); np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Unmount first\n",
        "!fusermount -u /content/drive || echo \"Already unmounted\"\n",
        "\n",
        "# Delete the mount folder entirely\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acfN6m07pO12",
        "outputId": "ef5576c5-652a-40a7-d9ea-da85c933b0b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "Already unmounted\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Paths & params\n",
        "base_path = \"/content/drive/MyDrive/ArabicVideoSummariser\"\n",
        "videos_path = os.path.join(base_path, \"videos\")\n",
        "captions_path = os.path.join(base_path, \"captions\")\n",
        "keyframes_path = os.path.join(base_path, \"keyframes\")\n",
        "transcripts_path = os.path.join(base_path, \"transcripts\")\n",
        "\n",
        "os.makedirs(captions_path, exist_ok=True)\n",
        "os.makedirs(keyframes_path, exist_ok=True)\n",
        "\n",
        "# Read params.json (must contain: {\"video_file\": \"...\"} )\n",
        "param_path = os.path.join(base_path, \"params.json\")\n",
        "with open(param_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "#video_filename = params.get(\"video_file\")\n",
        "video_filename=\"PaperMaking.mp4\"\n",
        "assert video_filename, \"params.json must include 'video_file'.\"\n",
        "video_path = os.path.join(videos_path, video_filename)\n",
        "assert os.path.exists(video_path), f\"Video not found: {video_path}\"\n",
        "\n",
        "video_name = os.path.splitext(video_filename)[0]\n",
        "keyframe_dir = os.path.join(keyframes_path, video_name)\n",
        "os.makedirs(keyframe_dir, exist_ok=True)\n",
        "\n",
        "captions_json_path = os.path.join(captions_path, f\"{video_name}.json\")\n",
        "# Arabic transcript with timecodes produced by your ASR stage:\n",
        "transcript_tc_path = os.path.join(transcripts_path, f\"{video_name}_ar_with_timecodes.txt\")\n",
        "\n",
        "print(f\"🎥 Processing video file: {video_filename}\")"
      ],
      "metadata": {
        "id": "VtP1iRGEnA1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models (device, BLIP-2, Marian MT, CLIP, mSBERT)\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"🖥️ Device:\", device)\n",
        "\n",
        "# BLIP-2 (EN captioning)\n",
        "caption_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", use_fast=False)\n",
        "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Marian MT (EN -> AR)\n",
        "translator_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "translator_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\").to(device)\n",
        "\n",
        "# CLIP for image<->EN text grounding\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "# Multilingual SBERT for AR caption <-> AR transcript similarity\n",
        "sbert = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=device)\n",
        "\n",
        "print(\"✅ Models loaded.\")\n"
      ],
      "metadata": {
        "id": "KL9honqono7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLFyG2Y7ULGF"
      },
      "outputs": [],
      "source": [
        "#Helper functions (frames, generation, scoring, transcript context)\n",
        "timecode_re = re.compile(r\"\\[(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)\\]\\s+(.*)\")\n",
        "\n",
        "def get_transcript_context_ar(scene_time: float, window_s: float = 10.0) -> str:\n",
        "    \"\"\"Return Arabic transcript text around scene_time ± window_s (concatenated).\"\"\"\n",
        "    if not os.path.exists(transcript_tc_path):\n",
        "        return \"\"\n",
        "    ctx = []\n",
        "    start_win, end_win = scene_time - window_s, scene_time + window_s\n",
        "    with open(transcript_tc_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            m = timecode_re.match(line.strip())\n",
        "            if not m:\n",
        "                continue\n",
        "            s, e, text = float(m.group(1)), float(m.group(2)), m.group(3)\n",
        "            if not (e < start_win or s > end_win):\n",
        "                ctx.append(text)\n",
        "    return \" \".join(ctx)[:1500]  # keep short for embeddings\n",
        "\n",
        "def grab_frames_around(cap: cv2.VideoCapture, fps: float, base_time_s: float, offsets=(0.0, 0.2, 0.4)):\n",
        "    \"\"\"Return list of PIL images at base_time_s + offsets (seconds).\"\"\"\n",
        "    images = []\n",
        "    for off in offsets:\n",
        "        t = max(0.0, base_time_s + off)\n",
        "        frame_idx = int(t * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ok, fr = cap.read()\n",
        "        if ok:\n",
        "            images.append(Image.fromarray(cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)))\n",
        "    return images\n",
        "\n",
        "@torch.no_grad()\n",
        "def blip2_nbest_en(images, beams=5, returns=3):\n",
        "    \"\"\"Generate N-best English captions across multiple images.\"\"\"\n",
        "    cands = []\n",
        "    for img in images:\n",
        "        inputs = caption_processor(images=[img], return_tensors=\"pt\", padding=True)\\\n",
        "                 .to(device, torch.float16 if device==\"cuda\" else torch.float32)\n",
        "        ids = caption_model.generate(\n",
        "            **inputs,\n",
        "            do_sample=False,                # deterministic\n",
        "            num_beams=beams,\n",
        "            num_return_sequences=returns,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=1.05,\n",
        "            max_new_tokens=50,\n",
        "        )\n",
        "        for seq in ids:\n",
        "            txt = caption_processor.decode(seq, skip_special_tokens=True).strip()\n",
        "            cands.append(txt)\n",
        "    # de-duplicate while preserving order\n",
        "    seen, uniq = set(), []\n",
        "    for t in cands:\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            uniq.append(t)\n",
        "    return uniq\n",
        "\n",
        "@torch.no_grad()\n",
        "def mt_nbest_ar(en_list, beams=5, returns=3):\n",
        "    \"\"\"Translate each EN candidate to K-best AR; return unique (en, ar) pairs.\"\"\"\n",
        "    pairs, seen = [], set()\n",
        "    for en in en_list:\n",
        "        ti = translator_tokenizer([en], return_tensors=\"pt\", padding=True).to(device)\n",
        "        out = translator_model.generate(\n",
        "            **ti,\n",
        "            do_sample=False,               # deterministic\n",
        "            num_beams=beams,\n",
        "            num_return_sequences=returns,\n",
        "            max_new_tokens=80,\n",
        "        )\n",
        "        for seq in out:\n",
        "            ar = translator_tokenizer.decode(seq, skip_special_tokens=True).strip()\n",
        "            key = (en, ar)\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                pairs.append(key)\n",
        "    return pairs\n",
        "\n",
        "@torch.no_grad()\n",
        "def clip_image_text_score(image_pil: Image.Image, text_en: str) -> float:\n",
        "    \"\"\"Cosine similarity between image and EN caption using CLIP.\"\"\"\n",
        "    inputs = clip_processor(text=[text_en], images=image_pil, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outs = clip_model(**inputs)\n",
        "    img = outs.image_embeds / outs.image_embeds.norm(dim=-1, keepdim=True)\n",
        "    txt = outs.text_embeds / outs.text_embeds.norm(dim=-1, keepdim=True)\n",
        "    return float((img @ txt.T).squeeze().detach().cpu())\n",
        "\n",
        "@torch.no_grad()\n",
        "def sbert_sim_ar(ar_caption: str, ar_context: str) -> float:\n",
        "    \"\"\"Cosine similarity between AR caption and AR transcript context.\"\"\"\n",
        "    if not ar_context:\n",
        "        return 0.0\n",
        "    embs = sbert.encode([ar_caption, ar_context], convert_to_tensor=True, normalize_embeddings=True)\n",
        "    return float(util.cos_sim(embs[0], embs[1]).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene detection (PySceneDetect) & video prep\n",
        "# Detect scenes\n",
        "scene_manager = SceneManager()\n",
        "scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
        "\n",
        "video = open_video(video_path)\n",
        "scene_manager.detect_scenes(video)\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "# VideoCapture for frame access\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "\n",
        "print(f\"🎬 Detected {len(scene_list)} scenes, FPS={fps:.2f}\")\n"
      ],
      "metadata": {
        "id": "V3BSgkx7orCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Caption each scene (multi-frame, N-best, re-rank), save JSON + keyframes\n",
        "\n",
        "# Weights for final scoring: visual grounding (CLIP) vs transcript consistency (SBERT)\n",
        "alpha, beta = 0.5, 0.5\n",
        "\n",
        "captions = {}\n",
        "\n",
        "for i, (start, _) in enumerate(scene_list):\n",
        "    scene_t = start.get_seconds()\n",
        "\n",
        "    # 1) Multi-frame sampling around boundary\n",
        "    images = grab_frames_around(cap, fps, scene_t, offsets=(0.0, 0.2, 0.4))\n",
        "    if not images:\n",
        "        print(f\"⚠️ Scene {i:03} has no decodable frames; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Save representative keyframe (first sampled image)\n",
        "    frame_name = f\"scene_{i:03}.jpg\"\n",
        "    frame_path = os.path.join(keyframe_dir, frame_name)\n",
        "    cv2.imwrite(frame_path, cv2.cvtColor(np.array(images[0]), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # 2) English N-best captions (BLIP-2 across frames)\n",
        "    en_cands = blip2_nbest_en(images, beams=5, returns=3)\n",
        "    if not en_cands:\n",
        "        print(f\"⚠️ No EN candidates for scene {i:03}; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 3) Arabic N-best translations (Marian)\n",
        "    pair_cands = mt_nbest_ar(en_cands, beams=5, returns=3)\n",
        "    if not pair_cands:\n",
        "        print(f\"⚠️ No AR candidates for scene {i:03}; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 4) Scores\n",
        "    # Visual score = average CLIP score over sampled frames for each EN candidate\n",
        "    en_vis = {}\n",
        "    for en in set([p[0] for p in pair_cands]):\n",
        "        en_vis[en] = float(np.mean([clip_image_text_score(img, en) for img in images]))\n",
        "\n",
        "    # Transcript context around scene time (Arabic)\n",
        "    ar_ctx = get_transcript_context_ar(scene_t)\n",
        "\n",
        "    # 5) Joint re-ranking\n",
        "    best, best_score = None, -1e9\n",
        "    for en, ar in pair_cands:\n",
        "        vscore = en_vis[en]\n",
        "        tscore = sbert_sim_ar(ar, ar_ctx)\n",
        "        score = alpha * vscore + beta * tscore\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best = {\"english\": en, \"arabic\": ar, \"v\": vscore, \"t\": tscore, \"score\": score}\n",
        "\n",
        "    # 6) Save best candidate\n",
        "    captions[frame_name] = {\n",
        "        \"scene_time\": round(scene_t, 2),\n",
        "        \"english\": best[\"english\"],\n",
        "        \"arabic\": best[\"arabic\"],\n",
        "        \"scores\": {\"visual\": round(best[\"v\"], 4), \"transcript\": round(best[\"t\"], 4)}\n",
        "    }\n",
        "\n",
        "    print(f\"✓ {frame_name} @ {scene_t:.2f}s | EN*: {best['english']} | AR*: {best['arabic']} \"\n",
        "          f\"| V={best['v']:.3f} T={best['t']:.3f}\")\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save JSON\n",
        "with open(captions_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(captions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Captions saved to: {captions_json_path}\")\n",
        "print(f\"🖼️ Keyframes dir: {keyframe_dir}\")\n"
      ],
      "metadata": {
        "id": "4bz1ymOWo5Gl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (env_scene)",
      "language": "python",
      "name": "env_scene"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}