# -*- coding: utf-8 -*-
"""05_summarise_UP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C18hatLh7yMWTo83pbvq5wo0QYg1ML6r
"""

# ---- Clean up warning ----
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import os, json, re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

BASE_PATH = "/content/drive/MyDrive/ArabicVideoSummariser"
MODEL_PATH = os.path.join(BASE_PATH,"models/AraBART-finetuned-ar_finetuned_20251110_1546")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)


params_path = os.path.join(BASE_PATH, "params.json")

with open(params_path, "r", encoding="utf-8") as f:
    params = json.load(f)

video_filename = params.get("video_file")
assert video_filename, "params.json must include 'video_file'."

video_name  = os.path.splitext(video_filename)[0]
validated_path   = os.path.join(BASE_PATH, "Validated")
summaries_path   = os.path.join(BASE_PATH, "summaries")

Validated_File=os.path.join(validated_path, f"{video_name}_Validated.txt")
Summary_File = os.path.join(summaries_path, f"{video_name}_Summary1.txt")

# ============================================================
# Arabic Summarization
# ============================================================
import os, re, torch
from sentence_transformers import SentenceTransformer, util

def summarize_file(
    input_path: str,
    output_path: str,
    input_max_len: int = 1024,
    max_new_tokens: int = 400,
    min_new_tokens: int = 80,
    num_beams: int = 4,
    no_repeat_ngram_size: int = 3,
    length_penalty: float = 1.0,
    repetition_penalty: float = 1.2,
):
    # Safety checks
    assert os.path.exists(input_path), f"Input file not found: {input_path}"

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device).eval()

    # Read input
    with open(input_path, "r", encoding="utf-8") as f:
        full_text = f.read().strip()

    if not full_text:
        raise ValueError(f"Input file is empty: {input_path}")

    # Tokenize (truncate if needed)
    inputs = tokenizer(
        full_text,
        return_tensors="pt",
        truncation=True,
        max_length=input_max_len
    ).to(device)

    # Generate summary
    with torch.no_grad():
        out_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            min_new_tokens=min_new_tokens,
            num_beams=num_beams,
            no_repeat_ngram_size=no_repeat_ngram_size,
            length_penalty=length_penalty,
            repetition_penalty=repetition_penalty,
            early_stopping=True
        )

    summary = tokenizer.decode(out_ids[0], skip_special_tokens=True).strip()

    # Save
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(summary)
    print(summary)
    print(f"\n Summary saved to: {output_path}")

# ============================
# Summarize Validated Fused Score Similarity transcript
# ============================
summarize_file(
    input_path=Validated_File,
    output_path=Summary_File,
    input_max_len=1024,
    max_new_tokens=400,
    min_new_tokens=80,
    num_beams=4,
)