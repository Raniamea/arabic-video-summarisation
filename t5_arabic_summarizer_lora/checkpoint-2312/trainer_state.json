{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.986183074265976,
  "eval_steps": 500,
  "global_step": 2312,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.17271157167530224,
      "grad_norm": 0.2659686505794525,
      "learning_rate": 1.0775862068965516e-05,
      "loss": 4.5856,
      "step": 50
    },
    {
      "epoch": 0.3454231433506045,
      "grad_norm": 0.26689261198043823,
      "learning_rate": 2.1551724137931033e-05,
      "loss": 4.6381,
      "step": 100
    },
    {
      "epoch": 0.5181347150259067,
      "grad_norm": 0.3229561448097229,
      "learning_rate": 3.232758620689655e-05,
      "loss": 4.5866,
      "step": 150
    },
    {
      "epoch": 0.690846286701209,
      "grad_norm": 0.30020204186439514,
      "learning_rate": 4.3103448275862066e-05,
      "loss": 4.564,
      "step": 200
    },
    {
      "epoch": 0.8635578583765112,
      "grad_norm": 0.2956536114215851,
      "learning_rate": 4.95673076923077e-05,
      "loss": 4.5779,
      "step": 250
    },
    {
      "epoch": 1.0362694300518134,
      "grad_norm": 0.28415292501449585,
      "learning_rate": 4.836538461538462e-05,
      "loss": 4.5497,
      "step": 300
    },
    {
      "epoch": 1.2089810017271156,
      "grad_norm": 0.2813575267791748,
      "learning_rate": 4.716346153846154e-05,
      "loss": 4.535,
      "step": 350
    },
    {
      "epoch": 1.381692573402418,
      "grad_norm": 0.2632867991924286,
      "learning_rate": 4.596153846153846e-05,
      "loss": 4.5449,
      "step": 400
    },
    {
      "epoch": 1.5544041450777202,
      "grad_norm": 0.2875136137008667,
      "learning_rate": 4.4759615384615386e-05,
      "loss": 4.5375,
      "step": 450
    },
    {
      "epoch": 1.7271157167530224,
      "grad_norm": 0.28319045901298523,
      "learning_rate": 4.355769230769231e-05,
      "loss": 4.4946,
      "step": 500
    },
    {
      "epoch": 1.8998272884283247,
      "grad_norm": 0.26684433221817017,
      "learning_rate": 4.2355769230769234e-05,
      "loss": 4.538,
      "step": 550
    },
    {
      "epoch": 2.0725388601036268,
      "grad_norm": 0.2817428708076477,
      "learning_rate": 4.115384615384615e-05,
      "loss": 4.5263,
      "step": 600
    },
    {
      "epoch": 2.245250431778929,
      "grad_norm": 0.2724882960319519,
      "learning_rate": 3.995192307692308e-05,
      "loss": 4.5136,
      "step": 650
    },
    {
      "epoch": 2.4179620034542313,
      "grad_norm": 0.2763455808162689,
      "learning_rate": 3.875e-05,
      "loss": 4.4938,
      "step": 700
    },
    {
      "epoch": 2.5906735751295336,
      "grad_norm": 0.27945026755332947,
      "learning_rate": 3.7548076923076924e-05,
      "loss": 4.4847,
      "step": 750
    },
    {
      "epoch": 2.763385146804836,
      "grad_norm": 0.2764894962310791,
      "learning_rate": 3.634615384615385e-05,
      "loss": 4.4851,
      "step": 800
    },
    {
      "epoch": 2.936096718480138,
      "grad_norm": 0.2598147988319397,
      "learning_rate": 3.5144230769230766e-05,
      "loss": 4.508,
      "step": 850
    },
    {
      "epoch": 3.1088082901554404,
      "grad_norm": 0.2843441367149353,
      "learning_rate": 3.3942307692307696e-05,
      "loss": 4.4591,
      "step": 900
    },
    {
      "epoch": 3.2815198618307426,
      "grad_norm": 0.2908937335014343,
      "learning_rate": 3.274038461538462e-05,
      "loss": 4.4311,
      "step": 950
    },
    {
      "epoch": 3.454231433506045,
      "grad_norm": 0.2864486873149872,
      "learning_rate": 3.153846153846154e-05,
      "loss": 4.4843,
      "step": 1000
    },
    {
      "epoch": 3.626943005181347,
      "grad_norm": 0.2703310251235962,
      "learning_rate": 3.0336538461538462e-05,
      "loss": 4.4666,
      "step": 1050
    },
    {
      "epoch": 3.7996545768566494,
      "grad_norm": 0.27078011631965637,
      "learning_rate": 2.913461538461539e-05,
      "loss": 4.4851,
      "step": 1100
    },
    {
      "epoch": 3.9723661485319517,
      "grad_norm": 0.28499501943588257,
      "learning_rate": 2.793269230769231e-05,
      "loss": 4.4643,
      "step": 1150
    },
    {
      "epoch": 4.1450777202072535,
      "grad_norm": 0.28571367263793945,
      "learning_rate": 2.673076923076923e-05,
      "loss": 4.4551,
      "step": 1200
    },
    {
      "epoch": 4.317789291882556,
      "grad_norm": 0.28597956895828247,
      "learning_rate": 2.5528846153846155e-05,
      "loss": 4.4554,
      "step": 1250
    },
    {
      "epoch": 4.490500863557858,
      "grad_norm": 0.27925753593444824,
      "learning_rate": 2.432692307692308e-05,
      "loss": 4.4459,
      "step": 1300
    },
    {
      "epoch": 4.66321243523316,
      "grad_norm": 0.3118098974227905,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 4.4416,
      "step": 1350
    },
    {
      "epoch": 4.835924006908463,
      "grad_norm": 0.3230679929256439,
      "learning_rate": 2.1923076923076924e-05,
      "loss": 4.4704,
      "step": 1400
    },
    {
      "epoch": 5.008635578583765,
      "grad_norm": 0.2805044651031494,
      "learning_rate": 2.0721153846153848e-05,
      "loss": 4.4251,
      "step": 1450
    },
    {
      "epoch": 5.181347150259067,
      "grad_norm": 0.329572468996048,
      "learning_rate": 1.951923076923077e-05,
      "loss": 4.4094,
      "step": 1500
    },
    {
      "epoch": 5.354058721934369,
      "grad_norm": 0.2864992618560791,
      "learning_rate": 1.8317307692307693e-05,
      "loss": 4.4521,
      "step": 1550
    },
    {
      "epoch": 5.526770293609672,
      "grad_norm": 0.2795945107936859,
      "learning_rate": 1.7115384615384617e-05,
      "loss": 4.4527,
      "step": 1600
    },
    {
      "epoch": 5.699481865284974,
      "grad_norm": 0.2904895544052124,
      "learning_rate": 1.5913461538461537e-05,
      "loss": 4.4294,
      "step": 1650
    },
    {
      "epoch": 5.872193436960276,
      "grad_norm": 0.2670315206050873,
      "learning_rate": 1.4711538461538463e-05,
      "loss": 4.4485,
      "step": 1700
    },
    {
      "epoch": 6.0449050086355784,
      "grad_norm": 0.2677766978740692,
      "learning_rate": 1.3509615384615384e-05,
      "loss": 4.4283,
      "step": 1750
    },
    {
      "epoch": 6.217616580310881,
      "grad_norm": 0.3141564428806305,
      "learning_rate": 1.230769230769231e-05,
      "loss": 4.4341,
      "step": 1800
    },
    {
      "epoch": 6.390328151986183,
      "grad_norm": 0.28931835293769836,
      "learning_rate": 1.110576923076923e-05,
      "loss": 4.4211,
      "step": 1850
    },
    {
      "epoch": 6.563039723661485,
      "grad_norm": 0.3265104591846466,
      "learning_rate": 9.903846153846155e-06,
      "loss": 4.4306,
      "step": 1900
    },
    {
      "epoch": 6.7357512953367875,
      "grad_norm": 0.3028056025505066,
      "learning_rate": 8.701923076923077e-06,
      "loss": 4.4396,
      "step": 1950
    },
    {
      "epoch": 6.90846286701209,
      "grad_norm": 0.27248525619506836,
      "learning_rate": 7.5e-06,
      "loss": 4.4353,
      "step": 2000
    },
    {
      "epoch": 7.081174438687392,
      "grad_norm": 0.2764107584953308,
      "learning_rate": 6.2980769230769234e-06,
      "loss": 4.4096,
      "step": 2050
    },
    {
      "epoch": 7.253886010362694,
      "grad_norm": 0.28916507959365845,
      "learning_rate": 5.096153846153847e-06,
      "loss": 4.4088,
      "step": 2100
    },
    {
      "epoch": 7.426597582037997,
      "grad_norm": 0.297169953584671,
      "learning_rate": 3.894230769230769e-06,
      "loss": 4.4575,
      "step": 2150
    },
    {
      "epoch": 7.599309153713299,
      "grad_norm": 0.2729662358760834,
      "learning_rate": 2.6923076923076928e-06,
      "loss": 4.4227,
      "step": 2200
    },
    {
      "epoch": 7.772020725388601,
      "grad_norm": 0.2924973666667938,
      "learning_rate": 1.4903846153846156e-06,
      "loss": 4.4262,
      "step": 2250
    },
    {
      "epoch": 7.944732297063903,
      "grad_norm": 0.3071429133415222,
      "learning_rate": 2.884615384615385e-07,
      "loss": 4.4123,
      "step": 2300
    }
  ],
  "logging_steps": 50,
  "max_steps": 2312,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.475727806201856e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
